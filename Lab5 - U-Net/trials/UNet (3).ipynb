{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jEZV87IDo_d"
      },
      "source": [
        "## Students Information\n",
        "\n",
        "Please enter the names and IDs of the two students below:\n",
        "\n",
        "1. **Name**: Donia Gameel Mahmoud   \n",
        "   **ID**: `9202523`\n",
        "\n",
        "2. **Name**: Heba Ashrar Raslan  \n",
        "   **ID**: `9203667`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mVQO2JFDo_n"
      },
      "source": [
        "## Students Instructions\n",
        "\n",
        "This is your last graded lab assignment, as you put the work you have studied in the lectures in action, please take this opportunity to enhance your understanding of the concepts and hone your skills. As you work on your assignment, please keep the following instructions in mind:\n",
        "\n",
        "- Clearly state your personal information where indicated.\n",
        "- Be ready with your work before the time of the next discussion slot in the schedule.\n",
        "- Plagiarism will be met with penalties, refrain from copying any answers to make the most out of the assignment. If any signs of plagiarism are detected, actions will be taken.\n",
        "- It is acceptable to share the workload of the assignment bearing the discussion in mind.\n",
        "- Feel free to [reach out](mailto:cmpsy27@gmail.com) if there were any ambiguities or post on the classroom."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wMXTmpHDo_p"
      },
      "source": [
        "## Submission Instructions\n",
        "\n",
        "To ensure a smooth evaluation process, please follow these steps for submitting your work:\n",
        "\n",
        "1. **Prepare Your Submission:** Alongside your main notebook, include any additional files that are necessary for running the notebook successfully. This might include data files, images, or supplementary scripts.\n",
        "\n",
        "2. **Rename Your Files:** Before submission, please rename your notebook to reflect the IDs of the two students working on this assignment. The format should be `ID1_ID2`, where `ID1` and `ID2` are the student IDs. For example, if the student IDs are `9123456` and `9876543`, then your notebook should be named `9123456_9876543.ipynb`.\n",
        "\n",
        "3. **Check for Completeness:** Ensure that all required tasks are completed and that the notebook runs from start to finish without errors. This step is crucial for a smooth evaluation.\n",
        "\n",
        "4. **Submit Your Work:** Once everything is in order, submit your notebook and any additional files via the designated submission link on Google Classroom **(code: 2yj6e24)**. Make sure you meet the submission deadline to avoid any late penalties.\n",
        "5. Please, note that the same student should submit the assignments for the pair throughout the semester.\n",
        "\n",
        "By following these instructions carefully, you help us in evaluating your work efficiently and fairly **and any failure to adhere to these guidelines can affect your grades**. If you encounter any difficulties or have questions about the submission process, please reach out as soon as possible.\n",
        "\n",
        "We look forward to seeing your completed assignments and wish you the best of luck!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHBeci07Do_q"
      },
      "source": [
        "## Installation Instructions\n",
        "\n",
        "In this lab assignment, we require additional Python libraries for machine learning (ML) and deep learning (DL) algorithms and frameworks. To fulfill these requirements, we need to install Pytorch.\n",
        "1. Install Pytorch \\\n",
        "PyTorch is a versatile and powerful machine learning library for Python, known for its flexibility and ease of use in research and production. It supports various deep learning operations and models, including convolutional and recurrent neural networks. For Windows users, the installation also requires ensuring that CUDA, provided by NVIDIA, is compatible to enable GPU acceleration. This enhances performance significantly, particularly in training large neural networks.\\\n",
        "For windows installation with GPU support you can [check out this link](https://pytorch.org/get-started/locally/) which is the source for the command below and please know that support for GPU is done for windows so you can also check out [previous versions](https://pytorch.org/get-started/previous-versions/), you could use CPU on windows smoothly, use linux or resort to [WSL](https://www.youtube.com/watch?v=R4m8YEixidI).\n",
        "\n",
        "```bash\n",
        "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "```\n",
        "```bash\n",
        "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROWIASRBDo_r"
      },
      "source": [
        "> **Note:** You are allowed to install any other necessary libraries you deem useful for solving the lab. Please ensure that any additional libraries are compatible with the assignment requirements and are properly documented in your submission.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDUwgujlDo_r"
      },
      "source": [
        "## U-Net\n",
        "\n",
        "U-Net is a convolutional neural network designed specifically for biomedical image segmentation. First introduced in a 2015 paper by Olaf Ronneberger, Philipp Fischer, and Thomas Brox, its architecture is notably shaped like the letter \"U\", which inspired its name. This structure comprises a contracting path to capture context and a symmetrically expanding path that aids in precise localization, making it particularly adept at leveraging small datasets to achieve highly accurate segmentations. U-Net's ability to accurately segment various tissues and medical conditions has made it a staple in medical imaging and has spurred adaptations for broader image analysis applications. Its success in the medical domain demonstrates its potential for high-impact applications in other fields requiring detailed image segmentation.\n",
        "\n",
        "U-Net's robust architecture has found significant applications in the field of satellite imaging as well. The ability of U-Net to effectively handle multi-scale and high-resolution images makes it particularly suitable for satellite image analysis. In satellite imaging, U-Net is commonly used for tasks such as land cover classification, road detection, and building segmentation. Its structure allows for precise segmentation of complex objects from high-resolution satellite imagery, even with relatively limited labeled datasets. This capability is crucial in environmental monitoring, urban planning, and disaster management, where accurate and detailed image analysis is required. The adaptability and efficiency of U-Net in handling spatial hierarchies and various textures present in satellite images highlight its versatility and effectiveness beyond its initial medical context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHdUIh2MDo_t"
      },
      "source": [
        "### U-Net Architecture\n",
        "\n",
        "![U-Net Architecture](unet.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBDWM3hdDo_u"
      },
      "source": [
        "### Detailed Overview of Functions in `utilities.py`\n",
        "\n",
        "The `utilities.py` file contains multiple functions to support image processing, data manipulation, and neural network training:\n",
        "\n",
        "- **`generate_random_data(height, width, count)`**:\n",
        "  - Generates random images and masks.\n",
        "\n",
        "- **`generate_img_and_mask(height, width)`**:\n",
        "  - Generates a single image and corresponding mask.\n",
        "\n",
        "- **`add_square(img, mask)`**:\n",
        "  - Adds a square to an image and updates the mask.\n",
        "\n",
        "- **`add_filled_square(img, mask)`**:\n",
        "  - Adds a filled square to an image and updates the mask.\n",
        "\n",
        "- **`logical_and(mask1, mask2)`**:\n",
        "  - Performs logical 'AND' operation between two masks.\n",
        "\n",
        "- **`add_mesh_square(img, mask)`**:\n",
        "  - Adds a mesh-patterned square to an image and updates the mask.\n",
        "\n",
        "- **`add_triangle(img, mask)`**:\n",
        "  - Adds a triangle to an image and updates the mask.\n",
        "\n",
        "- **`add_circle(img, mask)`**:\n",
        "  - Adds a circle to an image and updates the mask.\n",
        "\n",
        "- **`add_plus(img, mask)`**:\n",
        "  - Adds a plus sign to an image and updates the mask.\n",
        "\n",
        "- **`get_random_location(shape, zoom)`**:\n",
        "  - Gets a random location within an image shape.\n",
        "\n",
        "- **`plot_img_array(img_array, n_col)`**:\n",
        "  - Plots an array of images.\n",
        "\n",
        "- **`plot_side_by_side(img_arrays)`**:\n",
        "  - Plots images side by side for comparison.\n",
        "\n",
        "- **`plot_errors(train_losses, val_losses)`**:\n",
        "  - Plots training and validation loss errors.\n",
        "\n",
        "- **`masks_to_colorimg(masks)`**:\n",
        "  - Converts masks to colored images.\n",
        "\n",
        "- **`generate_images_and_masks_then_plot()`**:\n",
        "  - Generates images and masks and plots them.\n",
        "\n",
        "- **`reverse_transform(transform)`**:\n",
        "  - Reverses a transformation applied to an image.\n",
        "\n",
        "- **`get_data_loaders(train_dir, valid_dir, batch_size)`**:\n",
        "  - Gets data loaders for training and validation datasets.\n",
        "\n",
        "- **`dice_loss(inputs, targets, smooth)`**:\n",
        "  - Calculates Dice loss for model evaluation.\n",
        "\n",
        "- **`calc_loss(pred, target, metrics)`**:\n",
        "  - Calculates loss and updates metrics.\n",
        "\n",
        "- **`print_metrics(metrics, epoch_samples, phase)`**:\n",
        "  - Prints metrics for training or validation phases.\n",
        "\n",
        "- **`train_model(model, dataloaders, criterion, optimizer, num_epochs)`**:\n",
        "  - Trains a model given data loaders and training parameters.\n",
        "\n",
        "- **`run(model)`**:\n",
        "  - Runs a full model training and evaluation session.\n",
        "\n",
        "- **`__init__`, `__len__`, `__getitem__`**:\n",
        "  - Standard class methods used in Python classes, typically for data handling or model initialization.\n",
        "\n",
        "These functions collectively facilitate the full lifecycle of processing, training, and evaluating neural network models, particularly focusing on image-related tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xela_RBiDo_v"
      },
      "source": [
        "The primary objective of this assignment is to develop a neural network model capable of performing accurate image segmentation. Image segmentation involves dividing an image into segments that represent different objects or regions, which is crucial in various applications such as medical imaging, autonomous driving, and satellite image analysis.\n",
        "\n",
        "#### Image and Mask Generation\n",
        "In this assignment, synthetic images along with corresponding segmentation masks are generated to train and evaluate the segmentation model. Each image typically includes multiple geometric shapes placed randomly. These shapes can include simple forms such as circles, squares, and triangles, or more complex designs.\n",
        "\n",
        "#### Masks and Their Role in Segmentation\n",
        "- **Mask Generation**: Alongside each synthetic image, a mask is generated where each shape in the image has a corresponding segment in the mask. Each segment in the mask is represented by a unique color or grayscale intensity, where each intensity level corresponds to a different class (shape).\n",
        "- **Function of the Mask**: The mask serves as a \"ground truth\" for training the segmentation model. The model learns to predict these masks from the input images. Essentially, the task of the model is to map the input image to its corresponding output mask, segmenting the shapes it has learned during training.\n",
        "- **Learning Process**: During training, the model adjusts its parameters to minimize the difference between its predicted masks and the ground truth masks. This process involves optimizing a loss function, typically using backpropagation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbMuGZ3xDo_w"
      },
      "source": [
        "## Requirement - U-Net Application\n",
        "- **Implement or Use U-Net**:\n",
        "  - **Option 1: Implementation** - Students are encouraged to implement the U-Net architecture from scratch, including defining all the layers and connections based on the standard specifications found in the literature.\n",
        "  - **Option 2: Use Pre-existing Implementation** - Students may also opt to use an existing implementation of U-Net. This can include adapting an open-source model available in frameworks like PyTorch.\n",
        "\n",
        "- **Integration with `utilities.py`**:\n",
        "  - The U-Net model, whether self-implemented or pre-existing, must be integrated with the `utilities.py` script provided. This integration is essential for processing data, training the model, and evaluating its performance efficiently.\n",
        "  - The model should effectively process input images and produce accurate segmentation masks as output, corresponding to the different segments of the input images.\n",
        "  - **You can make any necessary modifications including writing your own training loop for example**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q33APvHiDo_x"
      },
      "outputs": [],
      "source": [
        "# import libraries here\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-GeMeXSbDo_0"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "        super().__init__()\n",
        "        # Define U-Net architecture with increased complexity\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, n_class, kernel_size=1),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through encoder\n",
        "        encoder_out = self.encoder(x)\n",
        "\n",
        "        # Forward pass through decoder\n",
        "        decoder_out = self.decoder(encoder_out)\n",
        "\n",
        "        return decoder_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nBkASaL1Do_0",
        "outputId": "d509ecae-201b-4bd8-dd67-ec5d616e0fe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/599\n",
            "----------\n",
            "LR 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: bce: 0.708417, dice: 0.990475, jaccard: 0.004266, loss: 0.849446\n",
            "val: bce: 0.708288, dice: 0.990680, jaccard: 0.004150, loss: 0.849484\n",
            "saving best model\n",
            "0m 4s\n",
            "Epoch 1/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.708204, dice: 0.990475, jaccard: 0.004266, loss: 0.849340\n",
            "val: bce: 0.708074, dice: 0.990680, jaccard: 0.004150, loss: 0.849377\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 2/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.707991, dice: 0.990475, jaccard: 0.004266, loss: 0.849233\n",
            "val: bce: 0.707860, dice: 0.990680, jaccard: 0.004150, loss: 0.849270\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 3/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.707776, dice: 0.990475, jaccard: 0.004266, loss: 0.849125\n",
            "val: bce: 0.707644, dice: 0.990680, jaccard: 0.004150, loss: 0.849162\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 4/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.707560, dice: 0.990475, jaccard: 0.004266, loss: 0.849017\n",
            "val: bce: 0.707426, dice: 0.990680, jaccard: 0.004150, loss: 0.849054\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 5/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.707341, dice: 0.990475, jaccard: 0.004266, loss: 0.848908\n",
            "val: bce: 0.707207, dice: 0.990681, jaccard: 0.004150, loss: 0.848944\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 6/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.707121, dice: 0.990475, jaccard: 0.004266, loss: 0.848798\n",
            "val: bce: 0.706986, dice: 0.990681, jaccard: 0.004150, loss: 0.848833\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 7/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.706900, dice: 0.990475, jaccard: 0.004266, loss: 0.848687\n",
            "val: bce: 0.706764, dice: 0.990681, jaccard: 0.004150, loss: 0.848722\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 8/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.706677, dice: 0.990475, jaccard: 0.004266, loss: 0.848576\n",
            "val: bce: 0.706541, dice: 0.990681, jaccard: 0.004150, loss: 0.848611\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 9/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.706454, dice: 0.990475, jaccard: 0.004266, loss: 0.848465\n",
            "val: bce: 0.706317, dice: 0.990681, jaccard: 0.004150, loss: 0.848499\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 10/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.706230, dice: 0.990475, jaccard: 0.004266, loss: 0.848353\n",
            "val: bce: 0.706091, dice: 0.990681, jaccard: 0.004150, loss: 0.848386\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 11/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.706002, dice: 0.990475, jaccard: 0.004266, loss: 0.848239\n",
            "val: bce: 0.705861, dice: 0.990681, jaccard: 0.004150, loss: 0.848271\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 12/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.705771, dice: 0.990475, jaccard: 0.004266, loss: 0.848123\n",
            "val: bce: 0.705629, dice: 0.990681, jaccard: 0.004150, loss: 0.848155\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 13/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.705538, dice: 0.990475, jaccard: 0.004266, loss: 0.848007\n",
            "val: bce: 0.705393, dice: 0.990681, jaccard: 0.004150, loss: 0.848037\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 14/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.705301, dice: 0.990475, jaccard: 0.004266, loss: 0.847888\n",
            "val: bce: 0.705154, dice: 0.990681, jaccard: 0.004150, loss: 0.847918\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 15/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.705061, dice: 0.990475, jaccard: 0.004266, loss: 0.847768\n",
            "val: bce: 0.704912, dice: 0.990681, jaccard: 0.004150, loss: 0.847797\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 16/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.704818, dice: 0.990475, jaccard: 0.004266, loss: 0.847647\n",
            "val: bce: 0.704668, dice: 0.990681, jaccard: 0.004150, loss: 0.847674\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 17/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.704572, dice: 0.990475, jaccard: 0.004266, loss: 0.847524\n",
            "val: bce: 0.704419, dice: 0.990681, jaccard: 0.004150, loss: 0.847550\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 18/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.704322, dice: 0.990476, jaccard: 0.004266, loss: 0.847399\n",
            "val: bce: 0.704166, dice: 0.990681, jaccard: 0.004150, loss: 0.847424\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 19/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.704067, dice: 0.990476, jaccard: 0.004266, loss: 0.847271\n",
            "val: bce: 0.703908, dice: 0.990681, jaccard: 0.004150, loss: 0.847294\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 20/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.703807, dice: 0.990476, jaccard: 0.004266, loss: 0.847141\n",
            "val: bce: 0.703644, dice: 0.990681, jaccard: 0.004150, loss: 0.847163\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 21/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.703541, dice: 0.990476, jaccard: 0.004266, loss: 0.847008\n",
            "val: bce: 0.703372, dice: 0.990681, jaccard: 0.004150, loss: 0.847027\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 22/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.703264, dice: 0.990476, jaccard: 0.004266, loss: 0.846870\n",
            "val: bce: 0.703090, dice: 0.990681, jaccard: 0.004150, loss: 0.846886\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 23/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.702979, dice: 0.990476, jaccard: 0.004266, loss: 0.846727\n",
            "val: bce: 0.702799, dice: 0.990681, jaccard: 0.004150, loss: 0.846740\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 24/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.702685, dice: 0.990476, jaccard: 0.004266, loss: 0.846581\n",
            "val: bce: 0.702500, dice: 0.990681, jaccard: 0.004150, loss: 0.846591\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 25/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.702381, dice: 0.990476, jaccard: 0.004266, loss: 0.846428\n",
            "val: bce: 0.702185, dice: 0.990681, jaccard: 0.004150, loss: 0.846433\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 26/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.702059, dice: 0.990476, jaccard: 0.004266, loss: 0.846268\n",
            "val: bce: 0.701853, dice: 0.990682, jaccard: 0.004150, loss: 0.846267\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 27/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.701720, dice: 0.990476, jaccard: 0.004266, loss: 0.846098\n",
            "val: bce: 0.701503, dice: 0.990682, jaccard: 0.004150, loss: 0.846092\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 28/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.701364, dice: 0.990476, jaccard: 0.004266, loss: 0.845920\n",
            "val: bce: 0.701132, dice: 0.990682, jaccard: 0.004150, loss: 0.845907\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 29/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.700981, dice: 0.990476, jaccard: 0.004266, loss: 0.845729\n",
            "val: bce: 0.700726, dice: 0.990682, jaccard: 0.004150, loss: 0.845704\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 30/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.700558, dice: 0.990477, jaccard: 0.004266, loss: 0.845517\n",
            "val: bce: 0.700270, dice: 0.990682, jaccard: 0.004150, loss: 0.845476\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 31/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.700074, dice: 0.990477, jaccard: 0.004266, loss: 0.845275\n",
            "val: bce: 0.699724, dice: 0.990683, jaccard: 0.004150, loss: 0.845203\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 32/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.699461, dice: 0.990477, jaccard: 0.004266, loss: 0.844969\n",
            "val: bce: 0.698960, dice: 0.990683, jaccard: 0.004150, loss: 0.844821\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 33/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.698530, dice: 0.990479, jaccard: 0.004266, loss: 0.844504\n",
            "val: bce: 0.697618, dice: 0.990685, jaccard: 0.004150, loss: 0.844152\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 34/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.696624, dice: 0.990482, jaccard: 0.004266, loss: 0.843553\n",
            "val: bce: 0.694228, dice: 0.990693, jaccard: 0.004150, loss: 0.842461\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 35/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.691274, dice: 0.990495, jaccard: 0.004266, loss: 0.840885\n",
            "val: bce: 0.683935, dice: 0.990718, jaccard: 0.004031, loss: 0.837327\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 36/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.674457, dice: 0.990542, jaccard: 0.003561, loss: 0.832499\n",
            "val: bce: 0.650560, dice: 0.990817, jaccard: 0.002108, loss: 0.820689\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 37/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.620574, dice: 0.990729, jaccard: 0.001614, loss: 0.805652\n",
            "val: bce: 0.549144, dice: 0.991244, jaccard: 0.001435, loss: 0.770194\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 38/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.475737, dice: 0.991573, jaccard: 0.001227, loss: 0.733655\n",
            "val: bce: 0.331654, dice: 0.993241, jaccard: 0.001276, loss: 0.662447\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 39/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.244683, dice: 0.994575, jaccard: 0.001272, loss: 0.619629\n",
            "val: bce: 0.129976, dice: 0.997220, jaccard: 0.001790, loss: 0.563598\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 40/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.105917, dice: 0.997760, jaccard: 0.000505, loss: 0.551838\n",
            "val: bce: 0.094705, dice: 0.997912, jaccard: 0.000000, loss: 0.546308\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 41/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.107579, dice: 0.997550, jaccard: 0.000000, loss: 0.552565\n",
            "val: bce: 0.124829, dice: 0.997146, jaccard: 0.000000, loss: 0.560987\n",
            "0m 2s\n",
            "Epoch 42/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.125965, dice: 0.997005, jaccard: 0.000000, loss: 0.561485\n",
            "val: bce: 0.118730, dice: 0.997134, jaccard: 0.000000, loss: 0.557932\n",
            "0m 2s\n",
            "Epoch 43/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.108957, dice: 0.997264, jaccard: 0.000000, loss: 0.553111\n",
            "val: bce: 0.092245, dice: 0.997716, jaccard: 0.000000, loss: 0.544980\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 44/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.084081, dice: 0.997904, jaccard: 0.000000, loss: 0.540993\n",
            "val: bce: 0.073381, dice: 0.998282, jaccard: 0.000000, loss: 0.535832\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 45/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.070747, dice: 0.998317, jaccard: 0.000000, loss: 0.534532\n",
            "val: bce: 0.068938, dice: 0.998322, jaccard: 0.000000, loss: 0.533630\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 46/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.069882, dice: 0.998174, jaccard: 0.000000, loss: 0.534028\n",
            "val: bce: 0.071501, dice: 0.998071, jaccard: 0.000000, loss: 0.534786\n",
            "0m 2s\n",
            "Epoch 47/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.071915, dice: 0.997965, jaccard: 0.000000, loss: 0.534940\n",
            "val: bce: 0.071153, dice: 0.998051, jaccard: 0.000000, loss: 0.534602\n",
            "0m 2s\n",
            "Epoch 48/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.069938, dice: 0.998054, jaccard: 0.000000, loss: 0.533996\n",
            "val: bce: 0.067365, dice: 0.998260, jaccard: 0.000000, loss: 0.532812\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 49/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.066427, dice: 0.998280, jaccard: 0.000000, loss: 0.532353\n",
            "val: bce: 0.064810, dice: 0.998409, jaccard: 0.000000, loss: 0.531609\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 50/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.064717, dice: 0.998374, jaccard: 0.000000, loss: 0.531546\n",
            "val: bce: 0.064230, dice: 0.998417, jaccard: 0.000000, loss: 0.531324\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 51/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.064379, dice: 0.998373, jaccard: 0.000000, loss: 0.531376\n",
            "val: bce: 0.063758, dice: 0.998396, jaccard: 0.000000, loss: 0.531077\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 52/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.063737, dice: 0.998361, jaccard: 0.000000, loss: 0.531049\n",
            "val: bce: 0.062796, dice: 0.998394, jaccard: 0.000000, loss: 0.530595\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 53/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.062776, dice: 0.998359, jaccard: 0.000000, loss: 0.530568\n",
            "val: bce: 0.061844, dice: 0.998391, jaccard: 0.000000, loss: 0.530117\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 54/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.061935, dice: 0.998346, jaccard: 0.000000, loss: 0.530141\n",
            "val: bce: 0.061147, dice: 0.998371, jaccard: 0.000000, loss: 0.529759\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 55/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.061300, dice: 0.998322, jaccard: 0.000000, loss: 0.529811\n",
            "val: bce: 0.060549, dice: 0.998347, jaccard: 0.000000, loss: 0.529448\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 56/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.060713, dice: 0.998300, jaccard: 0.000000, loss: 0.529506\n",
            "val: bce: 0.059892, dice: 0.998327, jaccard: 0.000000, loss: 0.529110\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 57/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.060066, dice: 0.998283, jaccard: 0.000000, loss: 0.529174\n",
            "val: bce: 0.059191, dice: 0.998310, jaccard: 0.000000, loss: 0.528750\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 58/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.059401, dice: 0.998269, jaccard: 0.000000, loss: 0.528835\n",
            "val: bce: 0.058512, dice: 0.998291, jaccard: 0.000000, loss: 0.528402\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 59/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.058761, dice: 0.998251, jaccard: 0.000000, loss: 0.528506\n",
            "val: bce: 0.057853, dice: 0.998269, jaccard: 0.000000, loss: 0.528061\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 60/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.058130, dice: 0.998229, jaccard: 0.000000, loss: 0.528179\n",
            "val: bce: 0.057187, dice: 0.998244, jaccard: 0.000000, loss: 0.527716\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 61/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.057477, dice: 0.998205, jaccard: 0.000000, loss: 0.527841\n",
            "val: bce: 0.056511, dice: 0.998218, jaccard: 0.000000, loss: 0.527364\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 62/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.056811, dice: 0.998179, jaccard: 0.000000, loss: 0.527495\n",
            "val: bce: 0.055821, dice: 0.998190, jaccard: 0.000000, loss: 0.527006\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 63/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.056134, dice: 0.998150, jaccard: 0.000000, loss: 0.527142\n",
            "val: bce: 0.055113, dice: 0.998158, jaccard: 0.000000, loss: 0.526635\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 64/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.055425, dice: 0.998119, jaccard: 0.000000, loss: 0.526772\n",
            "val: bce: 0.054386, dice: 0.998125, jaccard: 0.000000, loss: 0.526256\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 65/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.054707, dice: 0.998086, jaccard: 0.000000, loss: 0.526397\n",
            "val: bce: 0.053636, dice: 0.998089, jaccard: 0.000000, loss: 0.525863\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 66/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.053963, dice: 0.998050, jaccard: 0.000000, loss: 0.526007\n",
            "val: bce: 0.052868, dice: 0.998050, jaccard: 0.000000, loss: 0.525459\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 67/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.053205, dice: 0.998011, jaccard: 0.000000, loss: 0.525608\n",
            "val: bce: 0.052085, dice: 0.998008, jaccard: 0.000000, loss: 0.525046\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 68/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.052424, dice: 0.997968, jaccard: 0.000000, loss: 0.525196\n",
            "val: bce: 0.051281, dice: 0.997959, jaccard: 0.000000, loss: 0.524620\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 69/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.051619, dice: 0.997920, jaccard: 0.000000, loss: 0.524770\n",
            "val: bce: 0.050458, dice: 0.997907, jaccard: 0.000000, loss: 0.524182\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 70/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.050796, dice: 0.997866, jaccard: 0.000000, loss: 0.524331\n",
            "val: bce: 0.049626, dice: 0.997847, jaccard: 0.000000, loss: 0.523736\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 71/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.049966, dice: 0.997804, jaccard: 0.000000, loss: 0.523885\n",
            "val: bce: 0.048777, dice: 0.997779, jaccard: 0.000000, loss: 0.523278\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 72/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.049123, dice: 0.997732, jaccard: 0.000000, loss: 0.523428\n",
            "val: bce: 0.047910, dice: 0.997698, jaccard: 0.000000, loss: 0.522804\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 73/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.048243, dice: 0.997651, jaccard: 0.000000, loss: 0.522947\n",
            "val: bce: 0.047017, dice: 0.997613, jaccard: 0.000000, loss: 0.522315\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 74/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.047350, dice: 0.997567, jaccard: 0.000000, loss: 0.522458\n",
            "val: bce: 0.046117, dice: 0.997519, jaccard: 0.000000, loss: 0.521818\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 75/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.046449, dice: 0.997467, jaccard: 0.000000, loss: 0.521958\n",
            "val: bce: 0.045204, dice: 0.997407, jaccard: 0.000000, loss: 0.521305\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 76/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.045536, dice: 0.997348, jaccard: 0.000000, loss: 0.521442\n",
            "val: bce: 0.044277, dice: 0.997278, jaccard: 0.000000, loss: 0.520777\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 77/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.044605, dice: 0.997216, jaccard: 0.000000, loss: 0.520911\n",
            "val: bce: 0.043329, dice: 0.997133, jaccard: 0.000000, loss: 0.520231\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 78/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.043642, dice: 0.997072, jaccard: 0.000000, loss: 0.520357\n",
            "val: bce: 0.042356, dice: 0.996982, jaccard: 0.000000, loss: 0.519669\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 79/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.042678, dice: 0.996910, jaccard: 0.000000, loss: 0.519794\n",
            "val: bce: 0.041403, dice: 0.996787, jaccard: 0.000000, loss: 0.519095\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 80/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.041718, dice: 0.996702, jaccard: 0.000000, loss: 0.519210\n",
            "val: bce: 0.040438, dice: 0.996576, jaccard: 0.000000, loss: 0.518507\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 81/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.040744, dice: 0.996486, jaccard: 0.000000, loss: 0.518615\n",
            "val: bce: 0.039475, dice: 0.996331, jaccard: 0.000000, loss: 0.517903\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 82/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.039791, dice: 0.996221, jaccard: 0.000000, loss: 0.518006\n",
            "val: bce: 0.038512, dice: 0.996040, jaccard: 0.000000, loss: 0.517276\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 83/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.038801, dice: 0.995927, jaccard: 0.000000, loss: 0.517364\n",
            "val: bce: 0.037544, dice: 0.995716, jaccard: 0.000000, loss: 0.516630\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 84/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.037827, dice: 0.995578, jaccard: 0.000000, loss: 0.516703\n",
            "val: bce: 0.036603, dice: 0.995330, jaccard: 0.000000, loss: 0.515966\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 85/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.036922, dice: 0.995151, jaccard: 0.000000, loss: 0.516037\n",
            "val: bce: 0.035696, dice: 0.994858, jaccard: 0.000000, loss: 0.515277\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 86/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.035982, dice: 0.994671, jaccard: 0.000000, loss: 0.515326\n",
            "val: bce: 0.034754, dice: 0.994354, jaccard: 0.000000, loss: 0.514554\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 87/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.035037, dice: 0.994142, jaccard: 0.000000, loss: 0.514590\n",
            "val: bce: 0.033927, dice: 0.993702, jaccard: 0.000000, loss: 0.513815\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 88/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.034233, dice: 0.993428, jaccard: 0.000000, loss: 0.513830\n",
            "val: bce: 0.033085, dice: 0.993025, jaccard: 0.000000, loss: 0.513055\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 89/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.033386, dice: 0.992729, jaccard: 0.000000, loss: 0.513057\n",
            "val: bce: 0.032364, dice: 0.992214, jaccard: 0.000000, loss: 0.512289\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 90/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.032732, dice: 0.991802, jaccard: 0.000000, loss: 0.512267\n",
            "val: bce: 0.031765, dice: 0.991299, jaccard: 0.000000, loss: 0.511532\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 91/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.032103, dice: 0.990891, jaccard: 0.000000, loss: 0.511497\n",
            "val: bce: 0.031177, dice: 0.990410, jaccard: 0.000000, loss: 0.510794\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 92/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031607, dice: 0.989854, jaccard: 0.000000, loss: 0.510730\n",
            "val: bce: 0.030742, dice: 0.989433, jaccard: 0.000000, loss: 0.510088\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 93/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030949, dice: 0.988925, jaccard: 0.000000, loss: 0.509937\n",
            "val: bce: 0.030486, dice: 0.988308, jaccard: 0.000000, loss: 0.509397\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 94/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030725, dice: 0.987542, jaccard: 0.000000, loss: 0.509133\n",
            "val: bce: 0.029723, dice: 0.987519, jaccard: 0.000000, loss: 0.508621\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 95/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030066, dice: 0.986331, jaccard: 0.000000, loss: 0.508198\n",
            "val: bce: 0.029999, dice: 0.985522, jaccard: 0.000000, loss: 0.507760\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 96/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029668, dice: 0.984468, jaccard: 0.000000, loss: 0.507068\n",
            "val: bce: 0.029027, dice: 0.984029, jaccard: 0.000000, loss: 0.506528\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 97/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029290, dice: 0.981701, jaccard: 0.000000, loss: 0.505495\n",
            "val: bce: 0.028240, dice: 0.981617, jaccard: 0.000000, loss: 0.504928\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 98/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028723, dice: 0.977923, jaccard: 0.000000, loss: 0.503323\n",
            "val: bce: 0.027614, dice: 0.977422, jaccard: 0.000000, loss: 0.502518\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 99/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027979, dice: 0.972027, jaccard: 0.000000, loss: 0.500003\n",
            "val: bce: 0.028390, dice: 0.969128, jaccard: 0.000000, loss: 0.498759\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 100/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027767, dice: 0.963331, jaccard: 0.000000, loss: 0.495549\n",
            "val: bce: 0.028193, dice: 0.959482, jaccard: 0.000000, loss: 0.493837\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 101/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027547, dice: 0.952947, jaccard: 0.000000, loss: 0.490247\n",
            "val: bce: 0.029196, dice: 0.948914, jaccard: 0.000000, loss: 0.489055\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 102/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028087, dice: 0.943192, jaccard: 0.000000, loss: 0.485639\n",
            "val: bce: 0.026860, dice: 0.943642, jaccard: 0.000000, loss: 0.485251\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 103/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027762, dice: 0.936395, jaccard: 0.000000, loss: 0.482079\n",
            "val: bce: 0.027438, dice: 0.935348, jaccard: 0.000000, loss: 0.481393\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 104/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027602, dice: 0.929965, jaccard: 0.000000, loss: 0.478783\n",
            "val: bce: 0.028220, dice: 0.929820, jaccard: 0.000000, loss: 0.479020\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 105/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027737, dice: 0.925122, jaccard: 0.000038, loss: 0.476430\n",
            "val: bce: 0.026847, dice: 0.926745, jaccard: 0.000039, loss: 0.476796\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 106/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027152, dice: 0.921684, jaccard: 0.000207, loss: 0.474418\n",
            "val: bce: 0.027055, dice: 0.923043, jaccard: 0.000115, loss: 0.475049\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 107/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027117, dice: 0.918404, jaccard: 0.001113, loss: 0.472760\n",
            "val: bce: 0.026168, dice: 0.920496, jaccard: 0.000682, loss: 0.473332\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 108/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027325, dice: 0.915190, jaccard: 0.001784, loss: 0.471258\n",
            "val: bce: 0.025377, dice: 0.918765, jaccard: 0.000828, loss: 0.472071\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 109/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027133, dice: 0.912862, jaccard: 0.002518, loss: 0.469998\n",
            "val: bce: 0.025559, dice: 0.915198, jaccard: 0.001488, loss: 0.470378\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 110/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027042, dice: 0.910013, jaccard: 0.003095, loss: 0.468527\n",
            "val: bce: 0.026311, dice: 0.911012, jaccard: 0.002033, loss: 0.468662\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 111/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026908, dice: 0.907583, jaccard: 0.003596, loss: 0.467245\n",
            "val: bce: 0.027760, dice: 0.907453, jaccard: 0.002877, loss: 0.467606\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 112/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027388, dice: 0.904880, jaccard: 0.004583, loss: 0.466134\n",
            "val: bce: 0.025952, dice: 0.905890, jaccard: 0.002393, loss: 0.465921\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 113/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027677, dice: 0.902027, jaccard: 0.006011, loss: 0.464852\n",
            "val: bce: 0.025841, dice: 0.903403, jaccard: 0.002754, loss: 0.464622\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 114/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026991, dice: 0.900409, jaccard: 0.005928, loss: 0.463700\n",
            "val: bce: 0.027223, dice: 0.899631, jaccard: 0.004901, loss: 0.463427\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 115/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027087, dice: 0.898442, jaccard: 0.007183, loss: 0.462765\n",
            "val: bce: 0.025977, dice: 0.898112, jaccard: 0.004965, loss: 0.462045\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 116/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027411, dice: 0.895486, jaccard: 0.009462, loss: 0.461448\n",
            "val: bce: 0.026236, dice: 0.895428, jaccard: 0.007133, loss: 0.460832\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 117/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026837, dice: 0.893863, jaccard: 0.009894, loss: 0.460350\n",
            "val: bce: 0.025723, dice: 0.893504, jaccard: 0.008073, loss: 0.459614\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 118/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026873, dice: 0.891685, jaccard: 0.012382, loss: 0.459279\n",
            "val: bce: 0.025319, dice: 0.891343, jaccard: 0.009169, loss: 0.458331\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 119/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.026013, dice: 0.890370, jaccard: 0.011994, loss: 0.458192\n",
            "val: bce: 0.027115, dice: 0.888436, jaccard: 0.017053, loss: 0.457776\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 120/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026382, dice: 0.887992, jaccard: 0.015587, loss: 0.457187\n",
            "val: bce: 0.024580, dice: 0.887060, jaccard: 0.011486, loss: 0.455820\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 121/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025934, dice: 0.885651, jaccard: 0.015565, loss: 0.455793\n",
            "val: bce: 0.025381, dice: 0.884058, jaccard: 0.016403, loss: 0.454719\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 122/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025463, dice: 0.883985, jaccard: 0.015179, loss: 0.454724\n",
            "val: bce: 0.024483, dice: 0.882181, jaccard: 0.015916, loss: 0.453332\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 123/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025382, dice: 0.881703, jaccard: 0.017423, loss: 0.453542\n",
            "val: bce: 0.025197, dice: 0.879595, jaccard: 0.021111, loss: 0.452396\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 124/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025200, dice: 0.879754, jaccard: 0.018996, loss: 0.452477\n",
            "val: bce: 0.024821, dice: 0.877302, jaccard: 0.022222, loss: 0.451061\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 125/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025150, dice: 0.877460, jaccard: 0.021294, loss: 0.451305\n",
            "val: bce: 0.024576, dice: 0.875044, jaccard: 0.024196, loss: 0.449810\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 126/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024889, dice: 0.875439, jaccard: 0.022281, loss: 0.450164\n",
            "val: bce: 0.024783, dice: 0.872705, jaccard: 0.028415, loss: 0.448744\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 127/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024782, dice: 0.873205, jaccard: 0.024822, loss: 0.448993\n",
            "val: bce: 0.023201, dice: 0.870855, jaccard: 0.022272, loss: 0.447028\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 128/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024772, dice: 0.871412, jaccard: 0.027151, loss: 0.448092\n",
            "val: bce: 0.022816, dice: 0.869116, jaccard: 0.022246, loss: 0.445966\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 129/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024292, dice: 0.869665, jaccard: 0.027463, loss: 0.446979\n",
            "val: bce: 0.023512, dice: 0.866131, jaccard: 0.029580, loss: 0.444821\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 130/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024230, dice: 0.867324, jaccard: 0.030055, loss: 0.445777\n",
            "val: bce: 0.022462, dice: 0.864651, jaccard: 0.025491, loss: 0.443556\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 131/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024076, dice: 0.866354, jaccard: 0.031191, loss: 0.445215\n",
            "val: bce: 0.022227, dice: 0.862874, jaccard: 0.026406, loss: 0.442550\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 132/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023608, dice: 0.864640, jaccard: 0.032199, loss: 0.444124\n",
            "val: bce: 0.023935, dice: 0.860313, jaccard: 0.041440, loss: 0.442124\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 133/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023447, dice: 0.862265, jaccard: 0.034543, loss: 0.442856\n",
            "val: bce: 0.023521, dice: 0.858433, jaccard: 0.042133, loss: 0.440977\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 134/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023266, dice: 0.860469, jaccard: 0.036880, loss: 0.441868\n",
            "val: bce: 0.022873, dice: 0.856674, jaccard: 0.041379, loss: 0.439774\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 135/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023023, dice: 0.858911, jaccard: 0.038469, loss: 0.440967\n",
            "val: bce: 0.023107, dice: 0.854842, jaccard: 0.044192, loss: 0.438974\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 136/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023142, dice: 0.857820, jaccard: 0.041291, loss: 0.440481\n",
            "val: bce: 0.024023, dice: 0.853779, jaccard: 0.053101, loss: 0.438901\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 137/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022892, dice: 0.855832, jaccard: 0.041136, loss: 0.439362\n",
            "val: bce: 0.023799, dice: 0.851973, jaccard: 0.053539, loss: 0.437886\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 138/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023166, dice: 0.854067, jaccard: 0.044872, loss: 0.438617\n",
            "val: bce: 0.022324, dice: 0.850699, jaccard: 0.045620, loss: 0.436511\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 139/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023397, dice: 0.852458, jaccard: 0.049601, loss: 0.437928\n",
            "val: bce: 0.021908, dice: 0.850427, jaccard: 0.043205, loss: 0.436168\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 140/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022960, dice: 0.852035, jaccard: 0.046510, loss: 0.437497\n",
            "val: bce: 0.022953, dice: 0.847827, jaccard: 0.053615, loss: 0.435390\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 141/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022693, dice: 0.849786, jaccard: 0.047798, loss: 0.436240\n",
            "val: bce: 0.023127, dice: 0.846963, jaccard: 0.057394, loss: 0.435045\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 142/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022582, dice: 0.848551, jaccard: 0.049327, loss: 0.435567\n",
            "val: bce: 0.023680, dice: 0.846298, jaccard: 0.062428, loss: 0.434989\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 143/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023018, dice: 0.847549, jaccard: 0.053588, loss: 0.435283\n",
            "val: bce: 0.022663, dice: 0.844478, jaccard: 0.059458, loss: 0.433570\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 144/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023336, dice: 0.845926, jaccard: 0.059654, loss: 0.434631\n",
            "val: bce: 0.021805, dice: 0.845489, jaccard: 0.054414, loss: 0.433647\n",
            "0m 2s\n",
            "Epoch 145/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022904, dice: 0.845627, jaccard: 0.058692, loss: 0.434265\n",
            "val: bce: 0.023110, dice: 0.842828, jaccard: 0.070909, loss: 0.432969\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 146/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022609, dice: 0.844413, jaccard: 0.059251, loss: 0.433511\n",
            "val: bce: 0.024578, dice: 0.843513, jaccard: 0.080971, loss: 0.434045\n",
            "0m 2s\n",
            "Epoch 147/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023379, dice: 0.843283, jaccard: 0.069209, loss: 0.433331\n",
            "val: bce: 0.022343, dice: 0.840980, jaccard: 0.071192, loss: 0.431662\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 148/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023344, dice: 0.841630, jaccard: 0.074157, loss: 0.432487\n",
            "val: bce: 0.022244, dice: 0.840423, jaccard: 0.073329, loss: 0.431334\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 149/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.022734, dice: 0.840541, jaccard: 0.070520, loss: 0.431638\n",
            "val: bce: 0.023797, dice: 0.839617, jaccard: 0.090194, loss: 0.431707\n",
            "0m 2s\n",
            "Epoch 150/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023099, dice: 0.838936, jaccard: 0.080041, loss: 0.431017\n",
            "val: bce: 0.022660, dice: 0.838179, jaccard: 0.086934, loss: 0.430420\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 151/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023130, dice: 0.837731, jaccard: 0.086820, loss: 0.430431\n",
            "val: bce: 0.022569, dice: 0.837576, jaccard: 0.090327, loss: 0.430072\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 152/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022778, dice: 0.836640, jaccard: 0.088030, loss: 0.429709\n",
            "val: bce: 0.023360, dice: 0.836273, jaccard: 0.100445, loss: 0.429816\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 153/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023202, dice: 0.835167, jaccard: 0.097921, loss: 0.429185\n",
            "val: bce: 0.022783, dice: 0.835535, jaccard: 0.098770, loss: 0.429159\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 154/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023204, dice: 0.834142, jaccard: 0.101868, loss: 0.428673\n",
            "val: bce: 0.022868, dice: 0.834640, jaccard: 0.103724, loss: 0.428754\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 155/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022966, dice: 0.832971, jaccard: 0.103566, loss: 0.427969\n",
            "val: bce: 0.023813, dice: 0.834145, jaccard: 0.114606, loss: 0.428979\n",
            "0m 2s\n",
            "Epoch 156/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023278, dice: 0.831813, jaccard: 0.110108, loss: 0.427546\n",
            "val: bce: 0.023883, dice: 0.832597, jaccard: 0.117918, loss: 0.428240\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 157/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023576, dice: 0.829827, jaccard: 0.118480, loss: 0.426702\n",
            "val: bce: 0.022987, dice: 0.831967, jaccard: 0.113495, loss: 0.427477\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 158/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023639, dice: 0.829465, jaccard: 0.120939, loss: 0.426552\n",
            "val: bce: 0.023499, dice: 0.829649, jaccard: 0.121343, loss: 0.426574\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 159/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023524, dice: 0.827349, jaccard: 0.122852, loss: 0.425436\n",
            "val: bce: 0.024227, dice: 0.829346, jaccard: 0.127940, loss: 0.426786\n",
            "0m 2s\n",
            "Epoch 160/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023493, dice: 0.825202, jaccard: 0.125805, loss: 0.424347\n",
            "val: bce: 0.024337, dice: 0.827989, jaccard: 0.129684, loss: 0.426163\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 161/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024021, dice: 0.822818, jaccard: 0.132130, loss: 0.423419\n",
            "val: bce: 0.024026, dice: 0.826013, jaccard: 0.125008, loss: 0.425020\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 162/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024216, dice: 0.821503, jaccard: 0.131967, loss: 0.422859\n",
            "val: bce: 0.024428, dice: 0.823367, jaccard: 0.131442, loss: 0.423897\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 163/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024454, dice: 0.818937, jaccard: 0.132118, loss: 0.421696\n",
            "val: bce: 0.025426, dice: 0.821142, jaccard: 0.133590, loss: 0.423284\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 164/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024939, dice: 0.816448, jaccard: 0.135238, loss: 0.420693\n",
            "val: bce: 0.026119, dice: 0.820273, jaccard: 0.134873, loss: 0.423196\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 165/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025109, dice: 0.813232, jaccard: 0.136620, loss: 0.419170\n",
            "val: bce: 0.026394, dice: 0.816313, jaccard: 0.136400, loss: 0.421353\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 166/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025992, dice: 0.810742, jaccard: 0.138424, loss: 0.418367\n",
            "val: bce: 0.027293, dice: 0.813053, jaccard: 0.136746, loss: 0.420173\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 167/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026618, dice: 0.806164, jaccard: 0.142191, loss: 0.416391\n",
            "val: bce: 0.027050, dice: 0.809895, jaccard: 0.135167, loss: 0.418472\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 168/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027063, dice: 0.803101, jaccard: 0.142977, loss: 0.415082\n",
            "val: bce: 0.027695, dice: 0.806808, jaccard: 0.136974, loss: 0.417251\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 169/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028169, dice: 0.802621, jaccard: 0.140031, loss: 0.415395\n",
            "val: bce: 0.030267, dice: 0.806120, jaccard: 0.137692, loss: 0.418193\n",
            "0m 2s\n",
            "Epoch 170/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028449, dice: 0.798028, jaccard: 0.141819, loss: 0.413238\n",
            "val: bce: 0.031776, dice: 0.806931, jaccard: 0.138335, loss: 0.419353\n",
            "0m 2s\n",
            "Epoch 171/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028719, dice: 0.795295, jaccard: 0.144136, loss: 0.412007\n",
            "val: bce: 0.029016, dice: 0.797913, jaccard: 0.141731, loss: 0.413464\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 172/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028955, dice: 0.790540, jaccard: 0.148613, loss: 0.409747\n",
            "val: bce: 0.029239, dice: 0.798865, jaccard: 0.136441, loss: 0.414052\n",
            "0m 2s\n",
            "Epoch 173/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029439, dice: 0.789929, jaccard: 0.146602, loss: 0.409684\n",
            "val: bce: 0.029811, dice: 0.792343, jaccard: 0.143654, loss: 0.411077\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 174/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028924, dice: 0.786757, jaccard: 0.146788, loss: 0.407841\n",
            "val: bce: 0.032054, dice: 0.794883, jaccard: 0.143521, loss: 0.413469\n",
            "0m 2s\n",
            "Epoch 175/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029042, dice: 0.782739, jaccard: 0.148548, loss: 0.405891\n",
            "val: bce: 0.033039, dice: 0.792423, jaccard: 0.142564, loss: 0.412731\n",
            "0m 2s\n",
            "Epoch 176/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029910, dice: 0.780317, jaccard: 0.149406, loss: 0.405113\n",
            "val: bce: 0.029759, dice: 0.782882, jaccard: 0.145781, loss: 0.406320\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 177/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029278, dice: 0.774603, jaccard: 0.154284, loss: 0.401940\n",
            "val: bce: 0.029216, dice: 0.780420, jaccard: 0.146208, loss: 0.404818\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 178/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029428, dice: 0.771479, jaccard: 0.153694, loss: 0.400454\n",
            "val: bce: 0.029519, dice: 0.778547, jaccard: 0.143514, loss: 0.404033\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 179/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.029640, dice: 0.768111, jaccard: 0.152484, loss: 0.398875\n",
            "val: bce: 0.030486, dice: 0.773824, jaccard: 0.148124, loss: 0.402155\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 180/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029203, dice: 0.763508, jaccard: 0.155809, loss: 0.396355\n",
            "val: bce: 0.030614, dice: 0.771245, jaccard: 0.149654, loss: 0.400930\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 181/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029427, dice: 0.760947, jaccard: 0.155521, loss: 0.395187\n",
            "val: bce: 0.032367, dice: 0.770831, jaccard: 0.146629, loss: 0.401599\n",
            "0m 2s\n",
            "Epoch 182/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030242, dice: 0.757914, jaccard: 0.155331, loss: 0.394078\n",
            "val: bce: 0.030989, dice: 0.764420, jaccard: 0.149998, loss: 0.397704\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 183/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029959, dice: 0.752889, jaccard: 0.159626, loss: 0.391424\n",
            "val: bce: 0.030412, dice: 0.761067, jaccard: 0.154808, loss: 0.395739\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 184/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029913, dice: 0.749818, jaccard: 0.160269, loss: 0.389865\n",
            "val: bce: 0.030724, dice: 0.758492, jaccard: 0.150904, loss: 0.394608\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 185/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030318, dice: 0.746717, jaccard: 0.159001, loss: 0.388517\n",
            "val: bce: 0.031501, dice: 0.756484, jaccard: 0.152474, loss: 0.393993\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 186/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030433, dice: 0.744134, jaccard: 0.163204, loss: 0.387283\n",
            "val: bce: 0.031218, dice: 0.753039, jaccard: 0.156901, loss: 0.392129\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 187/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030795, dice: 0.741727, jaccard: 0.163682, loss: 0.386261\n",
            "val: bce: 0.030084, dice: 0.751906, jaccard: 0.154116, loss: 0.390995\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 188/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031165, dice: 0.740470, jaccard: 0.162379, loss: 0.385818\n",
            "val: bce: 0.029889, dice: 0.753503, jaccard: 0.155264, loss: 0.391696\n",
            "0m 2s\n",
            "Epoch 189/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031524, dice: 0.737925, jaccard: 0.166711, loss: 0.384725\n",
            "val: bce: 0.030085, dice: 0.753029, jaccard: 0.156436, loss: 0.391557\n",
            "0m 2s\n",
            "Epoch 190/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031492, dice: 0.737216, jaccard: 0.167926, loss: 0.384354\n",
            "val: bce: 0.030171, dice: 0.748935, jaccard: 0.157122, loss: 0.389553\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 191/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031344, dice: 0.734464, jaccard: 0.167546, loss: 0.382904\n",
            "val: bce: 0.030630, dice: 0.744230, jaccard: 0.161663, loss: 0.387430\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 192/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031284, dice: 0.732503, jaccard: 0.172147, loss: 0.381894\n",
            "val: bce: 0.030508, dice: 0.742368, jaccard: 0.164740, loss: 0.386438\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 193/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031313, dice: 0.729825, jaccard: 0.174586, loss: 0.380569\n",
            "val: bce: 0.030731, dice: 0.739027, jaccard: 0.167533, loss: 0.384879\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 194/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030813, dice: 0.726440, jaccard: 0.177886, loss: 0.378627\n",
            "val: bce: 0.032394, dice: 0.738943, jaccard: 0.165768, loss: 0.385668\n",
            "0m 2s\n",
            "Epoch 195/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031261, dice: 0.725337, jaccard: 0.179201, loss: 0.378299\n",
            "val: bce: 0.032199, dice: 0.737173, jaccard: 0.167607, loss: 0.384686\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 196/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031446, dice: 0.724132, jaccard: 0.181476, loss: 0.377789\n",
            "val: bce: 0.033115, dice: 0.737000, jaccard: 0.168143, loss: 0.385057\n",
            "0m 2s\n",
            "Epoch 197/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031554, dice: 0.722913, jaccard: 0.182650, loss: 0.377233\n",
            "val: bce: 0.032342, dice: 0.734282, jaccard: 0.169711, loss: 0.383312\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 198/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031234, dice: 0.721183, jaccard: 0.183146, loss: 0.376209\n",
            "val: bce: 0.033183, dice: 0.735351, jaccard: 0.168970, loss: 0.384267\n",
            "0m 2s\n",
            "Epoch 199/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031167, dice: 0.717674, jaccard: 0.189316, loss: 0.374420\n",
            "val: bce: 0.032033, dice: 0.731078, jaccard: 0.174145, loss: 0.381555\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 200/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031212, dice: 0.714815, jaccard: 0.190745, loss: 0.373014\n",
            "val: bce: 0.031765, dice: 0.728928, jaccard: 0.174182, loss: 0.380346\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 201/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031210, dice: 0.712924, jaccard: 0.191313, loss: 0.372067\n",
            "val: bce: 0.031421, dice: 0.727327, jaccard: 0.176872, loss: 0.379374\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 202/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031272, dice: 0.711280, jaccard: 0.194011, loss: 0.371276\n",
            "val: bce: 0.031254, dice: 0.726357, jaccard: 0.177993, loss: 0.378805\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 203/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031459, dice: 0.709662, jaccard: 0.194211, loss: 0.370560\n",
            "val: bce: 0.032151, dice: 0.724210, jaccard: 0.178390, loss: 0.378180\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 204/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031659, dice: 0.708606, jaccard: 0.196046, loss: 0.370133\n",
            "val: bce: 0.032104, dice: 0.723229, jaccard: 0.179539, loss: 0.377667\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 205/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031589, dice: 0.707236, jaccard: 0.196679, loss: 0.369413\n",
            "val: bce: 0.031635, dice: 0.722440, jaccard: 0.179993, loss: 0.377037\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 206/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031657, dice: 0.705675, jaccard: 0.197985, loss: 0.368666\n",
            "val: bce: 0.031473, dice: 0.721950, jaccard: 0.182483, loss: 0.376711\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 207/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031686, dice: 0.703382, jaccard: 0.200749, loss: 0.367534\n",
            "val: bce: 0.031180, dice: 0.726456, jaccard: 0.180718, loss: 0.378818\n",
            "0m 2s\n",
            "Epoch 208/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031434, dice: 0.703144, jaccard: 0.201893, loss: 0.367289\n",
            "val: bce: 0.030760, dice: 0.723714, jaccard: 0.181557, loss: 0.377237\n",
            "0m 2s\n",
            "Epoch 209/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.031265, dice: 0.701359, jaccard: 0.202914, loss: 0.366312\n",
            "val: bce: 0.031068, dice: 0.722480, jaccard: 0.183946, loss: 0.376774\n",
            "0m 2s\n",
            "Epoch 210/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031353, dice: 0.699561, jaccard: 0.206161, loss: 0.365457\n",
            "val: bce: 0.031240, dice: 0.718050, jaccard: 0.187298, loss: 0.374645\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 211/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031135, dice: 0.697873, jaccard: 0.207026, loss: 0.364504\n",
            "val: bce: 0.031485, dice: 0.715205, jaccard: 0.188816, loss: 0.373345\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 212/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031310, dice: 0.698047, jaccard: 0.208454, loss: 0.364678\n",
            "val: bce: 0.031964, dice: 0.715156, jaccard: 0.189579, loss: 0.373560\n",
            "0m 2s\n",
            "Epoch 213/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031808, dice: 0.697541, jaccard: 0.208309, loss: 0.364675\n",
            "val: bce: 0.032525, dice: 0.713992, jaccard: 0.191223, loss: 0.373259\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 214/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031685, dice: 0.697604, jaccard: 0.209981, loss: 0.364645\n",
            "val: bce: 0.031920, dice: 0.713096, jaccard: 0.193123, loss: 0.372508\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 215/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031228, dice: 0.695565, jaccard: 0.214105, loss: 0.363396\n",
            "val: bce: 0.031737, dice: 0.711210, jaccard: 0.195805, loss: 0.371474\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 216/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031002, dice: 0.691744, jaccard: 0.218415, loss: 0.361373\n",
            "val: bce: 0.031327, dice: 0.711187, jaccard: 0.196659, loss: 0.371257\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 217/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030501, dice: 0.689713, jaccard: 0.221286, loss: 0.360107\n",
            "val: bce: 0.031268, dice: 0.709877, jaccard: 0.199791, loss: 0.370573\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 218/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030533, dice: 0.689172, jaccard: 0.223902, loss: 0.359853\n",
            "val: bce: 0.031224, dice: 0.709114, jaccard: 0.200722, loss: 0.370169\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 219/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030887, dice: 0.686354, jaccard: 0.226132, loss: 0.358621\n",
            "val: bce: 0.031865, dice: 0.707470, jaccard: 0.202288, loss: 0.369668\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 220/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030779, dice: 0.684489, jaccard: 0.228648, loss: 0.357634\n",
            "val: bce: 0.031652, dice: 0.707733, jaccard: 0.203159, loss: 0.369693\n",
            "0m 2s\n",
            "Epoch 221/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030418, dice: 0.683706, jaccard: 0.231814, loss: 0.357062\n",
            "val: bce: 0.032540, dice: 0.708705, jaccard: 0.204104, loss: 0.370622\n",
            "0m 2s\n",
            "Epoch 222/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031092, dice: 0.684664, jaccard: 0.232107, loss: 0.357878\n",
            "val: bce: 0.033166, dice: 0.709305, jaccard: 0.204736, loss: 0.371236\n",
            "0m 2s\n",
            "Epoch 223/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.031071, dice: 0.683723, jaccard: 0.234931, loss: 0.357397\n",
            "val: bce: 0.032664, dice: 0.707842, jaccard: 0.207873, loss: 0.370253\n",
            "0m 2s\n",
            "Epoch 224/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030746, dice: 0.681687, jaccard: 0.237843, loss: 0.356216\n",
            "val: bce: 0.032906, dice: 0.707797, jaccard: 0.208684, loss: 0.370351\n",
            "0m 2s\n",
            "Epoch 225/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030744, dice: 0.682074, jaccard: 0.237930, loss: 0.356409\n",
            "val: bce: 0.032357, dice: 0.706202, jaccard: 0.211266, loss: 0.369280\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 226/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030789, dice: 0.682385, jaccard: 0.237583, loss: 0.356587\n",
            "val: bce: 0.032057, dice: 0.702849, jaccard: 0.213988, loss: 0.367453\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 227/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030423, dice: 0.676114, jaccard: 0.246886, loss: 0.353269\n",
            "val: bce: 0.030845, dice: 0.705160, jaccard: 0.213561, loss: 0.368003\n",
            "0m 2s\n",
            "Epoch 228/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029848, dice: 0.674674, jaccard: 0.248411, loss: 0.352261\n",
            "val: bce: 0.030789, dice: 0.701395, jaccard: 0.216992, loss: 0.366092\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 229/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029816, dice: 0.672273, jaccard: 0.251012, loss: 0.351044\n",
            "val: bce: 0.031253, dice: 0.699367, jaccard: 0.219055, loss: 0.365310\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 230/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030051, dice: 0.670311, jaccard: 0.252816, loss: 0.350181\n",
            "val: bce: 0.031672, dice: 0.697861, jaccard: 0.221619, loss: 0.364766\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 231/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030023, dice: 0.669633, jaccard: 0.255078, loss: 0.349828\n",
            "val: bce: 0.032139, dice: 0.700123, jaccard: 0.219011, loss: 0.366131\n",
            "0m 2s\n",
            "Epoch 232/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030438, dice: 0.669863, jaccard: 0.253791, loss: 0.350151\n",
            "val: bce: 0.032070, dice: 0.695633, jaccard: 0.223602, loss: 0.363852\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 233/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030606, dice: 0.666687, jaccard: 0.257225, loss: 0.348646\n",
            "val: bce: 0.031801, dice: 0.695112, jaccard: 0.225359, loss: 0.363457\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 234/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030078, dice: 0.664863, jaccard: 0.260643, loss: 0.347470\n",
            "val: bce: 0.031711, dice: 0.694201, jaccard: 0.226526, loss: 0.362956\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 235/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030143, dice: 0.662903, jaccard: 0.263197, loss: 0.346523\n",
            "val: bce: 0.031676, dice: 0.694550, jaccard: 0.226402, loss: 0.363113\n",
            "0m 2s\n",
            "Epoch 236/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030153, dice: 0.661794, jaccard: 0.263181, loss: 0.345974\n",
            "val: bce: 0.031741, dice: 0.692479, jaccard: 0.228783, loss: 0.362110\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 237/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030006, dice: 0.659043, jaccard: 0.267278, loss: 0.344525\n",
            "val: bce: 0.031455, dice: 0.692641, jaccard: 0.230046, loss: 0.362048\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 238/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029935, dice: 0.658206, jaccard: 0.268513, loss: 0.344070\n",
            "val: bce: 0.032173, dice: 0.690231, jaccard: 0.231191, loss: 0.361202\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 239/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.030240, dice: 0.655669, jaccard: 0.271135, loss: 0.342955\n",
            "val: bce: 0.031944, dice: 0.689848, jaccard: 0.232611, loss: 0.360896\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 240/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029902, dice: 0.654871, jaccard: 0.272514, loss: 0.342387\n",
            "val: bce: 0.032202, dice: 0.689824, jaccard: 0.231989, loss: 0.361013\n",
            "0m 2s\n",
            "Epoch 241/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029966, dice: 0.652262, jaccard: 0.274690, loss: 0.341114\n",
            "val: bce: 0.032082, dice: 0.688185, jaccard: 0.233680, loss: 0.360133\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 242/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029934, dice: 0.650373, jaccard: 0.276562, loss: 0.340154\n",
            "val: bce: 0.032068, dice: 0.686072, jaccard: 0.237680, loss: 0.359070\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 243/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029956, dice: 0.648588, jaccard: 0.278160, loss: 0.339272\n",
            "val: bce: 0.031962, dice: 0.686112, jaccard: 0.238251, loss: 0.359037\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 244/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029896, dice: 0.646452, jaccard: 0.281254, loss: 0.338174\n",
            "val: bce: 0.032123, dice: 0.685259, jaccard: 0.239508, loss: 0.358691\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 245/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029825, dice: 0.644841, jaccard: 0.283708, loss: 0.337333\n",
            "val: bce: 0.032220, dice: 0.683193, jaccard: 0.241029, loss: 0.357706\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 246/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029844, dice: 0.642440, jaccard: 0.285169, loss: 0.336142\n",
            "val: bce: 0.032482, dice: 0.682903, jaccard: 0.239750, loss: 0.357693\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 247/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030212, dice: 0.643474, jaccard: 0.283289, loss: 0.336843\n",
            "val: bce: 0.032834, dice: 0.691574, jaccard: 0.237460, loss: 0.362204\n",
            "0m 2s\n",
            "Epoch 248/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030119, dice: 0.645134, jaccard: 0.281165, loss: 0.337626\n",
            "val: bce: 0.032706, dice: 0.680311, jaccard: 0.242416, loss: 0.356509\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 249/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.030431, dice: 0.643122, jaccard: 0.282858, loss: 0.336776\n",
            "val: bce: 0.032136, dice: 0.683648, jaccard: 0.238839, loss: 0.357892\n",
            "0m 2s\n",
            "Epoch 250/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029665, dice: 0.641546, jaccard: 0.285603, loss: 0.335606\n",
            "val: bce: 0.032434, dice: 0.678466, jaccard: 0.243457, loss: 0.355450\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 251/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029564, dice: 0.636728, jaccard: 0.289958, loss: 0.333146\n",
            "val: bce: 0.031737, dice: 0.678592, jaccard: 0.247493, loss: 0.355165\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 252/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029182, dice: 0.634122, jaccard: 0.291392, loss: 0.331652\n",
            "val: bce: 0.032049, dice: 0.680917, jaccard: 0.247393, loss: 0.356483\n",
            "0m 2s\n",
            "Epoch 253/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029130, dice: 0.633189, jaccard: 0.293635, loss: 0.331159\n",
            "val: bce: 0.032248, dice: 0.680603, jaccard: 0.246912, loss: 0.356426\n",
            "0m 2s\n",
            "Epoch 254/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029385, dice: 0.633837, jaccard: 0.292167, loss: 0.331611\n",
            "val: bce: 0.032177, dice: 0.673455, jaccard: 0.248775, loss: 0.352816\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 255/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029237, dice: 0.626772, jaccard: 0.297902, loss: 0.328004\n",
            "val: bce: 0.031620, dice: 0.674576, jaccard: 0.249016, loss: 0.353098\n",
            "0m 2s\n",
            "Epoch 256/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028993, dice: 0.625762, jaccard: 0.298667, loss: 0.327377\n",
            "val: bce: 0.032287, dice: 0.672256, jaccard: 0.248182, loss: 0.352272\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 257/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029222, dice: 0.625199, jaccard: 0.298150, loss: 0.327211\n",
            "val: bce: 0.031687, dice: 0.671529, jaccard: 0.252171, loss: 0.351608\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 258/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029121, dice: 0.622158, jaccard: 0.301765, loss: 0.325639\n",
            "val: bce: 0.031880, dice: 0.669895, jaccard: 0.252643, loss: 0.350888\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 259/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028874, dice: 0.622640, jaccard: 0.300709, loss: 0.325757\n",
            "val: bce: 0.031804, dice: 0.670114, jaccard: 0.251297, loss: 0.350959\n",
            "0m 2s\n",
            "Epoch 260/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029557, dice: 0.620229, jaccard: 0.300692, loss: 0.324893\n",
            "val: bce: 0.033008, dice: 0.671357, jaccard: 0.245593, loss: 0.352183\n",
            "0m 2s\n",
            "Epoch 261/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029640, dice: 0.623607, jaccard: 0.298532, loss: 0.326623\n",
            "val: bce: 0.032659, dice: 0.678361, jaccard: 0.239322, loss: 0.355510\n",
            "0m 2s\n",
            "Epoch 262/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029143, dice: 0.625507, jaccard: 0.295179, loss: 0.327325\n",
            "val: bce: 0.032642, dice: 0.672823, jaccard: 0.242532, loss: 0.352733\n",
            "0m 2s\n",
            "Epoch 263/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029759, dice: 0.622612, jaccard: 0.297866, loss: 0.326185\n",
            "val: bce: 0.032129, dice: 0.669989, jaccard: 0.245893, loss: 0.351059\n",
            "0m 2s\n",
            "Epoch 264/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.029501, dice: 0.624557, jaccard: 0.295617, loss: 0.327029\n",
            "val: bce: 0.031371, dice: 0.664584, jaccard: 0.251207, loss: 0.347978\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 265/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028649, dice: 0.619230, jaccard: 0.299871, loss: 0.323939\n",
            "val: bce: 0.031444, dice: 0.663588, jaccard: 0.251642, loss: 0.347516\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 266/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028746, dice: 0.613772, jaccard: 0.303673, loss: 0.321259\n",
            "val: bce: 0.031601, dice: 0.663822, jaccard: 0.251291, loss: 0.347712\n",
            "0m 2s\n",
            "Epoch 267/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028475, dice: 0.611989, jaccard: 0.304606, loss: 0.320232\n",
            "val: bce: 0.031405, dice: 0.663415, jaccard: 0.251842, loss: 0.347410\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 268/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028427, dice: 0.609771, jaccard: 0.307280, loss: 0.319099\n",
            "val: bce: 0.031194, dice: 0.660641, jaccard: 0.255239, loss: 0.345917\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 269/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.028279, dice: 0.605097, jaccard: 0.311712, loss: 0.316688\n",
            "val: bce: 0.031005, dice: 0.659025, jaccard: 0.257518, loss: 0.345015\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 270/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028046, dice: 0.606179, jaccard: 0.310377, loss: 0.317112\n",
            "val: bce: 0.031616, dice: 0.656860, jaccard: 0.257942, loss: 0.344238\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 271/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028723, dice: 0.604928, jaccard: 0.309059, loss: 0.316825\n",
            "val: bce: 0.031371, dice: 0.660611, jaccard: 0.254496, loss: 0.345991\n",
            "0m 2s\n",
            "Epoch 272/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028254, dice: 0.604175, jaccard: 0.310280, loss: 0.316214\n",
            "val: bce: 0.031694, dice: 0.655461, jaccard: 0.259369, loss: 0.343578\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 273/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028531, dice: 0.602870, jaccard: 0.311944, loss: 0.315700\n",
            "val: bce: 0.031764, dice: 0.655273, jaccard: 0.260671, loss: 0.343519\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 274/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028445, dice: 0.601296, jaccard: 0.312161, loss: 0.314871\n",
            "val: bce: 0.032087, dice: 0.652644, jaccard: 0.263676, loss: 0.342366\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 275/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028632, dice: 0.598163, jaccard: 0.313910, loss: 0.313397\n",
            "val: bce: 0.031513, dice: 0.652864, jaccard: 0.262269, loss: 0.342188\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 276/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027933, dice: 0.597499, jaccard: 0.315807, loss: 0.312716\n",
            "val: bce: 0.031844, dice: 0.654140, jaccard: 0.263310, loss: 0.342992\n",
            "0m 2s\n",
            "Epoch 277/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028036, dice: 0.596244, jaccard: 0.316100, loss: 0.312140\n",
            "val: bce: 0.031550, dice: 0.650990, jaccard: 0.263732, loss: 0.341270\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 278/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028279, dice: 0.594664, jaccard: 0.317067, loss: 0.311472\n",
            "val: bce: 0.032028, dice: 0.650964, jaccard: 0.264320, loss: 0.341496\n",
            "0m 2s\n",
            "Epoch 279/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028139, dice: 0.594889, jaccard: 0.317284, loss: 0.311514\n",
            "val: bce: 0.031102, dice: 0.648930, jaccard: 0.263586, loss: 0.340016\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 280/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027818, dice: 0.590913, jaccard: 0.320591, loss: 0.309365\n",
            "val: bce: 0.031277, dice: 0.648942, jaccard: 0.264030, loss: 0.340110\n",
            "0m 2s\n",
            "Epoch 281/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028083, dice: 0.589451, jaccard: 0.321564, loss: 0.308767\n",
            "val: bce: 0.031143, dice: 0.647163, jaccard: 0.263369, loss: 0.339153\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 282/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027834, dice: 0.589169, jaccard: 0.321955, loss: 0.308502\n",
            "val: bce: 0.031212, dice: 0.653528, jaccard: 0.256762, loss: 0.342370\n",
            "0m 2s\n",
            "Epoch 283/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027947, dice: 0.590527, jaccard: 0.321137, loss: 0.309237\n",
            "val: bce: 0.031420, dice: 0.646092, jaccard: 0.264690, loss: 0.338756\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 284/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028061, dice: 0.587195, jaccard: 0.322363, loss: 0.307628\n",
            "val: bce: 0.030975, dice: 0.644033, jaccard: 0.266038, loss: 0.337504\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 285/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027636, dice: 0.584677, jaccard: 0.324688, loss: 0.306157\n",
            "val: bce: 0.030900, dice: 0.641758, jaccard: 0.268514, loss: 0.336329\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 286/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027631, dice: 0.584210, jaccard: 0.326581, loss: 0.305920\n",
            "val: bce: 0.031092, dice: 0.642636, jaccard: 0.267935, loss: 0.336864\n",
            "0m 2s\n",
            "Epoch 287/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027722, dice: 0.582445, jaccard: 0.326843, loss: 0.305083\n",
            "val: bce: 0.030945, dice: 0.641827, jaccard: 0.267164, loss: 0.336386\n",
            "0m 2s\n",
            "Epoch 288/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027640, dice: 0.582838, jaccard: 0.326950, loss: 0.305239\n",
            "val: bce: 0.030943, dice: 0.639228, jaccard: 0.270227, loss: 0.335086\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 289/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027544, dice: 0.582306, jaccard: 0.327481, loss: 0.304925\n",
            "val: bce: 0.031168, dice: 0.637828, jaccard: 0.272001, loss: 0.334498\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 290/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027605, dice: 0.579157, jaccard: 0.330052, loss: 0.303381\n",
            "val: bce: 0.031687, dice: 0.636551, jaccard: 0.271194, loss: 0.334119\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 291/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027678, dice: 0.578676, jaccard: 0.330070, loss: 0.303177\n",
            "val: bce: 0.031006, dice: 0.635601, jaccard: 0.273043, loss: 0.333303\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 292/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027420, dice: 0.576936, jaccard: 0.332937, loss: 0.302178\n",
            "val: bce: 0.030995, dice: 0.633965, jaccard: 0.273766, loss: 0.332480\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 293/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027726, dice: 0.576492, jaccard: 0.332140, loss: 0.302109\n",
            "val: bce: 0.031156, dice: 0.642447, jaccard: 0.264750, loss: 0.336801\n",
            "0m 2s\n",
            "Epoch 294/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027740, dice: 0.577674, jaccard: 0.331274, loss: 0.302707\n",
            "val: bce: 0.031252, dice: 0.633450, jaccard: 0.273024, loss: 0.332351\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 295/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027667, dice: 0.573544, jaccard: 0.335069, loss: 0.300605\n",
            "val: bce: 0.030935, dice: 0.637608, jaccard: 0.267811, loss: 0.334271\n",
            "0m 2s\n",
            "Epoch 296/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027310, dice: 0.574112, jaccard: 0.334501, loss: 0.300711\n",
            "val: bce: 0.030728, dice: 0.630694, jaccard: 0.276170, loss: 0.330711\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 297/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027113, dice: 0.571646, jaccard: 0.337759, loss: 0.299380\n",
            "val: bce: 0.031226, dice: 0.628663, jaccard: 0.278290, loss: 0.329945\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 298/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027576, dice: 0.571425, jaccard: 0.335275, loss: 0.299501\n",
            "val: bce: 0.031340, dice: 0.630795, jaccard: 0.274071, loss: 0.331068\n",
            "0m 2s\n",
            "Epoch 299/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.027262, dice: 0.570802, jaccard: 0.336108, loss: 0.299032\n",
            "val: bce: 0.031259, dice: 0.628066, jaccard: 0.277525, loss: 0.329662\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 300/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027706, dice: 0.570317, jaccard: 0.337602, loss: 0.299012\n",
            "val: bce: 0.030738, dice: 0.632433, jaccard: 0.272767, loss: 0.331586\n",
            "0m 2s\n",
            "Epoch 301/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027493, dice: 0.571695, jaccard: 0.335761, loss: 0.299594\n",
            "val: bce: 0.030841, dice: 0.640841, jaccard: 0.264562, loss: 0.335841\n",
            "0m 2s\n",
            "Epoch 302/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027764, dice: 0.576233, jaccard: 0.329585, loss: 0.301998\n",
            "val: bce: 0.031322, dice: 0.624906, jaccard: 0.278933, loss: 0.328114\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 303/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027844, dice: 0.571632, jaccard: 0.332804, loss: 0.299738\n",
            "val: bce: 0.032634, dice: 0.632035, jaccard: 0.276453, loss: 0.332334\n",
            "0m 2s\n",
            "Epoch 304/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027607, dice: 0.572998, jaccard: 0.334204, loss: 0.300303\n",
            "val: bce: 0.031471, dice: 0.625444, jaccard: 0.278263, loss: 0.328457\n",
            "0m 2s\n",
            "Epoch 305/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027374, dice: 0.565891, jaccard: 0.339035, loss: 0.296632\n",
            "val: bce: 0.031253, dice: 0.623068, jaccard: 0.280776, loss: 0.327161\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 306/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027112, dice: 0.562659, jaccard: 0.342975, loss: 0.294885\n",
            "val: bce: 0.030213, dice: 0.625668, jaccard: 0.279613, loss: 0.327940\n",
            "0m 2s\n",
            "Epoch 307/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026459, dice: 0.560968, jaccard: 0.345004, loss: 0.293714\n",
            "val: bce: 0.030207, dice: 0.625057, jaccard: 0.278096, loss: 0.327632\n",
            "0m 2s\n",
            "Epoch 308/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027140, dice: 0.563099, jaccard: 0.342277, loss: 0.295120\n",
            "val: bce: 0.030587, dice: 0.637096, jaccard: 0.267279, loss: 0.333842\n",
            "0m 2s\n",
            "Epoch 309/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027295, dice: 0.570404, jaccard: 0.335366, loss: 0.298849\n",
            "val: bce: 0.030649, dice: 0.620738, jaccard: 0.283311, loss: 0.325693\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 310/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026874, dice: 0.568507, jaccard: 0.336003, loss: 0.297690\n",
            "val: bce: 0.034052, dice: 0.635233, jaccard: 0.274454, loss: 0.334642\n",
            "0m 2s\n",
            "Epoch 311/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028663, dice: 0.572301, jaccard: 0.332362, loss: 0.300482\n",
            "val: bce: 0.034377, dice: 0.631003, jaccard: 0.277493, loss: 0.332690\n",
            "0m 2s\n",
            "Epoch 312/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028169, dice: 0.571635, jaccard: 0.331503, loss: 0.299902\n",
            "val: bce: 0.031032, dice: 0.620373, jaccard: 0.282218, loss: 0.325703\n",
            "0m 2s\n",
            "Epoch 313/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027555, dice: 0.568016, jaccard: 0.333573, loss: 0.297786\n",
            "val: bce: 0.032006, dice: 0.615931, jaccard: 0.286632, loss: 0.323968\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 314/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.028058, dice: 0.564778, jaccard: 0.337189, loss: 0.296418\n",
            "val: bce: 0.030175, dice: 0.620577, jaccard: 0.279761, loss: 0.325376\n",
            "0m 2s\n",
            "Epoch 315/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026927, dice: 0.561094, jaccard: 0.341401, loss: 0.294011\n",
            "val: bce: 0.030041, dice: 0.628548, jaccard: 0.272460, loss: 0.329295\n",
            "0m 2s\n",
            "Epoch 316/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026710, dice: 0.560552, jaccard: 0.341647, loss: 0.293631\n",
            "val: bce: 0.030828, dice: 0.647616, jaccard: 0.257695, loss: 0.339222\n",
            "0m 2s\n",
            "Epoch 317/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.027682, dice: 0.565969, jaccard: 0.334764, loss: 0.296826\n",
            "val: bce: 0.030506, dice: 0.628462, jaccard: 0.272051, loss: 0.329484\n",
            "0m 2s\n",
            "Epoch 318/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026602, dice: 0.558900, jaccard: 0.343307, loss: 0.292751\n",
            "val: bce: 0.029410, dice: 0.620749, jaccard: 0.282596, loss: 0.325080\n",
            "0m 2s\n",
            "Epoch 319/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026429, dice: 0.554541, jaccard: 0.348556, loss: 0.290485\n",
            "val: bce: 0.030863, dice: 0.617229, jaccard: 0.285856, loss: 0.324046\n",
            "0m 2s\n",
            "Epoch 320/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026412, dice: 0.552989, jaccard: 0.347991, loss: 0.289700\n",
            "val: bce: 0.030230, dice: 0.615486, jaccard: 0.286951, loss: 0.322858\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 321/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026000, dice: 0.550644, jaccard: 0.350067, loss: 0.288322\n",
            "val: bce: 0.030194, dice: 0.613633, jaccard: 0.289646, loss: 0.321913\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 322/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026108, dice: 0.547127, jaccard: 0.352841, loss: 0.286617\n",
            "val: bce: 0.030418, dice: 0.615590, jaccard: 0.286647, loss: 0.323004\n",
            "0m 2s\n",
            "Epoch 323/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026132, dice: 0.545920, jaccard: 0.352922, loss: 0.286026\n",
            "val: bce: 0.030093, dice: 0.615864, jaccard: 0.287076, loss: 0.322978\n",
            "0m 2s\n",
            "Epoch 324/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025974, dice: 0.544483, jaccard: 0.355761, loss: 0.285229\n",
            "val: bce: 0.030362, dice: 0.614043, jaccard: 0.289477, loss: 0.322202\n",
            "0m 2s\n",
            "Epoch 325/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026274, dice: 0.543925, jaccard: 0.354967, loss: 0.285100\n",
            "val: bce: 0.030417, dice: 0.614284, jaccard: 0.287168, loss: 0.322351\n",
            "0m 2s\n",
            "Epoch 326/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026154, dice: 0.543207, jaccard: 0.354977, loss: 0.284681\n",
            "val: bce: 0.030395, dice: 0.614172, jaccard: 0.287774, loss: 0.322283\n",
            "0m 2s\n",
            "Epoch 327/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026363, dice: 0.540745, jaccard: 0.357354, loss: 0.283554\n",
            "val: bce: 0.030494, dice: 0.613944, jaccard: 0.286861, loss: 0.322219\n",
            "0m 2s\n",
            "Epoch 328/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026226, dice: 0.539486, jaccard: 0.357350, loss: 0.282856\n",
            "val: bce: 0.030735, dice: 0.613497, jaccard: 0.288456, loss: 0.322116\n",
            "0m 2s\n",
            "Epoch 329/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.026392, dice: 0.539161, jaccard: 0.357308, loss: 0.282777\n",
            "val: bce: 0.030691, dice: 0.613170, jaccard: 0.289082, loss: 0.321931\n",
            "0m 2s\n",
            "Epoch 330/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026114, dice: 0.537940, jaccard: 0.358850, loss: 0.282027\n",
            "val: bce: 0.031044, dice: 0.612854, jaccard: 0.289695, loss: 0.321949\n",
            "0m 2s\n",
            "Epoch 331/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026428, dice: 0.537595, jaccard: 0.358177, loss: 0.282011\n",
            "val: bce: 0.030830, dice: 0.611020, jaccard: 0.289400, loss: 0.320925\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 332/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026254, dice: 0.535679, jaccard: 0.359544, loss: 0.280967\n",
            "val: bce: 0.030549, dice: 0.613722, jaccard: 0.288237, loss: 0.322136\n",
            "0m 2s\n",
            "Epoch 333/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026447, dice: 0.535122, jaccard: 0.359859, loss: 0.280784\n",
            "val: bce: 0.030605, dice: 0.614117, jaccard: 0.285892, loss: 0.322361\n",
            "0m 2s\n",
            "Epoch 334/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026061, dice: 0.535339, jaccard: 0.359073, loss: 0.280700\n",
            "val: bce: 0.030861, dice: 0.609554, jaccard: 0.290882, loss: 0.320207\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 335/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026466, dice: 0.533408, jaccard: 0.361106, loss: 0.279937\n",
            "val: bce: 0.030939, dice: 0.613010, jaccard: 0.289197, loss: 0.321974\n",
            "0m 2s\n",
            "Epoch 336/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026133, dice: 0.533481, jaccard: 0.360514, loss: 0.279807\n",
            "val: bce: 0.030670, dice: 0.609580, jaccard: 0.290457, loss: 0.320125\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 337/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026270, dice: 0.532021, jaccard: 0.361406, loss: 0.279146\n",
            "val: bce: 0.030770, dice: 0.616553, jaccard: 0.283262, loss: 0.323661\n",
            "0m 2s\n",
            "Epoch 338/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026166, dice: 0.531651, jaccard: 0.362415, loss: 0.278909\n",
            "val: bce: 0.030645, dice: 0.609575, jaccard: 0.290869, loss: 0.320110\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 339/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026175, dice: 0.531942, jaccard: 0.361430, loss: 0.279059\n",
            "val: bce: 0.031386, dice: 0.608328, jaccard: 0.289948, loss: 0.319857\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 340/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026753, dice: 0.530224, jaccard: 0.361606, loss: 0.278488\n",
            "val: bce: 0.030951, dice: 0.611706, jaccard: 0.288459, loss: 0.321328\n",
            "0m 2s\n",
            "Epoch 341/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026033, dice: 0.529351, jaccard: 0.362488, loss: 0.277692\n",
            "val: bce: 0.030468, dice: 0.612013, jaccard: 0.287744, loss: 0.321240\n",
            "0m 2s\n",
            "Epoch 342/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026414, dice: 0.528641, jaccard: 0.363848, loss: 0.277527\n",
            "val: bce: 0.031842, dice: 0.606252, jaccard: 0.292729, loss: 0.319047\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 343/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026267, dice: 0.527573, jaccard: 0.364220, loss: 0.276920\n",
            "val: bce: 0.030374, dice: 0.609284, jaccard: 0.290616, loss: 0.319829\n",
            "0m 2s\n",
            "Epoch 344/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025941, dice: 0.526383, jaccard: 0.364639, loss: 0.276162\n",
            "val: bce: 0.031456, dice: 0.607879, jaccard: 0.291984, loss: 0.319667\n",
            "0m 2s\n",
            "Epoch 345/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026602, dice: 0.526970, jaccard: 0.364235, loss: 0.276786\n",
            "val: bce: 0.030511, dice: 0.614568, jaccard: 0.284914, loss: 0.322539\n",
            "0m 2s\n",
            "Epoch 346/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025898, dice: 0.526628, jaccard: 0.364249, loss: 0.276263\n",
            "val: bce: 0.031161, dice: 0.605442, jaccard: 0.293634, loss: 0.318302\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 347/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026302, dice: 0.524038, jaccard: 0.366160, loss: 0.275170\n",
            "val: bce: 0.031257, dice: 0.609977, jaccard: 0.289843, loss: 0.320617\n",
            "0m 2s\n",
            "Epoch 348/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026102, dice: 0.524268, jaccard: 0.366136, loss: 0.275185\n",
            "val: bce: 0.030702, dice: 0.612138, jaccard: 0.287033, loss: 0.321420\n",
            "0m 2s\n",
            "Epoch 349/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026081, dice: 0.524263, jaccard: 0.365874, loss: 0.275172\n",
            "val: bce: 0.031691, dice: 0.607406, jaccard: 0.291732, loss: 0.319548\n",
            "0m 2s\n",
            "Epoch 350/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026424, dice: 0.523001, jaccard: 0.366557, loss: 0.274712\n",
            "val: bce: 0.030569, dice: 0.609905, jaccard: 0.288898, loss: 0.320237\n",
            "0m 2s\n",
            "Epoch 351/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025914, dice: 0.521740, jaccard: 0.367722, loss: 0.273827\n",
            "val: bce: 0.030886, dice: 0.606321, jaccard: 0.292520, loss: 0.318604\n",
            "0m 2s\n",
            "Epoch 352/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026093, dice: 0.520275, jaccard: 0.368906, loss: 0.273184\n",
            "val: bce: 0.030936, dice: 0.609753, jaccard: 0.290107, loss: 0.320345\n",
            "0m 2s\n",
            "Epoch 353/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025914, dice: 0.520583, jaccard: 0.368418, loss: 0.273248\n",
            "val: bce: 0.031436, dice: 0.603488, jaccard: 0.295740, loss: 0.317462\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 354/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026292, dice: 0.521608, jaccard: 0.368914, loss: 0.273950\n",
            "val: bce: 0.030555, dice: 0.609789, jaccard: 0.290010, loss: 0.320172\n",
            "0m 2s\n",
            "Epoch 355/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025829, dice: 0.521471, jaccard: 0.367146, loss: 0.273650\n",
            "val: bce: 0.030520, dice: 0.610672, jaccard: 0.287301, loss: 0.320596\n",
            "0m 2s\n",
            "Epoch 356/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026209, dice: 0.521986, jaccard: 0.367604, loss: 0.274098\n",
            "val: bce: 0.032556, dice: 0.604479, jaccard: 0.295571, loss: 0.318518\n",
            "0m 2s\n",
            "Epoch 357/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026474, dice: 0.523963, jaccard: 0.366903, loss: 0.275218\n",
            "val: bce: 0.031060, dice: 0.602487, jaccard: 0.295876, loss: 0.316774\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 358/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026680, dice: 0.525465, jaccard: 0.364820, loss: 0.276072\n",
            "val: bce: 0.031427, dice: 0.617256, jaccard: 0.280129, loss: 0.324341\n",
            "0m 2s\n",
            "Epoch 359/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.026819, dice: 0.524386, jaccard: 0.363837, loss: 0.275603\n",
            "val: bce: 0.030769, dice: 0.609344, jaccard: 0.289210, loss: 0.320057\n",
            "0m 2s\n",
            "Epoch 360/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026084, dice: 0.520687, jaccard: 0.366643, loss: 0.273385\n",
            "val: bce: 0.030735, dice: 0.601943, jaccard: 0.295346, loss: 0.316339\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 361/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025918, dice: 0.519907, jaccard: 0.367034, loss: 0.272913\n",
            "val: bce: 0.032089, dice: 0.600854, jaccard: 0.297722, loss: 0.316472\n",
            "0m 2s\n",
            "Epoch 362/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026167, dice: 0.522913, jaccard: 0.366413, loss: 0.274540\n",
            "val: bce: 0.030576, dice: 0.604208, jaccard: 0.294190, loss: 0.317392\n",
            "0m 2s\n",
            "Epoch 363/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026296, dice: 0.528008, jaccard: 0.362792, loss: 0.277152\n",
            "val: bce: 0.030944, dice: 0.617818, jaccard: 0.278308, loss: 0.324381\n",
            "0m 2s\n",
            "Epoch 364/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026331, dice: 0.523055, jaccard: 0.364848, loss: 0.274693\n",
            "val: bce: 0.030930, dice: 0.613259, jaccard: 0.285855, loss: 0.322094\n",
            "0m 2s\n",
            "Epoch 365/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.026053, dice: 0.522374, jaccard: 0.367021, loss: 0.274213\n",
            "val: bce: 0.030565, dice: 0.605600, jaccard: 0.292161, loss: 0.318082\n",
            "0m 2s\n",
            "Epoch 366/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025540, dice: 0.515879, jaccard: 0.369793, loss: 0.270709\n",
            "val: bce: 0.030944, dice: 0.601060, jaccard: 0.295707, loss: 0.316002\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 367/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025728, dice: 0.513804, jaccard: 0.372315, loss: 0.269766\n",
            "val: bce: 0.030253, dice: 0.607378, jaccard: 0.293266, loss: 0.318815\n",
            "0m 2s\n",
            "Epoch 368/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025053, dice: 0.513312, jaccard: 0.373194, loss: 0.269182\n",
            "val: bce: 0.031263, dice: 0.598855, jaccard: 0.299165, loss: 0.315059\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 369/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025408, dice: 0.513618, jaccard: 0.375215, loss: 0.269513\n",
            "val: bce: 0.031321, dice: 0.603850, jaccard: 0.294333, loss: 0.317585\n",
            "0m 2s\n",
            "Epoch 370/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025122, dice: 0.512822, jaccard: 0.374173, loss: 0.268972\n",
            "val: bce: 0.031502, dice: 0.600036, jaccard: 0.299542, loss: 0.315769\n",
            "0m 2s\n",
            "Epoch 371/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025615, dice: 0.513301, jaccard: 0.374749, loss: 0.269458\n",
            "val: bce: 0.030990, dice: 0.602036, jaccard: 0.296472, loss: 0.316513\n",
            "0m 2s\n",
            "Epoch 372/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025695, dice: 0.510106, jaccard: 0.376360, loss: 0.267901\n",
            "val: bce: 0.030720, dice: 0.603361, jaccard: 0.295030, loss: 0.317041\n",
            "0m 2s\n",
            "Epoch 373/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025453, dice: 0.510477, jaccard: 0.375657, loss: 0.267965\n",
            "val: bce: 0.030394, dice: 0.608854, jaccard: 0.290004, loss: 0.319624\n",
            "0m 2s\n",
            "Epoch 374/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025472, dice: 0.510140, jaccard: 0.377643, loss: 0.267806\n",
            "val: bce: 0.030518, dice: 0.616213, jaccard: 0.283357, loss: 0.323366\n",
            "0m 2s\n",
            "Epoch 375/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025795, dice: 0.516199, jaccard: 0.372077, loss: 0.270997\n",
            "val: bce: 0.030414, dice: 0.607002, jaccard: 0.291103, loss: 0.318708\n",
            "0m 2s\n",
            "Epoch 376/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025528, dice: 0.519575, jaccard: 0.370242, loss: 0.272552\n",
            "val: bce: 0.032580, dice: 0.604644, jaccard: 0.296857, loss: 0.318612\n",
            "0m 2s\n",
            "Epoch 377/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025626, dice: 0.515266, jaccard: 0.373348, loss: 0.270446\n",
            "val: bce: 0.032181, dice: 0.598072, jaccard: 0.300451, loss: 0.315126\n",
            "0m 2s\n",
            "Epoch 378/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025393, dice: 0.509440, jaccard: 0.377309, loss: 0.267417\n",
            "val: bce: 0.030959, dice: 0.598686, jaccard: 0.300017, loss: 0.314823\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 379/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025164, dice: 0.507037, jaccard: 0.378977, loss: 0.266101\n",
            "val: bce: 0.031125, dice: 0.599681, jaccard: 0.299210, loss: 0.315403\n",
            "0m 2s\n",
            "Epoch 380/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024931, dice: 0.508311, jaccard: 0.378225, loss: 0.266621\n",
            "val: bce: 0.030625, dice: 0.598327, jaccard: 0.299030, loss: 0.314476\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 381/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024836, dice: 0.507053, jaccard: 0.381298, loss: 0.265944\n",
            "val: bce: 0.030196, dice: 0.604177, jaccard: 0.297610, loss: 0.317187\n",
            "0m 2s\n",
            "Epoch 382/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024836, dice: 0.505434, jaccard: 0.385174, loss: 0.265135\n",
            "val: bce: 0.030043, dice: 0.603981, jaccard: 0.295982, loss: 0.317012\n",
            "0m 2s\n",
            "Epoch 383/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024648, dice: 0.505560, jaccard: 0.382085, loss: 0.265104\n",
            "val: bce: 0.030775, dice: 0.598524, jaccard: 0.300543, loss: 0.314649\n",
            "0m 2s\n",
            "Epoch 384/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025060, dice: 0.503939, jaccard: 0.383295, loss: 0.264500\n",
            "val: bce: 0.031288, dice: 0.597782, jaccard: 0.302368, loss: 0.314535\n",
            "0m 2s\n",
            "Epoch 385/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024696, dice: 0.502693, jaccard: 0.385630, loss: 0.263694\n",
            "val: bce: 0.030820, dice: 0.599517, jaccard: 0.300912, loss: 0.315168\n",
            "0m 2s\n",
            "Epoch 386/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024644, dice: 0.502117, jaccard: 0.387255, loss: 0.263380\n",
            "val: bce: 0.030762, dice: 0.598812, jaccard: 0.302933, loss: 0.314787\n",
            "0m 2s\n",
            "Epoch 387/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024582, dice: 0.500537, jaccard: 0.389920, loss: 0.262559\n",
            "val: bce: 0.030482, dice: 0.597032, jaccard: 0.303950, loss: 0.313757\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 388/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024534, dice: 0.499612, jaccard: 0.391865, loss: 0.262073\n",
            "val: bce: 0.030550, dice: 0.600335, jaccard: 0.301537, loss: 0.315443\n",
            "0m 2s\n",
            "Epoch 389/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.024524, dice: 0.498533, jaccard: 0.392844, loss: 0.261529\n",
            "val: bce: 0.030396, dice: 0.598543, jaccard: 0.303467, loss: 0.314469\n",
            "0m 2s\n",
            "Epoch 390/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024464, dice: 0.497824, jaccard: 0.393699, loss: 0.261144\n",
            "val: bce: 0.030675, dice: 0.597607, jaccard: 0.305312, loss: 0.314141\n",
            "0m 2s\n",
            "Epoch 391/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024696, dice: 0.496055, jaccard: 0.396099, loss: 0.260375\n",
            "val: bce: 0.030960, dice: 0.596870, jaccard: 0.305263, loss: 0.313915\n",
            "0m 2s\n",
            "Epoch 392/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024543, dice: 0.495396, jaccard: 0.397550, loss: 0.259970\n",
            "val: bce: 0.030569, dice: 0.595667, jaccard: 0.306008, loss: 0.313118\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 393/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024480, dice: 0.494008, jaccard: 0.397900, loss: 0.259244\n",
            "val: bce: 0.030630, dice: 0.599275, jaccard: 0.303382, loss: 0.314952\n",
            "0m 2s\n",
            "Epoch 394/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024528, dice: 0.493416, jaccard: 0.400146, loss: 0.258972\n",
            "val: bce: 0.030739, dice: 0.594216, jaccard: 0.307938, loss: 0.312478\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 395/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024445, dice: 0.493772, jaccard: 0.400563, loss: 0.259108\n",
            "val: bce: 0.030987, dice: 0.592546, jaccard: 0.308823, loss: 0.311767\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 396/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024575, dice: 0.492794, jaccard: 0.401101, loss: 0.258684\n",
            "val: bce: 0.030463, dice: 0.612335, jaccard: 0.293562, loss: 0.321399\n",
            "0m 2s\n",
            "Epoch 397/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024725, dice: 0.498673, jaccard: 0.394821, loss: 0.261699\n",
            "val: bce: 0.033173, dice: 0.590320, jaccard: 0.309611, loss: 0.311747\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 398/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025081, dice: 0.496371, jaccard: 0.397334, loss: 0.260726\n",
            "val: bce: 0.031311, dice: 0.596884, jaccard: 0.305813, loss: 0.314097\n",
            "0m 2s\n",
            "Epoch 399/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025114, dice: 0.490640, jaccard: 0.401666, loss: 0.257877\n",
            "val: bce: 0.030779, dice: 0.593434, jaccard: 0.308837, loss: 0.312107\n",
            "0m 2s\n",
            "Epoch 400/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024225, dice: 0.489554, jaccard: 0.402458, loss: 0.256889\n",
            "val: bce: 0.031007, dice: 0.589666, jaccard: 0.310868, loss: 0.310337\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 401/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024541, dice: 0.487624, jaccard: 0.407468, loss: 0.256082\n",
            "val: bce: 0.030694, dice: 0.599095, jaccard: 0.304930, loss: 0.314894\n",
            "0m 2s\n",
            "Epoch 402/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023868, dice: 0.487609, jaccard: 0.407432, loss: 0.255738\n",
            "val: bce: 0.032697, dice: 0.591682, jaccard: 0.310409, loss: 0.312190\n",
            "0m 2s\n",
            "Epoch 403/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024627, dice: 0.486680, jaccard: 0.408918, loss: 0.255653\n",
            "val: bce: 0.031318, dice: 0.593381, jaccard: 0.308477, loss: 0.312349\n",
            "0m 2s\n",
            "Epoch 404/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024375, dice: 0.485060, jaccard: 0.408831, loss: 0.254718\n",
            "val: bce: 0.030549, dice: 0.593719, jaccard: 0.307932, loss: 0.312134\n",
            "0m 2s\n",
            "Epoch 405/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024382, dice: 0.485811, jaccard: 0.408014, loss: 0.255096\n",
            "val: bce: 0.031336, dice: 0.592205, jaccard: 0.308717, loss: 0.311771\n",
            "0m 2s\n",
            "Epoch 406/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024367, dice: 0.483764, jaccard: 0.412008, loss: 0.254066\n",
            "val: bce: 0.033353, dice: 0.592714, jaccard: 0.310841, loss: 0.313033\n",
            "0m 2s\n",
            "Epoch 407/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024583, dice: 0.489856, jaccard: 0.406375, loss: 0.257220\n",
            "val: bce: 0.030326, dice: 0.605741, jaccard: 0.296576, loss: 0.318033\n",
            "0m 2s\n",
            "Epoch 408/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024475, dice: 0.494088, jaccard: 0.401515, loss: 0.259282\n",
            "val: bce: 0.033660, dice: 0.591964, jaccard: 0.311286, loss: 0.312812\n",
            "0m 2s\n",
            "Epoch 409/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.025282, dice: 0.487677, jaccard: 0.408113, loss: 0.256479\n",
            "val: bce: 0.032403, dice: 0.584666, jaccard: 0.315103, loss: 0.308535\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 410/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024548, dice: 0.486063, jaccard: 0.407976, loss: 0.255306\n",
            "val: bce: 0.030383, dice: 0.595004, jaccard: 0.306182, loss: 0.312693\n",
            "0m 2s\n",
            "Epoch 411/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024264, dice: 0.480595, jaccard: 0.414632, loss: 0.252430\n",
            "val: bce: 0.030498, dice: 0.598243, jaccard: 0.305099, loss: 0.314370\n",
            "0m 2s\n",
            "Epoch 412/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024023, dice: 0.481253, jaccard: 0.414606, loss: 0.252638\n",
            "val: bce: 0.032515, dice: 0.589436, jaccard: 0.311102, loss: 0.310975\n",
            "0m 2s\n",
            "Epoch 413/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024451, dice: 0.483928, jaccard: 0.412295, loss: 0.254190\n",
            "val: bce: 0.031734, dice: 0.588081, jaccard: 0.315489, loss: 0.309907\n",
            "0m 2s\n",
            "Epoch 414/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024448, dice: 0.483671, jaccard: 0.414156, loss: 0.254060\n",
            "val: bce: 0.030714, dice: 0.610375, jaccard: 0.293316, loss: 0.320545\n",
            "0m 2s\n",
            "Epoch 415/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024875, dice: 0.492293, jaccard: 0.403295, loss: 0.258584\n",
            "val: bce: 0.031227, dice: 0.579507, jaccard: 0.317643, loss: 0.305367\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 416/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024487, dice: 0.482581, jaccard: 0.411495, loss: 0.253534\n",
            "val: bce: 0.033344, dice: 0.587729, jaccard: 0.315860, loss: 0.310537\n",
            "0m 2s\n",
            "Epoch 417/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024493, dice: 0.474439, jaccard: 0.421231, loss: 0.249466\n",
            "val: bce: 0.032512, dice: 0.589941, jaccard: 0.311037, loss: 0.311227\n",
            "0m 2s\n",
            "Epoch 418/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023648, dice: 0.469329, jaccard: 0.425864, loss: 0.246488\n",
            "val: bce: 0.031027, dice: 0.588637, jaccard: 0.312823, loss: 0.309832\n",
            "0m 2s\n",
            "Epoch 419/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.023090, dice: 0.466703, jaccard: 0.430006, loss: 0.244896\n",
            "val: bce: 0.030896, dice: 0.587372, jaccard: 0.315915, loss: 0.309134\n",
            "0m 2s\n",
            "Epoch 420/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022920, dice: 0.466230, jaccard: 0.433255, loss: 0.244575\n",
            "val: bce: 0.030562, dice: 0.590282, jaccard: 0.312944, loss: 0.310422\n",
            "0m 2s\n",
            "Epoch 421/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022842, dice: 0.462730, jaccard: 0.435965, loss: 0.242786\n",
            "val: bce: 0.030565, dice: 0.595107, jaccard: 0.309753, loss: 0.312836\n",
            "0m 2s\n",
            "Epoch 422/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023012, dice: 0.465266, jaccard: 0.433824, loss: 0.244139\n",
            "val: bce: 0.033931, dice: 0.593164, jaccard: 0.311992, loss: 0.313547\n",
            "0m 2s\n",
            "Epoch 423/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023961, dice: 0.466292, jaccard: 0.434096, loss: 0.245126\n",
            "val: bce: 0.031014, dice: 0.599059, jaccard: 0.306922, loss: 0.315037\n",
            "0m 2s\n",
            "Epoch 424/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023562, dice: 0.468086, jaccard: 0.429732, loss: 0.245824\n",
            "val: bce: 0.033691, dice: 0.590045, jaccard: 0.314176, loss: 0.311868\n",
            "0m 2s\n",
            "Epoch 425/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023660, dice: 0.462722, jaccard: 0.436406, loss: 0.243191\n",
            "val: bce: 0.030896, dice: 0.587819, jaccard: 0.314932, loss: 0.309357\n",
            "0m 2s\n",
            "Epoch 426/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023075, dice: 0.456443, jaccard: 0.442267, loss: 0.239759\n",
            "val: bce: 0.031449, dice: 0.588913, jaccard: 0.315653, loss: 0.310181\n",
            "0m 2s\n",
            "Epoch 427/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022829, dice: 0.457514, jaccard: 0.444453, loss: 0.240172\n",
            "val: bce: 0.030345, dice: 0.601680, jaccard: 0.305540, loss: 0.316012\n",
            "0m 2s\n",
            "Epoch 428/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023058, dice: 0.463435, jaccard: 0.436662, loss: 0.243246\n",
            "val: bce: 0.036441, dice: 0.604068, jaccard: 0.302474, loss: 0.320254\n",
            "0m 2s\n",
            "Epoch 429/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024206, dice: 0.474476, jaccard: 0.425272, loss: 0.249341\n",
            "val: bce: 0.031324, dice: 0.600138, jaccard: 0.303606, loss: 0.315731\n",
            "0m 2s\n",
            "Epoch 430/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023778, dice: 0.458664, jaccard: 0.440020, loss: 0.241221\n",
            "val: bce: 0.031408, dice: 0.603238, jaccard: 0.304129, loss: 0.317323\n",
            "0m 2s\n",
            "Epoch 431/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023772, dice: 0.471652, jaccard: 0.426520, loss: 0.247712\n",
            "val: bce: 0.033646, dice: 0.589267, jaccard: 0.314021, loss: 0.311457\n",
            "0m 2s\n",
            "Epoch 432/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024057, dice: 0.470202, jaccard: 0.430840, loss: 0.247129\n",
            "val: bce: 0.036231, dice: 0.592376, jaccard: 0.313817, loss: 0.314303\n",
            "0m 2s\n",
            "Epoch 433/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.024227, dice: 0.459494, jaccard: 0.438164, loss: 0.241860\n",
            "val: bce: 0.032935, dice: 0.581658, jaccard: 0.319174, loss: 0.307297\n",
            "0m 2s\n",
            "Epoch 434/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023274, dice: 0.457427, jaccard: 0.441258, loss: 0.240351\n",
            "val: bce: 0.030609, dice: 0.590115, jaccard: 0.314886, loss: 0.310362\n",
            "0m 2s\n",
            "Epoch 435/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022824, dice: 0.448934, jaccard: 0.452224, loss: 0.235879\n",
            "val: bce: 0.030289, dice: 0.585924, jaccard: 0.319277, loss: 0.308107\n",
            "0m 2s\n",
            "Epoch 436/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021846, dice: 0.441750, jaccard: 0.459626, loss: 0.231798\n",
            "val: bce: 0.029779, dice: 0.581710, jaccard: 0.322148, loss: 0.305744\n",
            "0m 2s\n",
            "Epoch 437/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021810, dice: 0.438064, jaccard: 0.464241, loss: 0.229937\n",
            "val: bce: 0.031236, dice: 0.586880, jaccard: 0.320818, loss: 0.309058\n",
            "0m 2s\n",
            "Epoch 438/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022005, dice: 0.434537, jaccard: 0.468677, loss: 0.228271\n",
            "val: bce: 0.030462, dice: 0.582621, jaccard: 0.324193, loss: 0.306542\n",
            "0m 2s\n",
            "Epoch 439/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021654, dice: 0.432812, jaccard: 0.470459, loss: 0.227233\n",
            "val: bce: 0.031843, dice: 0.583942, jaccard: 0.323481, loss: 0.307893\n",
            "0m 2s\n",
            "Epoch 440/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022056, dice: 0.430906, jaccard: 0.472344, loss: 0.226481\n",
            "val: bce: 0.030946, dice: 0.579541, jaccard: 0.327351, loss: 0.305243\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 441/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021805, dice: 0.432489, jaccard: 0.468883, loss: 0.227147\n",
            "val: bce: 0.034815, dice: 0.590996, jaccard: 0.317051, loss: 0.312906\n",
            "0m 2s\n",
            "Epoch 442/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023533, dice: 0.441697, jaccard: 0.461597, loss: 0.232615\n",
            "val: bce: 0.031355, dice: 0.587581, jaccard: 0.318450, loss: 0.309468\n",
            "0m 2s\n",
            "Epoch 443/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022814, dice: 0.444965, jaccard: 0.454940, loss: 0.233890\n",
            "val: bce: 0.032000, dice: 0.571008, jaccard: 0.332029, loss: 0.301504\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 444/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023217, dice: 0.437628, jaccard: 0.464716, loss: 0.230423\n",
            "val: bce: 0.033854, dice: 0.580074, jaccard: 0.322570, loss: 0.306964\n",
            "0m 2s\n",
            "Epoch 445/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023072, dice: 0.439438, jaccard: 0.460563, loss: 0.231255\n",
            "val: bce: 0.030602, dice: 0.583510, jaccard: 0.323692, loss: 0.307056\n",
            "0m 2s\n",
            "Epoch 446/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023277, dice: 0.449435, jaccard: 0.452222, loss: 0.236356\n",
            "val: bce: 0.031502, dice: 0.574583, jaccard: 0.329331, loss: 0.303042\n",
            "0m 2s\n",
            "Epoch 447/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022939, dice: 0.430217, jaccard: 0.468757, loss: 0.226578\n",
            "val: bce: 0.031286, dice: 0.575930, jaccard: 0.325499, loss: 0.303608\n",
            "0m 2s\n",
            "Epoch 448/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021888, dice: 0.428853, jaccard: 0.472095, loss: 0.225371\n",
            "val: bce: 0.033490, dice: 0.582195, jaccard: 0.326390, loss: 0.307842\n",
            "0m 2s\n",
            "Epoch 449/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.022324, dice: 0.422109, jaccard: 0.481564, loss: 0.222217\n",
            "val: bce: 0.031998, dice: 0.579555, jaccard: 0.329308, loss: 0.305777\n",
            "0m 2s\n",
            "Epoch 450/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021585, dice: 0.419952, jaccard: 0.482908, loss: 0.220769\n",
            "val: bce: 0.030921, dice: 0.580736, jaccard: 0.327622, loss: 0.305828\n",
            "0m 2s\n",
            "Epoch 451/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021835, dice: 0.416419, jaccard: 0.485829, loss: 0.219127\n",
            "val: bce: 0.031526, dice: 0.575908, jaccard: 0.333267, loss: 0.303717\n",
            "0m 2s\n",
            "Epoch 452/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021515, dice: 0.412805, jaccard: 0.489803, loss: 0.217160\n",
            "val: bce: 0.031211, dice: 0.578301, jaccard: 0.329048, loss: 0.304756\n",
            "0m 2s\n",
            "Epoch 453/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021587, dice: 0.416949, jaccard: 0.485146, loss: 0.219268\n",
            "val: bce: 0.032471, dice: 0.573006, jaccard: 0.335251, loss: 0.302739\n",
            "0m 2s\n",
            "Epoch 454/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022503, dice: 0.417633, jaccard: 0.484621, loss: 0.220068\n",
            "val: bce: 0.031567, dice: 0.576206, jaccard: 0.329435, loss: 0.303886\n",
            "0m 2s\n",
            "Epoch 455/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021614, dice: 0.409835, jaccard: 0.491314, loss: 0.215724\n",
            "val: bce: 0.031793, dice: 0.578157, jaccard: 0.329350, loss: 0.304975\n",
            "0m 2s\n",
            "Epoch 456/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022190, dice: 0.410611, jaccard: 0.491059, loss: 0.216400\n",
            "val: bce: 0.031643, dice: 0.578411, jaccard: 0.330366, loss: 0.305027\n",
            "0m 2s\n",
            "Epoch 457/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022164, dice: 0.419994, jaccard: 0.483182, loss: 0.221079\n",
            "val: bce: 0.031850, dice: 0.571629, jaccard: 0.334428, loss: 0.301739\n",
            "0m 2s\n",
            "Epoch 458/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022206, dice: 0.413093, jaccard: 0.488724, loss: 0.217649\n",
            "val: bce: 0.031606, dice: 0.576952, jaccard: 0.331165, loss: 0.304279\n",
            "0m 2s\n",
            "Epoch 459/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021752, dice: 0.407985, jaccard: 0.495009, loss: 0.214868\n",
            "val: bce: 0.031510, dice: 0.586659, jaccard: 0.321204, loss: 0.309085\n",
            "0m 2s\n",
            "Epoch 460/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021889, dice: 0.416061, jaccard: 0.485795, loss: 0.218975\n",
            "val: bce: 0.031847, dice: 0.568880, jaccard: 0.337841, loss: 0.300363\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 461/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021921, dice: 0.408403, jaccard: 0.492559, loss: 0.215162\n",
            "val: bce: 0.033771, dice: 0.579099, jaccard: 0.328099, loss: 0.306435\n",
            "0m 2s\n",
            "Epoch 462/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022069, dice: 0.416196, jaccard: 0.486162, loss: 0.219132\n",
            "val: bce: 0.031797, dice: 0.573855, jaccard: 0.332860, loss: 0.302826\n",
            "0m 2s\n",
            "Epoch 463/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022954, dice: 0.423697, jaccard: 0.477271, loss: 0.223326\n",
            "val: bce: 0.033766, dice: 0.604962, jaccard: 0.301964, loss: 0.319364\n",
            "0m 2s\n",
            "Epoch 464/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.023604, dice: 0.447688, jaccard: 0.451650, loss: 0.235646\n",
            "val: bce: 0.032456, dice: 0.568494, jaccard: 0.335066, loss: 0.300475\n",
            "0m 2s\n",
            "Epoch 465/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022763, dice: 0.413759, jaccard: 0.483871, loss: 0.218261\n",
            "val: bce: 0.033415, dice: 0.570380, jaccard: 0.335205, loss: 0.301897\n",
            "0m 2s\n",
            "Epoch 466/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022168, dice: 0.407615, jaccard: 0.492498, loss: 0.214892\n",
            "val: bce: 0.033496, dice: 0.573735, jaccard: 0.328547, loss: 0.303616\n",
            "0m 2s\n",
            "Epoch 467/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.022121, dice: 0.409542, jaccard: 0.490709, loss: 0.215831\n",
            "val: bce: 0.032914, dice: 0.573906, jaccard: 0.333271, loss: 0.303410\n",
            "0m 2s\n",
            "Epoch 468/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021740, dice: 0.410203, jaccard: 0.493112, loss: 0.215972\n",
            "val: bce: 0.033504, dice: 0.572899, jaccard: 0.332834, loss: 0.303201\n",
            "0m 2s\n",
            "Epoch 469/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021998, dice: 0.413033, jaccard: 0.486809, loss: 0.217516\n",
            "val: bce: 0.031454, dice: 0.575623, jaccard: 0.330883, loss: 0.303539\n",
            "0m 2s\n",
            "Epoch 470/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021563, dice: 0.404725, jaccard: 0.498855, loss: 0.213144\n",
            "val: bce: 0.031483, dice: 0.581483, jaccard: 0.327349, loss: 0.306483\n",
            "0m 2s\n",
            "Epoch 471/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021742, dice: 0.401656, jaccard: 0.499299, loss: 0.211699\n",
            "val: bce: 0.031684, dice: 0.574320, jaccard: 0.331724, loss: 0.303002\n",
            "0m 2s\n",
            "Epoch 472/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021486, dice: 0.397157, jaccard: 0.503557, loss: 0.209322\n",
            "val: bce: 0.031290, dice: 0.575930, jaccard: 0.332242, loss: 0.303610\n",
            "0m 2s\n",
            "Epoch 473/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021306, dice: 0.393006, jaccard: 0.508916, loss: 0.207156\n",
            "val: bce: 0.031828, dice: 0.568944, jaccard: 0.338442, loss: 0.300386\n",
            "0m 2s\n",
            "Epoch 474/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021176, dice: 0.391475, jaccard: 0.511215, loss: 0.206325\n",
            "val: bce: 0.031091, dice: 0.574329, jaccard: 0.333072, loss: 0.302710\n",
            "0m 2s\n",
            "Epoch 475/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020846, dice: 0.391092, jaccard: 0.512781, loss: 0.205969\n",
            "val: bce: 0.031609, dice: 0.566571, jaccard: 0.341234, loss: 0.299090\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 476/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021034, dice: 0.389228, jaccard: 0.514980, loss: 0.205131\n",
            "val: bce: 0.031455, dice: 0.572894, jaccard: 0.334897, loss: 0.302174\n",
            "0m 2s\n",
            "Epoch 477/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020965, dice: 0.388450, jaccard: 0.515491, loss: 0.204707\n",
            "val: bce: 0.031852, dice: 0.569761, jaccard: 0.338142, loss: 0.300807\n",
            "0m 2s\n",
            "Epoch 478/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021215, dice: 0.387055, jaccard: 0.516512, loss: 0.204135\n",
            "val: bce: 0.031650, dice: 0.572940, jaccard: 0.336239, loss: 0.302295\n",
            "0m 2s\n",
            "Epoch 479/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.021188, dice: 0.386607, jaccard: 0.517533, loss: 0.203898\n",
            "val: bce: 0.031897, dice: 0.570591, jaccard: 0.335989, loss: 0.301244\n",
            "0m 2s\n",
            "Epoch 480/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021201, dice: 0.386633, jaccard: 0.517049, loss: 0.203917\n",
            "val: bce: 0.031800, dice: 0.570446, jaccard: 0.338643, loss: 0.301123\n",
            "0m 2s\n",
            "Epoch 481/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021151, dice: 0.385530, jaccard: 0.516863, loss: 0.203341\n",
            "val: bce: 0.032074, dice: 0.573054, jaccard: 0.334898, loss: 0.302564\n",
            "0m 2s\n",
            "Epoch 482/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021156, dice: 0.384031, jaccard: 0.519036, loss: 0.202594\n",
            "val: bce: 0.032281, dice: 0.568220, jaccard: 0.338990, loss: 0.300251\n",
            "0m 2s\n",
            "Epoch 483/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021183, dice: 0.383967, jaccard: 0.519920, loss: 0.202575\n",
            "val: bce: 0.031911, dice: 0.568936, jaccard: 0.338108, loss: 0.300424\n",
            "0m 2s\n",
            "Epoch 484/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021100, dice: 0.382248, jaccard: 0.521482, loss: 0.201674\n",
            "val: bce: 0.032459, dice: 0.570995, jaccard: 0.336507, loss: 0.301727\n",
            "0m 2s\n",
            "Epoch 485/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021357, dice: 0.386007, jaccard: 0.518137, loss: 0.203682\n",
            "val: bce: 0.032544, dice: 0.566868, jaccard: 0.340928, loss: 0.299706\n",
            "0m 2s\n",
            "Epoch 486/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021620, dice: 0.385865, jaccard: 0.516108, loss: 0.203743\n",
            "val: bce: 0.032129, dice: 0.581793, jaccard: 0.327226, loss: 0.306961\n",
            "0m 2s\n",
            "Epoch 487/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021382, dice: 0.386842, jaccard: 0.515036, loss: 0.204112\n",
            "val: bce: 0.032274, dice: 0.566907, jaccard: 0.340326, loss: 0.299591\n",
            "0m 2s\n",
            "Epoch 488/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021242, dice: 0.381185, jaccard: 0.522245, loss: 0.201214\n",
            "val: bce: 0.032563, dice: 0.565930, jaccard: 0.340157, loss: 0.299246\n",
            "0m 2s\n",
            "Epoch 489/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021332, dice: 0.380066, jaccard: 0.522435, loss: 0.200699\n",
            "val: bce: 0.032190, dice: 0.572177, jaccard: 0.336638, loss: 0.302184\n",
            "0m 2s\n",
            "Epoch 490/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021069, dice: 0.379115, jaccard: 0.524230, loss: 0.200092\n",
            "val: bce: 0.032194, dice: 0.570829, jaccard: 0.337979, loss: 0.301511\n",
            "0m 2s\n",
            "Epoch 491/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020987, dice: 0.377755, jaccard: 0.525151, loss: 0.199371\n",
            "val: bce: 0.032540, dice: 0.568146, jaccard: 0.339247, loss: 0.300343\n",
            "0m 2s\n",
            "Epoch 492/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021274, dice: 0.379007, jaccard: 0.524475, loss: 0.200141\n",
            "val: bce: 0.032052, dice: 0.575208, jaccard: 0.335235, loss: 0.303630\n",
            "0m 2s\n",
            "Epoch 493/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021108, dice: 0.379126, jaccard: 0.525010, loss: 0.200117\n",
            "val: bce: 0.032390, dice: 0.571564, jaccard: 0.337704, loss: 0.301977\n",
            "0m 2s\n",
            "Epoch 494/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021321, dice: 0.378502, jaccard: 0.523746, loss: 0.199911\n",
            "val: bce: 0.033068, dice: 0.569564, jaccard: 0.337418, loss: 0.301316\n",
            "0m 2s\n",
            "Epoch 495/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021345, dice: 0.383446, jaccard: 0.520390, loss: 0.202396\n",
            "val: bce: 0.032624, dice: 0.565530, jaccard: 0.342652, loss: 0.299077\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 496/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021555, dice: 0.380631, jaccard: 0.520380, loss: 0.201093\n",
            "val: bce: 0.032545, dice: 0.586090, jaccard: 0.327071, loss: 0.309317\n",
            "0m 2s\n",
            "Epoch 497/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021500, dice: 0.388067, jaccard: 0.513859, loss: 0.204784\n",
            "val: bce: 0.033322, dice: 0.562260, jaccard: 0.344025, loss: 0.297791\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 498/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021650, dice: 0.382984, jaccard: 0.518884, loss: 0.202317\n",
            "val: bce: 0.034054, dice: 0.578205, jaccard: 0.326242, loss: 0.306130\n",
            "0m 2s\n",
            "Epoch 499/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021689, dice: 0.384499, jaccard: 0.515088, loss: 0.203094\n",
            "val: bce: 0.032336, dice: 0.567119, jaccard: 0.340355, loss: 0.299728\n",
            "0m 2s\n",
            "Epoch 500/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021277, dice: 0.376257, jaccard: 0.526041, loss: 0.198767\n",
            "val: bce: 0.032438, dice: 0.576256, jaccard: 0.335130, loss: 0.304347\n",
            "0m 2s\n",
            "Epoch 501/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020959, dice: 0.375328, jaccard: 0.526367, loss: 0.198143\n",
            "val: bce: 0.031885, dice: 0.572668, jaccard: 0.335973, loss: 0.302276\n",
            "0m 2s\n",
            "Epoch 502/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020802, dice: 0.373653, jaccard: 0.529862, loss: 0.197228\n",
            "val: bce: 0.032334, dice: 0.566581, jaccard: 0.343577, loss: 0.299457\n",
            "0m 2s\n",
            "Epoch 503/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021093, dice: 0.374291, jaccard: 0.527795, loss: 0.197692\n",
            "val: bce: 0.032617, dice: 0.571725, jaccard: 0.334992, loss: 0.302171\n",
            "0m 2s\n",
            "Epoch 504/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020893, dice: 0.371988, jaccard: 0.531059, loss: 0.196441\n",
            "val: bce: 0.032740, dice: 0.569960, jaccard: 0.340445, loss: 0.301350\n",
            "0m 2s\n",
            "Epoch 505/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021005, dice: 0.370488, jaccard: 0.532840, loss: 0.195746\n",
            "val: bce: 0.032367, dice: 0.567725, jaccard: 0.340841, loss: 0.300046\n",
            "0m 2s\n",
            "Epoch 506/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020934, dice: 0.370974, jaccard: 0.530817, loss: 0.195954\n",
            "val: bce: 0.032473, dice: 0.569885, jaccard: 0.338212, loss: 0.301179\n",
            "0m 2s\n",
            "Epoch 507/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020887, dice: 0.369098, jaccard: 0.533781, loss: 0.194992\n",
            "val: bce: 0.032414, dice: 0.567742, jaccard: 0.340918, loss: 0.300078\n",
            "0m 2s\n",
            "Epoch 508/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020957, dice: 0.368969, jaccard: 0.534338, loss: 0.194963\n",
            "val: bce: 0.032843, dice: 0.568272, jaccard: 0.340563, loss: 0.300558\n",
            "0m 2s\n",
            "Epoch 509/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.020993, dice: 0.368162, jaccard: 0.534390, loss: 0.194577\n",
            "val: bce: 0.032697, dice: 0.568966, jaccard: 0.339468, loss: 0.300831\n",
            "0m 2s\n",
            "Epoch 510/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020957, dice: 0.368300, jaccard: 0.535360, loss: 0.194629\n",
            "val: bce: 0.032641, dice: 0.569005, jaccard: 0.340593, loss: 0.300823\n",
            "0m 2s\n",
            "Epoch 511/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020810, dice: 0.367230, jaccard: 0.535303, loss: 0.194020\n",
            "val: bce: 0.032736, dice: 0.568542, jaccard: 0.339444, loss: 0.300639\n",
            "0m 2s\n",
            "Epoch 512/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021052, dice: 0.367631, jaccard: 0.535717, loss: 0.194341\n",
            "val: bce: 0.032656, dice: 0.569327, jaccard: 0.341273, loss: 0.300992\n",
            "0m 2s\n",
            "Epoch 513/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020808, dice: 0.366837, jaccard: 0.536323, loss: 0.193823\n",
            "val: bce: 0.032671, dice: 0.569922, jaccard: 0.338548, loss: 0.301296\n",
            "0m 2s\n",
            "Epoch 514/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021005, dice: 0.367066, jaccard: 0.535357, loss: 0.194036\n",
            "val: bce: 0.032907, dice: 0.567998, jaccard: 0.339460, loss: 0.300453\n",
            "0m 2s\n",
            "Epoch 515/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020886, dice: 0.365308, jaccard: 0.537480, loss: 0.193097\n",
            "val: bce: 0.032826, dice: 0.570094, jaccard: 0.338935, loss: 0.301460\n",
            "0m 2s\n",
            "Epoch 516/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021001, dice: 0.365170, jaccard: 0.537000, loss: 0.193085\n",
            "val: bce: 0.032779, dice: 0.569147, jaccard: 0.339551, loss: 0.300963\n",
            "0m 2s\n",
            "Epoch 517/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020760, dice: 0.364567, jaccard: 0.538235, loss: 0.192663\n",
            "val: bce: 0.032831, dice: 0.568165, jaccard: 0.340859, loss: 0.300498\n",
            "0m 2s\n",
            "Epoch 518/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020875, dice: 0.364381, jaccard: 0.538692, loss: 0.192628\n",
            "val: bce: 0.033257, dice: 0.565481, jaccard: 0.341094, loss: 0.299369\n",
            "0m 2s\n",
            "Epoch 519/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021172, dice: 0.366319, jaccard: 0.535480, loss: 0.193746\n",
            "val: bce: 0.032759, dice: 0.571293, jaccard: 0.338085, loss: 0.302026\n",
            "0m 2s\n",
            "Epoch 520/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020713, dice: 0.365068, jaccard: 0.538214, loss: 0.192891\n",
            "val: bce: 0.032941, dice: 0.567335, jaccard: 0.342275, loss: 0.300138\n",
            "0m 2s\n",
            "Epoch 521/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021058, dice: 0.363850, jaccard: 0.538573, loss: 0.192454\n",
            "val: bce: 0.033441, dice: 0.570069, jaccard: 0.337381, loss: 0.301755\n",
            "0m 2s\n",
            "Epoch 522/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021024, dice: 0.366598, jaccard: 0.534435, loss: 0.193811\n",
            "val: bce: 0.032962, dice: 0.567216, jaccard: 0.341365, loss: 0.300089\n",
            "0m 2s\n",
            "Epoch 523/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021558, dice: 0.378547, jaccard: 0.521659, loss: 0.200053\n",
            "val: bce: 0.032936, dice: 0.567414, jaccard: 0.341696, loss: 0.300175\n",
            "0m 2s\n",
            "Epoch 524/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021228, dice: 0.371333, jaccard: 0.528821, loss: 0.196281\n",
            "val: bce: 0.034970, dice: 0.571172, jaccard: 0.337128, loss: 0.303071\n",
            "0m 2s\n",
            "Epoch 525/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021572, dice: 0.375050, jaccard: 0.525592, loss: 0.198311\n",
            "val: bce: 0.033715, dice: 0.565166, jaccard: 0.340075, loss: 0.299441\n",
            "0m 2s\n",
            "Epoch 526/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021093, dice: 0.371563, jaccard: 0.530283, loss: 0.196328\n",
            "val: bce: 0.032844, dice: 0.571601, jaccard: 0.337606, loss: 0.302222\n",
            "0m 2s\n",
            "Epoch 527/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021271, dice: 0.368683, jaccard: 0.532077, loss: 0.194977\n",
            "val: bce: 0.033269, dice: 0.566644, jaccard: 0.339272, loss: 0.299957\n",
            "0m 2s\n",
            "Epoch 528/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020983, dice: 0.363713, jaccard: 0.536067, loss: 0.192348\n",
            "val: bce: 0.033663, dice: 0.566148, jaccard: 0.339355, loss: 0.299905\n",
            "0m 2s\n",
            "Epoch 529/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020968, dice: 0.362291, jaccard: 0.539787, loss: 0.191630\n",
            "val: bce: 0.033118, dice: 0.562373, jaccard: 0.344175, loss: 0.297745\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 530/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020707, dice: 0.361386, jaccard: 0.540203, loss: 0.191047\n",
            "val: bce: 0.032984, dice: 0.566720, jaccard: 0.340084, loss: 0.299852\n",
            "0m 2s\n",
            "Epoch 531/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020741, dice: 0.361979, jaccard: 0.540239, loss: 0.191360\n",
            "val: bce: 0.033098, dice: 0.563861, jaccard: 0.344084, loss: 0.298480\n",
            "0m 2s\n",
            "Epoch 532/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020893, dice: 0.362879, jaccard: 0.538923, loss: 0.191886\n",
            "val: bce: 0.033046, dice: 0.575407, jaccard: 0.336070, loss: 0.304226\n",
            "0m 2s\n",
            "Epoch 533/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020765, dice: 0.361327, jaccard: 0.540142, loss: 0.191046\n",
            "val: bce: 0.032829, dice: 0.575320, jaccard: 0.337108, loss: 0.304075\n",
            "0m 2s\n",
            "Epoch 534/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020772, dice: 0.360443, jaccard: 0.541944, loss: 0.190608\n",
            "val: bce: 0.033030, dice: 0.570483, jaccard: 0.339142, loss: 0.301756\n",
            "0m 2s\n",
            "Epoch 535/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020759, dice: 0.359640, jaccard: 0.541106, loss: 0.190200\n",
            "val: bce: 0.033186, dice: 0.561068, jaccard: 0.344161, loss: 0.297127\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 536/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020731, dice: 0.361085, jaccard: 0.541512, loss: 0.190908\n",
            "val: bce: 0.032752, dice: 0.564193, jaccard: 0.346602, loss: 0.298473\n",
            "0m 2s\n",
            "Epoch 537/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020745, dice: 0.361747, jaccard: 0.540600, loss: 0.191246\n",
            "val: bce: 0.032789, dice: 0.575631, jaccard: 0.334321, loss: 0.304210\n",
            "0m 2s\n",
            "Epoch 538/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020723, dice: 0.362016, jaccard: 0.539893, loss: 0.191369\n",
            "val: bce: 0.033185, dice: 0.561130, jaccard: 0.346267, loss: 0.297158\n",
            "0m 2s\n",
            "Epoch 539/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.020764, dice: 0.358718, jaccard: 0.543276, loss: 0.189741\n",
            "val: bce: 0.033427, dice: 0.568300, jaccard: 0.338847, loss: 0.300864\n",
            "0m 2s\n",
            "Epoch 540/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020934, dice: 0.359634, jaccard: 0.542321, loss: 0.190284\n",
            "val: bce: 0.033323, dice: 0.560392, jaccard: 0.346673, loss: 0.296857\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 541/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020713, dice: 0.361150, jaccard: 0.541367, loss: 0.190931\n",
            "val: bce: 0.032893, dice: 0.577935, jaccard: 0.334525, loss: 0.305414\n",
            "0m 2s\n",
            "Epoch 542/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021072, dice: 0.368320, jaccard: 0.531984, loss: 0.194696\n",
            "val: bce: 0.033850, dice: 0.560997, jaccard: 0.345795, loss: 0.297424\n",
            "0m 2s\n",
            "Epoch 543/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021351, dice: 0.367906, jaccard: 0.533687, loss: 0.194629\n",
            "val: bce: 0.033872, dice: 0.559066, jaccard: 0.345603, loss: 0.296469\n",
            "saving best model\n",
            "0m 2s\n",
            "Epoch 544/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021786, dice: 0.377533, jaccard: 0.522549, loss: 0.199660\n",
            "val: bce: 0.034280, dice: 0.588288, jaccard: 0.321777, loss: 0.311284\n",
            "0m 2s\n",
            "Epoch 545/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021817, dice: 0.376115, jaccard: 0.521786, loss: 0.198966\n",
            "val: bce: 0.033416, dice: 0.567051, jaccard: 0.341215, loss: 0.300233\n",
            "0m 2s\n",
            "Epoch 546/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021251, dice: 0.367052, jaccard: 0.532432, loss: 0.194152\n",
            "val: bce: 0.034498, dice: 0.562111, jaccard: 0.343843, loss: 0.298304\n",
            "0m 2s\n",
            "Epoch 547/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.021224, dice: 0.363715, jaccard: 0.533802, loss: 0.192469\n",
            "val: bce: 0.033174, dice: 0.563272, jaccard: 0.342709, loss: 0.298223\n",
            "0m 2s\n",
            "Epoch 548/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020420, dice: 0.359668, jaccard: 0.543769, loss: 0.190044\n",
            "val: bce: 0.033116, dice: 0.563518, jaccard: 0.344009, loss: 0.298317\n",
            "0m 2s\n",
            "Epoch 549/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020462, dice: 0.357613, jaccard: 0.543561, loss: 0.189037\n",
            "val: bce: 0.033224, dice: 0.566117, jaccard: 0.341332, loss: 0.299671\n",
            "0m 2s\n",
            "Epoch 550/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020413, dice: 0.354498, jaccard: 0.546488, loss: 0.187455\n",
            "val: bce: 0.032600, dice: 0.567014, jaccard: 0.342900, loss: 0.299807\n",
            "0m 2s\n",
            "Epoch 551/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020140, dice: 0.354349, jaccard: 0.548597, loss: 0.187245\n",
            "val: bce: 0.032673, dice: 0.563322, jaccard: 0.345348, loss: 0.297998\n",
            "0m 2s\n",
            "Epoch 552/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020358, dice: 0.354031, jaccard: 0.548304, loss: 0.187194\n",
            "val: bce: 0.032942, dice: 0.567812, jaccard: 0.341502, loss: 0.300377\n",
            "0m 2s\n",
            "Epoch 553/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020351, dice: 0.352893, jaccard: 0.549452, loss: 0.186622\n",
            "val: bce: 0.032853, dice: 0.562098, jaccard: 0.346195, loss: 0.297476\n",
            "0m 2s\n",
            "Epoch 554/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020294, dice: 0.353185, jaccard: 0.550290, loss: 0.186740\n",
            "val: bce: 0.033227, dice: 0.565433, jaccard: 0.343468, loss: 0.299330\n",
            "0m 2s\n",
            "Epoch 555/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020507, dice: 0.351527, jaccard: 0.549676, loss: 0.186017\n",
            "val: bce: 0.032972, dice: 0.568597, jaccard: 0.340262, loss: 0.300784\n",
            "0m 2s\n",
            "Epoch 556/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020169, dice: 0.351483, jaccard: 0.550996, loss: 0.185826\n",
            "val: bce: 0.033175, dice: 0.563936, jaccard: 0.344870, loss: 0.298555\n",
            "0m 2s\n",
            "Epoch 557/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020429, dice: 0.350530, jaccard: 0.551161, loss: 0.185479\n",
            "val: bce: 0.033074, dice: 0.568123, jaccard: 0.340981, loss: 0.300598\n",
            "0m 2s\n",
            "Epoch 558/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020297, dice: 0.350696, jaccard: 0.551118, loss: 0.185497\n",
            "val: bce: 0.033318, dice: 0.567128, jaccard: 0.341025, loss: 0.300223\n",
            "0m 2s\n",
            "Epoch 559/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020244, dice: 0.350417, jaccard: 0.553079, loss: 0.185330\n",
            "val: bce: 0.033643, dice: 0.561154, jaccard: 0.345448, loss: 0.297399\n",
            "0m 2s\n",
            "Epoch 560/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020410, dice: 0.349991, jaccard: 0.551890, loss: 0.185200\n",
            "val: bce: 0.033256, dice: 0.567944, jaccard: 0.340210, loss: 0.300600\n",
            "0m 2s\n",
            "Epoch 561/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020363, dice: 0.349652, jaccard: 0.553962, loss: 0.185007\n",
            "val: bce: 0.033221, dice: 0.568358, jaccard: 0.344135, loss: 0.300790\n",
            "0m 2s\n",
            "Epoch 562/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020434, dice: 0.349619, jaccard: 0.552540, loss: 0.185026\n",
            "val: bce: 0.033753, dice: 0.561377, jaccard: 0.343741, loss: 0.297565\n",
            "0m 2s\n",
            "Epoch 563/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020480, dice: 0.352742, jaccard: 0.548798, loss: 0.186611\n",
            "val: bce: 0.033295, dice: 0.564779, jaccard: 0.343791, loss: 0.299037\n",
            "0m 2s\n",
            "Epoch 564/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020674, dice: 0.350869, jaccard: 0.551121, loss: 0.185772\n",
            "val: bce: 0.033239, dice: 0.576458, jaccard: 0.337550, loss: 0.304848\n",
            "0m 2s\n",
            "Epoch 565/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020390, dice: 0.351984, jaccard: 0.550052, loss: 0.186187\n",
            "val: bce: 0.033356, dice: 0.562282, jaccard: 0.345172, loss: 0.297819\n",
            "0m 2s\n",
            "Epoch 566/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020481, dice: 0.350125, jaccard: 0.550918, loss: 0.185303\n",
            "val: bce: 0.033538, dice: 0.567174, jaccard: 0.341882, loss: 0.300356\n",
            "0m 2s\n",
            "Epoch 567/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020450, dice: 0.348208, jaccard: 0.553531, loss: 0.184329\n",
            "val: bce: 0.033205, dice: 0.566004, jaccard: 0.344671, loss: 0.299604\n",
            "0m 2s\n",
            "Epoch 568/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020333, dice: 0.347325, jaccard: 0.554945, loss: 0.183829\n",
            "val: bce: 0.033154, dice: 0.564369, jaccard: 0.343770, loss: 0.298761\n",
            "0m 2s\n",
            "Epoch 569/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.020283, dice: 0.347521, jaccard: 0.554707, loss: 0.183902\n",
            "val: bce: 0.033401, dice: 0.568232, jaccard: 0.342097, loss: 0.300817\n",
            "0m 2s\n",
            "Epoch 570/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020225, dice: 0.346872, jaccard: 0.555973, loss: 0.183549\n",
            "val: bce: 0.033311, dice: 0.563395, jaccard: 0.344080, loss: 0.298353\n",
            "0m 2s\n",
            "Epoch 571/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020198, dice: 0.346227, jaccard: 0.555879, loss: 0.183212\n",
            "val: bce: 0.033313, dice: 0.569673, jaccard: 0.343298, loss: 0.301493\n",
            "0m 2s\n",
            "Epoch 572/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020272, dice: 0.348393, jaccard: 0.554729, loss: 0.184333\n",
            "val: bce: 0.034033, dice: 0.562516, jaccard: 0.343896, loss: 0.298275\n",
            "0m 2s\n",
            "Epoch 573/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020582, dice: 0.349519, jaccard: 0.551652, loss: 0.185050\n",
            "val: bce: 0.033608, dice: 0.564696, jaccard: 0.342196, loss: 0.299152\n",
            "0m 2s\n",
            "Epoch 574/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020424, dice: 0.348999, jaccard: 0.553698, loss: 0.184712\n",
            "val: bce: 0.033489, dice: 0.573430, jaccard: 0.340201, loss: 0.303460\n",
            "0m 2s\n",
            "Epoch 575/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020677, dice: 0.352104, jaccard: 0.547911, loss: 0.186391\n",
            "val: bce: 0.033776, dice: 0.564998, jaccard: 0.342991, loss: 0.299387\n",
            "0m 2s\n",
            "Epoch 576/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020448, dice: 0.346071, jaccard: 0.554994, loss: 0.183259\n",
            "val: bce: 0.033469, dice: 0.562920, jaccard: 0.344609, loss: 0.298194\n",
            "0m 2s\n",
            "Epoch 577/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020275, dice: 0.347404, jaccard: 0.555459, loss: 0.183840\n",
            "val: bce: 0.034147, dice: 0.559393, jaccard: 0.347882, loss: 0.296770\n",
            "0m 2s\n",
            "Epoch 578/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020478, dice: 0.347487, jaccard: 0.553121, loss: 0.183982\n",
            "val: bce: 0.033212, dice: 0.569150, jaccard: 0.342653, loss: 0.301181\n",
            "0m 2s\n",
            "Epoch 579/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020246, dice: 0.348520, jaccard: 0.554301, loss: 0.184383\n",
            "val: bce: 0.033824, dice: 0.566338, jaccard: 0.342545, loss: 0.300081\n",
            "0m 2s\n",
            "Epoch 580/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020342, dice: 0.344664, jaccard: 0.556421, loss: 0.182503\n",
            "val: bce: 0.034079, dice: 0.559329, jaccard: 0.345956, loss: 0.296704\n",
            "0m 2s\n",
            "Epoch 581/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020165, dice: 0.344563, jaccard: 0.557508, loss: 0.182364\n",
            "val: bce: 0.033725, dice: 0.564279, jaccard: 0.344189, loss: 0.299002\n",
            "0m 2s\n",
            "Epoch 582/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020258, dice: 0.344874, jaccard: 0.557987, loss: 0.182566\n",
            "val: bce: 0.033462, dice: 0.569282, jaccard: 0.341156, loss: 0.301372\n",
            "0m 2s\n",
            "Epoch 583/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020217, dice: 0.344263, jaccard: 0.557228, loss: 0.182240\n",
            "val: bce: 0.033484, dice: 0.563928, jaccard: 0.344297, loss: 0.298706\n",
            "0m 2s\n",
            "Epoch 584/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020135, dice: 0.342693, jaccard: 0.559310, loss: 0.181414\n",
            "val: bce: 0.033658, dice: 0.562701, jaccard: 0.345438, loss: 0.298179\n",
            "0m 2s\n",
            "Epoch 585/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020231, dice: 0.342635, jaccard: 0.558919, loss: 0.181433\n",
            "val: bce: 0.033455, dice: 0.567140, jaccard: 0.343284, loss: 0.300297\n",
            "0m 2s\n",
            "Epoch 586/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020138, dice: 0.341978, jaccard: 0.560902, loss: 0.181058\n",
            "val: bce: 0.033533, dice: 0.566649, jaccard: 0.345119, loss: 0.300091\n",
            "0m 2s\n",
            "Epoch 587/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020138, dice: 0.343112, jaccard: 0.558608, loss: 0.181625\n",
            "val: bce: 0.033966, dice: 0.564265, jaccard: 0.343250, loss: 0.299116\n",
            "0m 2s\n",
            "Epoch 588/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020173, dice: 0.341365, jaccard: 0.560532, loss: 0.180769\n",
            "val: bce: 0.033810, dice: 0.561747, jaccard: 0.346866, loss: 0.297778\n",
            "0m 2s\n",
            "Epoch 589/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020095, dice: 0.340981, jaccard: 0.562369, loss: 0.180538\n",
            "val: bce: 0.033710, dice: 0.565467, jaccard: 0.343341, loss: 0.299589\n",
            "0m 2s\n",
            "Epoch 590/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020134, dice: 0.341021, jaccard: 0.560827, loss: 0.180578\n",
            "val: bce: 0.033986, dice: 0.561404, jaccard: 0.347181, loss: 0.297695\n",
            "0m 2s\n",
            "Epoch 591/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020180, dice: 0.340301, jaccard: 0.562738, loss: 0.180240\n",
            "val: bce: 0.033780, dice: 0.564025, jaccard: 0.345843, loss: 0.298903\n",
            "0m 2s\n",
            "Epoch 592/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020184, dice: 0.340253, jaccard: 0.562106, loss: 0.180218\n",
            "val: bce: 0.033867, dice: 0.567929, jaccard: 0.340955, loss: 0.300898\n",
            "0m 2s\n",
            "Epoch 593/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020101, dice: 0.340724, jaccard: 0.561574, loss: 0.180413\n",
            "val: bce: 0.033608, dice: 0.561163, jaccard: 0.348471, loss: 0.297385\n",
            "0m 2s\n",
            "Epoch 594/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020216, dice: 0.340304, jaccard: 0.562635, loss: 0.180260\n",
            "val: bce: 0.034001, dice: 0.565548, jaccard: 0.343267, loss: 0.299774\n",
            "0m 2s\n",
            "Epoch 595/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.019997, dice: 0.339885, jaccard: 0.561487, loss: 0.179941\n",
            "val: bce: 0.033827, dice: 0.562865, jaccard: 0.346718, loss: 0.298346\n",
            "0m 2s\n",
            "Epoch 596/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020270, dice: 0.339100, jaccard: 0.563881, loss: 0.179685\n",
            "val: bce: 0.034063, dice: 0.561747, jaccard: 0.346961, loss: 0.297905\n",
            "0m 2s\n",
            "Epoch 597/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020080, dice: 0.339079, jaccard: 0.563196, loss: 0.179579\n",
            "val: bce: 0.033774, dice: 0.565742, jaccard: 0.344594, loss: 0.299758\n",
            "0m 2s\n",
            "Epoch 598/599\n",
            "----------\n",
            "LR 1e-05\n",
            "train: bce: 0.020203, dice: 0.338854, jaccard: 0.563469, loss: 0.179529\n",
            "val: bce: 0.034153, dice: 0.561864, jaccard: 0.345596, loss: 0.298009\n",
            "0m 2s\n",
            "Epoch 599/599\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "train: bce: 0.020190, dice: 0.339884, jaccard: 0.562918, loss: 0.180037\n",
            "val: bce: 0.034140, dice: 0.565295, jaccard: 0.343868, loss: 0.299717\n",
            "0m 2s\n",
            "Best metrics for the best model:\n",
            "BCE: 0.033872\n",
            "Jaccard index: 0.345603\n",
            "Dice coefficient: 0.559066\n",
            "Loss: 0.296469\n",
            "(3, 6, 192, 192)\n",
            "Average Jaccard Index for test set: 0.3942\n",
            "Average Dice Score for test set: 0.5655\n",
            "Jacard index [0.3942323923110962]\n",
            "Dice Score [0.5655189156532288]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 9 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9gAAAPMCAYAAAC5bRH8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAADU4ElEQVR4nOzdeXxU1d3H8e+dmWSykIUkhCQQdpVNNhGkIIuiiH20IG4ILtVHsQVtQR8V6wJWxdbWWhWltop1xbqhtS3KjguggogiImDYSSAJyWSd9T5/UKIxCTDhTmYy+bxfr/t6MffeOfd3B/0xvznnnmOYpmkKAAAAAACcEFu4AwAAAAAAIBpQYAMAAAAAYAEKbAAAAAAALECBDQAAAACABSiwAQAAAACwAAU2AAAAAAAWoMAGAAAAAMACFNgAAAAAAFiAAhsAAAAAAAtQYAMAAAAAYIGwFthz585Vp06dFBcXp8GDB+uTTz4JZzgAAAAAADRa2ArsV199VTNmzNC9996r9evXq2/fvhozZowOHDgQrpAAAAAAAGg0wzRNMxwXHjx4sE4//XQ98cQTkqRAIKDc3FzddNNNuuOOO4763kAgoH379ikpKUmGYTRFuACihGmaKisrU05Ojmy26HtKhvwIoLGiPT9K5EgAjRNMfnQ0UUy1eDwerVu3TjNnzqzZZ7PZNHr0aK1evbrO+W63W263u+b13r171bNnzyaJFUB02r17t9q3bx/uME4Y+RGA1aIlP0rkSADWOp78GJYCu7CwUH6/X23btq21v23btvrmm2/qnD9nzhzNnj27qcID0AIkJSWFOwRLNJQfd+/ereTk5DBEBKC5crlcys3NjZr8KJEjAVgjmPwYliHi+/btU7t27fTxxx9ryJAhNftvu+02rVy5UmvXrq11/o9/fTxygwDQWKWlpVHx5aqh/Bgt9weg6bhcLqWkpERV/iBHArBCMPkxLD3YGRkZstvtKigoqLW/oKBAWVlZdc53Op1yOp1NFR4ANBvkRwBoGDkSQFMLywwWsbGxOu2007R06dKafYFAQEuXLq3Vow0AAAAAQHMRlh5sSZoxY4auvvpqDRw4UIMGDdKjjz6qiooK/fznPw9XSAAAAAAANFrYCuzLLrtMBw8e1D333KP8/Hz169dPixYtqjPxGQAAAAAAzUHYCmxJmjZtmqZNmxbOEAAAAAAAsERYnsEGAAAAACDaUGADAAAAAGABCmwAAAAAACxAgQ0AAAAAgAUosAEAAAAAsAAFNgAAAAAAFqDABgAAAADAAhTYAAAAAABYgAIbAAAAAAALUGADAAAAAGABCmwAAAAAACxAgQ0AAAAAgAUosAEAAAAAsAAFNgAAAAAAFqDABgAAAADAAhTYAAAAAABYgAIbAAAAAAALUGADAAAAAGABCmwAAAAAACxAgQ0AAAAAgAUosAEAAAAAsAAFNgAAAAAAFqDABgAAAADAAhTYAAAAAABYwPICe86cOTr99NOVlJSkzMxMjRs3Tlu2bKl1zsiRI2UYRq3txhtvtDoUAAAAAACajOUF9sqVKzV16lStWbNGixcvltfr1bnnnquKiopa511//fXav39/zfb73//e6lAAAAAAAGgyDqsbXLRoUa3Xzz33nDIzM7Vu3ToNHz68Zn9CQoKysrKsvjwAAAAAAGER8mewS0tLJUlpaWm19r/00kvKyMhQ7969NXPmTFVWVjbYhtvtlsvlqrUBAMiPAHA05EgATS2kBXYgENCvf/1rDR06VL17967Zf8UVV+jFF1/U8uXLNXPmTL3wwguaPHlyg+3MmTNHKSkpNVtubm4owwaAZoP8CAANI0cCaGqGaZpmqBr/xS9+of/85z/68MMP1b59+wbPW7Zsmc4++2xt27ZNXbt2rXPc7XbL7XbXvHa5XCRIACektLRUycnJ4Q7jhDWUH6Pl/gA0HZfLpZSUlKjKH+RIAFYIJj9a/gz2EdOmTdO7776rVatWHbW4lqTBgwdLUoMFttPplNPpDEmcANCckR8BoGHkSABNzfIC2zRN3XTTTXrrrbe0YsUKde7c+Zjv2bBhgyQpOzvb6nAAAAAAAGgSlhfYU6dO1csvv6y3335bSUlJys/PlySlpKQoPj5e27dv18svv6zzzz9f6enp2rhxo6ZPn67hw4erT58+VocDAAAAAECTsLzAfuqppyRJI0eOrLV//vz5uuaaaxQbG6slS5bo0UcfVUVFhXJzczVhwgTdddddVocCAAAAAECTCckQ8aPJzc3VypUrrb4sAAAAAABhFfJ1sAEAAAAAaAkosAEAAAAAsAAFdhPr2bOn3njzTbVr1y7coQBARNlUvEvj37tfe8oLwx0KAABAo1BgN7H09HSNGzdOo0aNUrdu3cIdDgBEjKJqlxbuWKPl+zZqa+m+cIcDAAAQNArsMDAMQ8+/8IKmT58e7lAAIOJctfwR/WnjQpmmecyJMwEAACIJBXYYXT5xoj755BMlJCSEOxQAiCivbFupQW9NV6XPHe5QAAAAjhsFdhi1bt1avU89VTdMmaKePXuGOxwAiBglngp9WbxTf9m8SJuKd4Y7HACIKF8V79RfN78nj98X7lAA/AgFdpjFxcXpkUce0ehzzlFiYmK4wwGAiOH2e3XL6r9p8d4NKvdWMVwcQIvmC/hV6qlQqadC7+1erxmr/6aD1aUq9VSQI4EI4gh3ADjsoYce0hVXXKEzBg8OdygAEFHuWPucXt66QmvHPxLuUAAgbNYe2KLz/n2vJMkb8Mnt96r7gimSYahHantyJBAh6MGOEHFxcerWrZuemjdPXbt2DXc4ABAx3H6vtpXu140fzNU2ZhcH0EJ1ScrSH864Vq0ccXL7vZKkcl+1yr1V5EggglBgR5C0tDRNmTJFZwwZova5ueEOBwAixiFPuZ7evEirC77RrvKD4Q4HAJpccmyChmb1VLwjts4xciQQOSiwI9Dzzz+vRx5hmA8A/NhVyx/RjI//Gu4wAKDJrSvcplNfm6q8soIGzyFHAuFHgR2BDMPQyJEjtXz5cqWnp4c7HACIKCv2famR79yhwmpXuEMBgKZznHOYrdj3pYYt/D8NW/h/mv/N4tDGBKAOCuwIlZGRoaHDhmncuHHq3qNHuMMBgIhR5C7Th/lfa2Hean19aHe4wwGAiFLkLtNHBZv1UcFmvbdnvf6181MFzEC4wwJaDArsCOZwOPTXv/1Nl192mRwOJnwHgCP8ZkDXr3pcr25fJW/Ax/I0AFCPV7d/oBtWPa4qn4ciG2giFNjNwPQZM7Rm7VqKbAD4kUc2vqXBb86Qjy+OAFCv/KpD6vbK9Xpv9/pwhwK0CBTYzUBSUpK6deume+69V927dw93OAAQMcq91drm2q/Zn72szQwXB4A6Aqap/KpDqv7v0l4AQosu0WYiOTlZd911l3bk5amwsFCFhYXhDgkAIkKZt0oPfP6qOie3VUZcstrEp4Q7JACIOMXuMu2tOPz9MTM+VTE2ygAgFOjBbmbm/eUvWvDqq+EOAwAizpRVT+iyJb8LdxgAEJGmrHpCnV66Tp1fvk7bS/eHOxwgalFgNzMOh0N9+vTRG2++qXbt2oU7HACIGH4zoI1FeRr/3v3aU84oHwD4Ib8ZkM/0yxvwH++KXwAagQK7GcrIyNC4ceM0atQodevWLdzhAEDEKHKXaeGONVq+b6O2lu4LdzgAEJE+O7iVHAmECAV2M2UYhp5/4QVNnz493KEAQMS5avkj+tPGhTJNkyW8AOBHyJFA6FBgN3OXT5yoTz75RAkJCeEOBQAiyivbVmrQW9NV6XOHOxQAiDjkSCA0KLCbudatW6v3qafqhilT1LNnz3CHAwARo8RToS+Ld+ovmxdpU/HOcIcDABGFHAmEBgV2FIiLi9Mjjzyi0eeco8TExHCHAwARw+336pbVf9PivRtU7q1iKCQA/MCRHPlh/tfhDgWIGpYX2LNmzZJhGLW27t271xyvrq7W1KlTlZ6erlatWmnChAkqKCiwOowW6aGHHtLSZcvCHQYARJw71j6ns/55Z7jDAAAAUS4kPdi9evXS/v37a7YPP/yw5tj06dP1z3/+U6+99ppWrlypffv26aKLLgpFGC1OXFycunXrpqfmzVPXrl3DHQ4ARAy336ttpft14wdztY2ZcwE0Qyel5GjemVOVFd/a8rZf2rpCsz97mVE+gAVCUmA7HA5lZWXVbBkZGZKk0tJSPfPMM3rkkUd01lln6bTTTtP8+fP18ccfa82aNaEIpcVJS0vTlClTdMaQIWqfmxvucAAgYhzylOvpzYu0uuAb7So/GO5wACAo2YlpuqHHeRqYeZKyE6wtsj/I36TXvvvw2CcCOKaQFNhbt25VTk6OunTpokmTJmnXrl2SpHXr1snr9Wr06NE153bv3l0dOnTQ6tWrG2zP7XbL5XLV2nB0zz//vB555JFwhwEgxMiPwbtq+SOa8fFfwx0GgCYQbTnSMAy9M+Zu3TXg8nCHAqABDqsbHDx4sJ577jmdcsop2r9/v2bPnq0zzzxTX331lfLz8xUbG6vU1NRa72nbtq3y8/MbbHPOnDmaPXu21aFGNcMwNHLkSC1fvlwXX3yxioqKwh0SgBAgPzbOin1fauQ7d+j1c+9URlxyuMMBECLRmCMNw9D4zkPUJ73zMc995pv39dyWJU0QFYAjLC+wx44dW/PnPn36aPDgwerYsaP+8Y9/KD4+vlFtzpw5UzNmzKh57XK5lNtMhz8XFhbqjddfb7Lr+fx++f3+JrsegKYVTfkxIz5ZEzr/pMmuZzfsshsspgFEs2jKkT+UnZCm7IS0Y563p7xQZZ7K42ozt1WbEw0LgEJQYP9YamqqTj75ZG3btk3nnHOOPB6PSkpKavViFxQUKCsrq8E2nE6nnE5nqENtEps3b9Yll1wS7jAARIloyo89W3fQ6+cy0zcA60RTjmyMy7sN1+Xdhoc7DKBFCflP9+Xl5dq+fbuys7N12mmnKSYmRkuXLq05vmXLFu3atUtDhgwJdSgAAAAAAISM5T3Yt956qy644AJ17NhR+/bt07333iu73a6JEycqJSVF1113nWbMmKG0tDQlJyfrpptu0pAhQ3TGGWdYHQoAAAAAAE3G8gJ7z549mjhxooqKitSmTRsNGzZMa9asUZs2h5/r+NOf/iSbzaYJEybI7XZrzJgxevLJJ60OAwAAAACAJmWYzXBFeZfLpZSUlHCHAaAZKy0tVXJy9M0efSQ/Ruv9AQidlpA/WsI9ArBeMLmD6VMBAAAAALAABTYAAAAAABagwAYAAAAAwAIU2AAAAAAAWIACGwAAAAAAC1BgAwAAAABgAQpsAAAAAAAsQIENAAAAAIAFKLABAAAAALAABTYAAAAAABagwAYAAAAAwAIU2AAAAAAAWIACGwAAAAAAC1BgAwAAAABgAQpsAAAAAAAsQIENAAAAAIAFKLABAAAAALAABTYAAAAAABagwAYAAAAAwAIU2AAAAAAAWIACGwAAAAAAC1BgAwAAAABgAQpsAAAAAAAsQIENAAAAAIAFLC+wO3XqJMMw6mxTp06VJI0cObLOsRtvvNHqMAAAAAAAaFIOqxv89NNP5ff7a15/9dVXOuecc3TJJZfU7Lv++ut133331bxOSEiwOgwAAAAAAJqU5QV2mzZtar1+6KGH1LVrV40YMaJmX0JCgrKysqy+NAAAAAAAYRPSZ7A9Ho9efPFFXXvttTIMo2b/Sy+9pIyMDPXu3VszZ85UZWXlUdtxu91yuVy1NgAA+REAjoYcCaCphbTAXrhwoUpKSnTNNdfU7Lviiiv04osvavny5Zo5c6ZeeOEFTZ48+ajtzJkzRykpKTVbbm5uKMMGgGaD/AgADSNHAmhqhmmaZqgaHzNmjGJjY/XPf/6zwXOWLVums88+W9u2bVPXrl3rPcftdsvtdte8drlcJEgAJ6S0tFTJycnhDuOENZQfo+X+ADQdl8ullJSUqMof5EgAVggmP1r+DPYRO3fu1JIlS/Tmm28e9bzBgwdL0lELbKfTKafTaXmMANDckR8BoGHkSABNLWRDxOfPn6/MzEz99Kc/Pep5GzZskCRlZ2eHKhQAAAAAAEIuJD3YgUBA8+fP19VXXy2H4/tLbN++XS+//LLOP/98paena+PGjZo+fbqGDx+uPn36hCIUAAAAAACaREgK7CVLlmjXrl269tpra+2PjY3VkiVL9Oijj6qiokK5ubmaMGGC7rrrrlCEAQAAAABAkwlJgX3uueeqvrnTcnNztXLlylBcEgAAAACAsArpMl0AAAAAALQUFNgAAAAAAFiAAhsAAAAAAAtQYAMAAAAAYAEKbAAAAAAALECBDQAAAACABSiwAQAAAACwAAU2AAAAAAAWoMAGAAAAAMACFNgAAAAAAFiAAhsAAAAAAAtQYAMAAAAAYAEKbAAAAAAALECBDQAAAACABSiwAQAAAACwAAU2AAAAAAAWoMAGAAAAAMACFNgAAAAAAFiAAhsAAAAAAAtQYAMAAAAAYAEKbAAAAAAALECBDQAAAACABSiwAQAAAACwAAU2AAAAAAAWoMAGAAAAEHE8vgqVVOQpEPCFOxTguAVdYK9atUoXXHCBcnJyZBiGFi5cWOu4aZq65557lJ2drfj4eI0ePVpbt26tdU5xcbEmTZqk5ORkpaam6rrrrlN5efkJ3QgAAACA6LG/5BP98/OrVO0rCXcowHFzBPuGiooK9e3bV9dee60uuuiiOsd///vf67HHHtPf//53de7cWXfffbfGjBmjr7/+WnFxcZKkSZMmaf/+/Vq8eLG8Xq9+/vOf64YbbtDLL7984neEkMrIyNBrr7+ue+6+Wy6XS48/8YSunDxZ/QcM0I033qjx48Zp6tSp6tOnj6666irNffJJFRcVadasWXr9jTf09sKFevfdd4NqAwCag0PFlfrN1Dfk9wdCfq2c3FTd84cLZRhGyK8FAKHm81dp2dd3yB+orrW/2luqgOnTsk23yW6Lqdk/oNONapvSv6nDBI5L0AX22LFjNXbs2HqPmaapRx99VHfddZd+9rOfSZKef/55tW3bVgsXLtTll1+uzZs3a9GiRfr00081cOBASdLjjz+u888/X3/4wx+Uk5NzAreDUAsEAirIz1e12y2Px6P8/Hz5fD5VVlbqQEGBTNOUy+VSYWGhJKm4qEglJSWSpIMHD6q8vDzoNgCgOfB6fNq4fo/8vtAX2GWu6mOfBAARqtpbooLSz2te+/zVOuDaIH/AU+/5ReWba73eXfyRqr0ltfYZhl3tW/9ENlvQ5Q0aYJpe+Vz/lszaQ/RtMe1kTzwjTFFFPsM0TbPRbzYMvfXWWxo3bpwk6bvvvlPXrl31+eefq1+/fjXnjRgxQv369dOf//xnPfvss7rlllt06NChmuM+n09xcXF67bXXNH78+DrXcbvdcrvdNa9dLpdyc3MbGzZOUExMjHy+w/+jORwOeb1e2Ww22e12eb1e2e12GYYhn88nh8Mh0zTl9/vlcDgUCAQUCASCagMnLiYm5pjnmKbZoj7v0tJSJScnhzuME9ZQfoyW+2tODuS7dNGIuU1SYHc+KUMv/ecGerBP0OGvQF7pWN+EDEOSI+o/b5fLpZSUlKjKH+TIyFRQukHvf3mTAqZ13zti7Am6ZNA7inEkWtZmS2SapvTfZ95Nv0sVX3eRGSirdY4jZbziOy04/MKwybDZmzrMJhdMfrT0J578/HxJUtu2bWvtb9u2bc2x/Px8ZWZm1g7C4VBaWlrNOT82Z84czZ4928pQ0UhZWVlas3atpkyZopJDh/TGm29q5IgR+snQobrzzjt12oABuvueezRo0CCdNWqUXnv9dR08eFC//MUv9Mmnn+qF55/XK6+8ElQbODGtWrXSuvXr1Srx6P/g7M/P1xmDB7eoIjsakB+BExAoV8WW06TA0eeBMWKylXDyGknH/rESkYUcGZkyknppwulvatHGX6qsek+4w8GP7H9psryH8iTTlOlLlfSjgtL4UoZjqCQp5YwblDr4f5s8xkjWLMZQzJw5UzNmzKh5TQ92+FRUVOivTz+tHXl5qqqq0l/mzVNJSYk2bdqkZ599Vl6vV6tWrdKOvDxJ0tsLF9YMC3/h+ef12bp1QbeBxpk2bZoy2rSRMzZWHTp0kNPpPOr5ia1a6Z5771UgENDOHTs0f/78JooUJ4L8CATHNE15C5+Q6SuUTLdM7y7JrH9Yas17AhXy5N8nySZbbEfFpF/bNMHihJEjI5PdFqP42AzZDOt6Pv0Brzbu/rs6ZoxSRlIPy9ptifyVRfKXH/zvK5vqzovtlXT4eOW3S2T6SpXQxSVH68tlj+Ozt7TAzsrKkiQVFBQoOzu7Zn9BQUHNkPGsrCwdOHCg1vt8Pp+Ki4tr3v9jTqfzmMUBmkZFRYXmz5+v4uJiBQIBPfPMMyopKVHV5s3at3evvF6vPv7oI30WGytJevfdd2uGhb/yyiuqqKgIug0EJzY2Vm3atNFNN9+sk0466bjfl5ycrLvuukuStGbNGr333ns6cOAAPdoRjvwIHD8z4JbpOyjPwcdlerYd/xsDZfIUPCBJsiUMlj1pjIyYTBkGPdqRjhwZedzeUvkCh4ftB0y/Ze0GTK++2vOCWsVlU2A3kunzyF9ZJDOIZdGqd62Vu+BjGcZ+Od3d5EjNVkxqauiCbAYsXQe7c+fOysrK0tKlS2v2uVwurV27VkOGDJEkDRkyRCUlJVq3bl3NOcuWLVMgENDgwYOtDAchkJWVpW+3btW5556rQYMG6bu8PHXt2lWTJ0/Wl199pcTERD3wwAN67/33JUmvvf66nnzySTkcDq1Zu1Y333xz0G0gOEc+027dup1wG127drUwMgAIL3/lJ4efJwymuP6RwH/bCLi3WxgZ0HKs3vo7vfHpRXrj04sYHh5hqvdt0K65w+U7tDOo95lum4pXtNO3//ektt19X4iiaz6C7sEuLy/Xtm3f/8OUl5enDRs2KC0tTR06dNCvf/1r3X///TrppJNqlunKycmpmQitR48eOu+883T99ddr3rx58nq9mjZtmi6//HJmEG8GiouLdcXEifr000/l8Xh02aWXau/evXr//fe1Z88eVVdX6+mnn9Ybb7whSbrn7rtV7XbL7/drypQp2pGXF3QbCI5hGMc1qdnR2Gw2xcTERP2EPgBaGlPSiY7K+e/EaMecGQ3AD7m9pVq99Xc6ULZRpoU917DGvhdeVsU3K2RPaEyONCRTiutUrLj2rAIUdIH92WefadQPJp468lzL1Vdfreeee0633XabKioqdMMNN6ikpETDhg3TokWLatbAlqSXXnpJ06ZN09lnny2bzaYJEyboscces+B2EGqBQEDFxcXyeDzy/ndov9/vV3V1tQ4dOiTTNFVRWSmH4/B/Wi6XSx7P4WfbSg4dUlVVVdBt4Pj17NlTffv2tay9gQMHyufz1fpRDQCaI3/1JgWqvrCsvUDlZzIMh2zO438UB2jJTJly+1wKBDH8GE2netcXqspbp1a9Gt9GTKpH9oRCVe1aK2d2H9li4q0LsBk5oWW6wuXINOloejk5OfouL0+XXXqpiouLtXzFCvXu1UsjR43SnDlzlNu+vR5++GENHTZMfU49Vas++ED5+fmadMUVytuxQ3+ZN0/PPPNMUG3g+L355psaV89SdyfiySef1LSpUy1tMxJE6xIt0bjMTnPBMl2RrSpvvHylb1vaZkz6LxSXO9fSNsOpJeSPlnCPkW7517drV9GqkLV/RrfbdEq2td+FWoL816ao8lvrHs9sP+V9xaR3i5p/p8K2TBei34EDB9S3Tx/t3btXfr9fvXv10nfffaf8/HwtW7pUlZWVmj17tuLjD/9ideXkyfL5fPJ6vRo5YoRKSkpUUlISVBsAAACIDoO63qIO6SP14bffP6vrsMVpTJ8ntWnPS9pRuPQo70ZzUfXdhTJ9V8iZdXe4Q2lylk5yhujndDo1cuRItWnTRqmpqRo1apQSEhLUrl07nTl8uOx2u04+5RQNGjRIktR/wAD16t1bNptNPxk6VB07dQq6DQAAADRvXn+Vtux/S7uLPlBxxbe1jgXMgApKN6jSc7CBd6N5MVWVly9PwaFwBxIWFNgISnJysu5/4AH17NlTnTt31oNz5igzM1NnnHGG7r33XjmdTo0fN06/nj5dknTjlCmaNGmSbDab7rzzTp191llBtwEAAIDmzeMr09rtf9Ta7X/Q13sX1DoWMD36LO8xHXBtDKpN05T8PkMBf3QMQ44ehiq3pcq9LzHcgYQFQ8QRlPz8fHXu1EnV1dUyTVO57dursrJSeXl5evXVV1VeXq477rhDdrtdkjR+/HiZpimfz6fTBgyQ1+uV1+sNqg0AAACgPls+zFVSepVyT6X3G5GBAhtBSUlJ0QMPPKCnn35aFZWVuuWWWzR71iydfMopGj9+vO64/XZNuPhidevaVffdd5+mTp0ql8ulZ555Rnffc49WrVqljz/6KKg2AAAAED2S4tqrV/srgnrPh0u36qPlh1c1yexcovgUtyTJXRGj0oJE7drYRoMv/cbyWNF4rg1fqGLL48r95RTZYmPDHU6TocBGUGJjY/WToUP1xhtvyOFwaOjQoYqPj1d2VpbOOOMM2e12devaVf0HDJAk9enTR4WFhTIMQ4MGDdKOvDx9FmQbAAAAaN5shkOpCZ1lmn6lt+pe70zfBwvKVFZaXe/7i7bEafMKrySpvCheyW0qa44V70lS0e5UjZ7sltPB7PCNEai2y1/paOQ62PXzFhWpquRrmf6Wte45BTaCcuDAAfXv16/m9ZFltI4M75ZUq9f5qquuqvnzWT9YPz2YNgAAANC8xcW01gX9nz/qOc8+/oEWvvL5Mdva/WWbOvsSEp36ab9nlJDYcnpKrVT+dZrKv8pQ6hn5lrWZfvZZSj+v5X2nZ5IzBCUjI0PLV6zQmWeeqb59+2rVBx+oY8eOGjd+vBa9957i4+N166236vnnDyfQuU8+qd/+9rey2+16a+FCXXPNNUG3AQAAgObNMIxam2maunf6Qk259O8126rF3x67ocOt1dmqq7z61dWv6IMlW0N2D1HNNCTT2iYrtryn/Feukumrf1RCtKIHG0EJBAIqyM9Xtdstj8ej/Px8+Xw+VVZW6kBBgUzTlMvlUmFhoSSpuKhIJSUlkqSDBw+qvLw86DZw/FavXq3WaWkaMWLECbdlmqYWL16sDZ8f+5dkAIh0toQhsvsOyV+xypL27EnnyJbQz5K2gJbky/V7VFhQJlPS52t36WBBmSXtBgKmNm3Yq6LCckvaa2mSeveULeagJOt6sI2YeNkTM3T4R5CWwzBN0+LfKkLP5XIpJSUl3GG0WDExMfL5Dj+f4XA45PV6ZbPZZLfb5fV6ZbfbZRiGfD6fHA6HTNOU3++Xw+FQIBBQIBAIqg0E58wzz9TKVSf+BdI0TfXq2VPffBOdE4aUlpYqOTn6ntM6kh+j9f4i2YF8ly4aMVd+XyDk1+p8UoZe+s8NMoyW9aXlRPnKV6lq20hL2krovkn2uB6WtBUpWkL+aAn3GKkOryoT0G+mvqEPloaml9nhsOnW2efpZ5f3D0n70a5q1yfa/9IkKWDB92+bQ8kDJiljzKwTbysCBJM7GCKOoGRlZWnrtm06d8wYDRo0SHk7dqhbt26afOWV+vKrr5SYmKgHHnxQ7y9eLEl67fXX9dS8eXI4HPrk00/1q1/9Kug2AAAA0LwVH6zQxaOe1NoPvgtJ+/EJMXr5vSka87PeIWm/JYhr108dpn0oR+tOJ9xW9qQXlTbilhMPqhliiDiCUlFRob8+/bR25OWpqqpKf5k3TyUlJdq0aZOeffZZeb1erVq1Sjvy8iRJby9cWDMs/IXnn9dn69YF3QaCs3PnTt17772aNm2a2rSpOwnI8dixY4fmP/ssw/QBRBVbbEfFZs2S9+ATMv2Ny29GTEfFpP9chiPD4uiA6OYPBFR0sDxko3wMw1BaRqLi4mNC0n5LYNhjZW+VqZRB16pq+wpVblsWdBuOlHZK6nupYjNOki0uKQRRRj6GiCMoNptNWVlZKi4uViAQUEZGhg4cOCCn06nk5GTl5+crJSVFsbGxOnDggDIyMhQIBFRcXKysrCxVVFSooqIiqDbQOMuWL9dJJ50kwzCUlZUlm+3oA1b8fr/y8w8/d7NmzRpdcvHFTRFm2ETr8ECGP4YPQ8SbB9M0VbXtLAU8WyXTlOnL17Fn9rHJcGRJhmRPOEPxnV9vilCbXEvIHy3hHiNV0cFy/e9Fz6m4qEIet7WPAMbFxygzK0nz376OWcQtUrbxDRWvePj7HaZPprdAhqONJL9M/yEZjrYyAxVSoLImRzpz+ivr4qfCFneoBJM76MFGULKysvTt1q26YuJEFRcXa8nSperbp49GjhypBx58UJ06dtQDDzygnwwdqv79+um1119XQX6+Jk+erDVr1+qvTz+t+fPnB9UGGuec0aNlGIaSkpK0Y+dOJSUd/VfE/Px8nXzSSfJ6vWqGv7sBwHGL77ZYkin5y1T+dScpcPRJkQxHlhJ7fisZMWppk/UAVknLSNRry3+hO6e+YflM3+eNO1W3zDpXdjtPv1ql1anj1arXhTWvA9Vfq2LLACV0e1Gmr0BVO69Uq54fyVP4tLzF89Wqx3LJcEj88EuBjeAUFxfriokT9emnn8rj8eiySy/V3r179f7772vPnj2qrq7W008/rTfeeEOSdM/dd6va7Zbf79eUKVO0Iy8v6DbQOH6/X5JUVlamK664Qg7H0f93r66qUnV1NcU1gKh2uNf/cD407UmK7/iyTPPovWmGLU4y4mQYfHkHGsswDDkcdstH3sy491z1O72DHA67pe22dIZhk37wg4UtrrPiu7whe0JvmWY3xXf5h4yYNopJv0KOpKGS3UmO/C8KbATlyHBvj8cjr8+n4uJi+f1+VVdX69ChQzJNUxWVlTXFnMvlksfjkSSVHDqkqqqqoNvAifH5fPrXu++GOwwAiDiGESNHyv+EOwygRel8Uhu5SqpqXhceKNeenYeCa6NbhlJax0uGoWFnn6zsdjw6GmqGPVkxqeMO/1mSLaWdJB1eTSHKVlQ4UVQwCEpGRoaWLF2qyy69VMXFxVq+YoV69+qlkaNGac6cOcpt3163zJihocOGqc+pp+rxJ55Qfn6+Jl1xhd548039Zd48PfPMM0G1AQDNg9Fkg4eb7koAYK0bbxlZ6/UbL67TH+5dFFQbU24ZqRHnnmJhVIB1mOQMQXE4HOratav27t0rv9+vjh076rvvvlNCQoIyMzO1bds2ZWZmKj4+Xnl5eerYsaN8Pp/27t2rbt26qaSkRCUlJUG1AYRCtE5wwwQ+4ePz+Q/3wjTBv6oxsXbl5KYyyRks1RLyR0u4x+bGVVqlvTsP6ZdXvKjqKu9xveehpy6mwEaTYpIzhIzP59OWLVtqXn/zzTeSJI/Ho5KSEkmqmYlaOrxk1BHbtm2r+XMwbQBAc+Bw2NWpK0s3AUAwklPi5TwlRuMm9j/u2cVzclNDGxRwAiiwAQAAAISN0+nQr35zTrjDACzBVG8AAAAAAFiAAhsAAAAAAAtQYAMAAAAAYAEKbAAAAAAALBB0gb1q1SpdcMEFysnJkWEYWrhwYc0xr9er22+/XaeeeqoSExOVk5Ojq666Svv27avVRqdOnWQYRq3toYceOuGbAQAAAAAgXIIusCsqKtS3b1/NnTu3zrHKykqtX79ed999t9avX68333xTW7Zs0YUXXljn3Pvuu0/79++v2W666abG3QEAAAAAABEg6GW6xo4dq7Fjx9Z7LCUlRYsXL66174knntCgQYO0a9cudejQoWZ/UlKSsrKygr08AAAAAAARKeTPYJeWlsowDKWmptba/9BDDyk9PV39+/fXww8/LJ+v4YXl3W63XC5XrQ0AQH4EgKMhRwJoaiEtsKurq3X77bdr4sSJSk5Ortl/8803a8GCBVq+fLmmTJmiBx98ULfddluD7cyZM0cpKSk1W25ubijDBoBmg/wIAA0jRwJoaoZpmmaj32wYeuuttzRu3Lg6x7xeryZMmKA9e/ZoxYoVtQrsH3v22Wc1ZcoUlZeXy+l01jnudrvldrtrXrtcLhIkgBNSWlp61LzUXDSUH6Pl/gA0HZfLpZSUlKjKH+RIAFYIJj8G/Qz28fB6vbr00ku1c+dOLVu27JhBDB48WD6fTzt27NApp5xS57jT6ay38AaAlo78CAANI0cCaGqWF9hHiuutW7dq+fLlSk9PP+Z7NmzYIJvNpszMTKvDAQAAAACgSQRdYJeXl2vbtm01r/Py8rRhwwalpaUpOztbF198sdavX693331Xfr9f+fn5kqS0tDTFxsZq9erVWrt2rUaNGqWkpCStXr1a06dP1+TJk9W6dWvr7gwAAAAAgCYUdIH92WefadSoUTWvZ8yYIUm6+uqrNWvWLL3zzjuSpH79+tV63/LlyzVy5Eg5nU4tWLBAs2bNktvtVufOnTV9+vSadgAAAAAAaI6CLrBHjhypo82Ldqw50wYMGKA1a9YEe1kAAAAAACJayNfBBgAAAACgJQjJLOJAKDVmRtCKigr5/f4QRQQAkcHrN+ULBLf6ZqzdkN1mhCgiAABaFgpsNDu33HKLbr/jjqDeM+bcc3k0AUDUe39ruRZtKQvqPdOHZahLemyIIgIAoGWhwEaz0L9/f90wZYokaeDAgUpKSgrq/bffcYfy8/Pl9/t158yZcrlcoQgTAJrcrkMercyrkCTtPOSV2xdcD/Z/tpQpOc4mm2FofO9kJcTw9BgAAI1FgY2I5XQ61bVrV8kwNHzECE35b4HdGD/72c8kHV6n/bV//EMHCwvl83r17bffWhUuADQZr9/UwQqfZErfFnr0QV5lo9v6Yn+1JMlmSAPbxysp1iabTWrbyiHDYOg4AADBoMBGxOratau+/OorS9t0OBxatny5JGnv3r3q0rmzfD6fpdcAgFA7WOHTrMUHLG0zYEp/XFUoSUqNs2nO2CzZqa8BAAgKBTYi0q233qrLJ06UpGP2oGzYsEE3TZtW8zohIUFvLVyohISEOuf+sK02bdpoxYoVmjlzpj744AOLIgeA0HpvS5k+2VN1XOe2T4nRFf1Sal67/aaeXF0sr//ow8jLPAH9YWWhxvdO1sltgptUEgCAlowCGxHF4XDo/PPP1znnnKMBAwbUe87XmzZp8+bN37/evFkfffRRzev4+Hi9+eabio+Lq9k3YuRIZWRk1GrH6XTqJ0OH6qf/8z8yDEOrVq2y+G4AwDr+gKkv86v19QG3dpd46z0nO8mh7GTHD17HqFvG9wWyx29qQE6cvD+YaXzLQY8qPIEfXUvaXuzRl/mHh49TZANoqfJL1svtK621z25zql3rM2QYzFmBugzTNIObDSUCuFwupaSkHPtENDtJSUnas3dvvZOYmaYpn8+n2bNn68EHHgiq3WXLl2vYsGFyOOr/TWnVqlUaOWJEo2JG81RaWqrk5ORwh2G5I/kxWu+vJavyBnTbv/MbnMTMZkgX9EjST3sE9/f+h5UHta3Io4ZW9zopI1b/N6JNsOGiGWoJ+aMl3COs9Z8vpuiAa6MkyTDsMmQoPjZDF/ZbIMP4/nulw2Fj3oooFkzuoAcbzYbP59Og00/X9u3bg37vJRdfrHHjxumvf/tbCCIDgPCyGdJvzmqjNonB/7N+4xlp+nxftV5YX2J9YAAQJQzDrv/p96ziYtJUXFipS876i/z//cHTMKQnX7lSuZ3SwhwlIgHjGhAxTj/9dN15552Kja27HuvXmzZp9uzZ2r59u8rLy4Nuu6ioSB99/LFmz56tsrK6a8R27NhRs2bNqjOMHAAiQV6xR//+pky+erqZs5McuqBHktokOhTXiCW2Wjnt6pYeq//pkSSno27vS1GFX+987VKZ29+o2AEgGpgBUx99tEbvvPaZXp+/RQfzy1V08PBWeKBcC579RB8t2xruMBEBKLARMQYOHKjb77hDTmftZ/0OHjyojz76SA8+8ECjiusjvtm8WQ/cf7+2bt1aZx3sjh076u577qHABhCRdh7y6L1vy+Wv/ai0WsXa1C09Vj/tkdyo4vqI7OQY/bR7kjJbORT3oyK7uMqvdzeXqdwdaODdANB8+P0BHSwo04F8V62tzFVdc47bV6YqT7F++CRtwAxo+cr39I+X39er8z/Rj5+yffOldVr6780qLChToKFnbtAiMEQcEe+Siy+uNYnZifD5fDpj8GDdc++9uuuuuyxpEwDC5cYz0tQtve6on8awGdKdo9ron5vL9O9v6o70AYBoUFxYoUtGPSnfj36xHD9xgG6ZNUaS9PmOeTrg2qgL+j8fVNuLFn6pj5Zt1VsfTFNCIpNDtlT0YCMiPPbYY7phypRa+3bt2qWLxo/XV199Jb/fuqGJPp9PL7/0kv73uuvqrIH92OOP6/obbrDsWgBwIkzT1CsbSrQqr6LW/tbxdv3ijDTlJMfIZrNmUh3DMGS3GRqcG6+rBqTqx3P1vLKhVKu+q6j/zQDQDLz/z0164I535fH45PcFam0fLd+me369UO5qr07O+pl6trtMyzffoZLKHSovjtN3n+TI57UprX2ZOvbPl1S3l9o0JZ8voOY3hTSsRA82IsLZo0erR48etfaVuVxauHBhSK73zTffyOVy1RreYxiGRo8erW+//TYk1wSAxth8wK38sto/BsbFGOqXExeSGWuzk2MUF2OTodpfH7856FbbJL42AGi+3NVeuau86jswV5JUXubWtm8OSJL27ynRocJyfT7+VHXrka22Ka20Lf9fMk2/zIAhn8eu8uJ4BXw2OWICqpMkgf/iX0oAAAAAUeFI54lhGLX+LEkXXNJPF1zSr+bczz/ZpV9OfKHmdXW1T9OvXaDb7h+rcZf315g+T+o/X9woT/pGnfSTPfrkjVNUWRrXdDeDZokCGwAAAEBUmLXuZa0v3K53xtytycv+oIy4FP15aP2P//U4NVsvvzdFt/7vq9q3u6TWsT07D+m2G/6h8ups+QKtJRmqKrNmzgtENwpsRKSVK1ZoydKlIb1GRUWF/jJvns7/6U/VpUuXkF4LAKxyckasemSGdvIcp93QiC6J+nJ/tQorWZ4LQOTz+H36+7dL5Q8ENKjNyZKkARndlBwb3+B74uJj1Klrun46oY+KDn6/Uk1JcaX+/eZG7dheJCnmvxtwfCiwEZFeeeUVPf300yG9RmlpqW6++Wa1b9+eAhtAszEoN0HDuySG9BoJsTZN7JeqQ1VFFNgAmgW336NZn72k3w68RD8/aYRMT7mmdx8tSfK7y+SzSXZbrBz22j9QGoaha286s9a+3/3m31q44PMmix3RhQIbAAAAQLPWKiZe31z+F2n//6n0k2t06KPsmmMBQ3rjlBgN7vELjejGajEILQpsRLxbb71VXbt1O+F2Pl+/PuS94gDQlPa/tEDVe/eecDuJJ5+szHEXWBARAISH6XfLvfz38pV8IX+5XaanXIen+pZkmBqa6FLaztdVlPedEk8pUUzGjbIn9Ku3rTE/6620Nq307OMfNFn8iB4U2IhI2Tk56tq1q7Zv364Lf/YzDRs27ITbfOP112sV2A6HQyeffLKSkpNPuG0AaCol1X4dKPepTaJdhz74SOVffnXCbfpGDq9VYPsDpgrKfar2sgYNgGbC71PZF6/J9FRIqv0YjSGpu1kl9771cuV/I5uzQEZMbxn2FNmcnes01W9QByWnxlNgo1Fs4Q4AqM8999yjt0K0BvYRmZmZWv/55zrrrLNCeh0AsNK7m8v05OqikF7D5Q7ot0sO6JuD7pBeBwCahCmVrmmr6l2tZHpsOvRBtsq+uEPVu34e7sgQhejBRkQyDOPIoJ7QX8doiisBgHWaol+ZvmsAzUXxyg+U/+rLcmZXy6i3+7Dud72Kb1Llr26thJNCHh5amKB7sFetWqULLrhAOTk5MgxDC3/Uy3jNNdfUFC1HtvPOO6/WOcXFxZo0aZKSk5OVmpqq6667TuXl5QIAAACAYHiLilTx1dcyA4Hjfo+/IkblBRXa9u0SedwVIYwOLU3QBXZFRYX69u2ruXPnNnjOeeedp/3799dsr7zySq3jkyZN0qZNm7R48WK9++67WrVqlW64gRn9WjKv1yufz1drn2EYiomJkS0EPcw2m00xMbXXNDRNU16vV34/S9IAiBx2Q6qTBk3Jb4amlzlgmvIH6rZsMw5vABAtDhV9p/feuUUV5QXymwF5/D6Z5uH8ZxiSI8ZWX+c3cFRBF9hjx47V/fffr/Hjxzd4jtPpVFZWVs3WunXrmmObN2/WokWL9Le//U2DBw/WsGHD9Pjjj2vBggXat29f4+4Czd45o0fr7rvvrrXvpJNP1o4dOzTw9NMtv970GTO0Zu1aORy1n5IYffbZuvuuuyy/HgA01ozhGRrXs/ZkjAXlPs38T752HvJYfr3FW8s1Z/lB/bjGnnFmhsb1YlJIANEj1fRrlLtMCWZAL3y7TL1f+6UqfNWSpNzOaXpr1TT1ODX7GK0AtYXkGewVK1YoMzNTrVu31llnnaX7779f6enpkqTVq1crNTVVAwcOrDl/9OjRstlsWrt2bb2Fu9vtltv9/UQrLpcrFGEjjA4ePChXaWmtfTExMcrOybH0OjabTdNnzNAF//M/atu2bf1x8N9XUI58pklJSXK5XHr0T39SIIghWjgx5MfoZhiGkpx2xcfU7kIJmFJpdUAFGR1lq6pQWtGJL9UVME0t3lqujfurVeau+/9wktOm+BjmRg2GaQa0ae8r8vorFWtPUM92E2XU/4AoQoQc2TIk9uiu7KsnyVPwR8k8/h8ebZLiZOrA629pn61UO40Dun/9q4q1fV8ifXHKNypIcskwpexPc2T32htsr0efbI0c012xsQ2fg+hneYF93nnn6aKLLlLnzp21fft23XnnnRo7dqxWr14tu92u/Px8ZWZm1g7C4VBaWpry8/PrbXPOnDmaPXu21aEiwlRUVGjfvn3KysqSzWb9F5D4+Hjl5ubq1ltvrVNcu91uHTx4sM4wdRyb3W7XjOnTlZ2To7179+qxP/+ZArsJkR9bhliHoZQ4m0qra/+/VZTZSc7qCrVyFSnGW93okYxev6niSr/e/7a8TnFtt0lJsTbZGR8eNNMM6Ou9r6jKU6SE2DbqkXMZBXYTI0e2DK16dFdCl/bK+/1cKeCV4Tj+B2hMU9r5zts6kBMvz+lO/W7D67VP6PzfzW+o9bbWiiuJl91TfwHd49RsXXXjTxp/I4gKlmf5yy+/XBdeeKFOPfVUjRs3Tu+++64+/fRTrVixotFtzpw5U6WlpTXb7t27rQsYEeOFF17Qqb17q6IiNBNNnDd2rL7durXODzyS9Mknn6hL587atm1bSK4NhAr5sWU4o0OCZp3TVk5H3SLXldpWm/qfK1+Ms9Htf5VfrbveK6i357pz61jNGZulNon0yKD5IUe2HKbPUMkH7eTel3jsk3/kX10d+izrGDnOZurLq77U/tP2NzJCtBQh/xm1S5cuysjIqClcsrKydODAgVrn+Hw+FRcXKysrq942nE6nkpOTa22IPqZphrQH+cjEZvUtyxXqawOhQn5sGWyG0fAEY4Yh02bXzq6n6VB6u6Db3pvbS/k5J9d55voHzctuY0lDNE/kyJbD5nSqy713KzbrLJVvSpMZxCyQQ/b51b34GKPvDEk2qbBnoQov36sHnrxIPfta+ygjokPI18Hes2ePioqKlJ19eIKAIUOGqKSkROvWrdNpp50mSVq2bJkCgYAGDx4c6nAQ4fx+vz766CP17du35r+ZUNu0aZO++OKLJrkWADSWzZC6pcdqT6m3zlBxGYZcrbMU462Ww1Nds9vu9ymh8vv5LUwZqkhqLfMHg8lLW2epolVr1Sc7yaH2KTH1HgOASGI4HGo9dIiqd+6Sr6RM0vLjfq/Laagy5vh+RGzbMUl92rTX8NEn6723vlJiklMndT88OrJ9p7TGhI4oE3SBXV5eXmsYbV5enjZs2KC0tDSlpaVp9uzZmjBhgrKysrR9+3bddttt6tatm8aMGSNJ6tGjh8477zxdf/31mjdvnrxer6ZNm6bLL79cORZPaIXmp7KyUuePHaunnnpKN0yZEtIekyPLMNz1m9/o7bffDtl1AMAKTodNvxqWoRfXH9KqvMp6zynK7KSizE41r1u5CnXSplU1rwN2u7b1GKqA/fiK5nG9ktW/XfwJxQ0ATSlr4qVKO6e/9j69otaSW/UxJcmQZg11arv9+Mqia7ufo3tOm1jz+qQebfXUK1eeYNQtgxnEsILmPGoq6AL7s88+06hRo2pez5gxQ5J09dVX66mnntLGjRv197//XSUlJcrJydG5556r3/72t3I6v3827KWXXtK0adN09tlny2azacKECXrssccsuB1Ei9mzZ+tf//qX3n7nnZD9D3bgwAGdM3q0duzYEZL2ASAULuiZrFOz4jR3dfExz61ITNXXfUd/v8MwFLAd+5/+JKdNM87MUHoCz10DaF5mrXtZ6/I3680p72v77AfkLd6oVj3rz5elhk1fOOJ1VXWRVsckapEzJahr3XLvGPkber4GdZRV79GyTbdJMpXTerAGdr5J7395k6q9h2qdl57UQ2eecm94grRA0AX2yJEjj/rrw3vvvXfMNtLS0vTyyy8He2m0IPn5+Vq3bp2eeuopjR8/3rLh4n6/Xy++8IIqKitVUlKir7/+mhmvATQrKXF2dWwdq5FdEhXzSb48JcUqS6277KAkmXaH3AnH98ypIemMjgmKtRtKiDGUneyQrRn3IABomfIrD2n9oZ36W8EWDek3WEm7U1S1c5ni2pXXml3ccyBetlbZ6jzyLK3ZvlIHjjEXj02Grjz5LA1s061mX2Y2z/Qfiz/g1faCfysz+VSZkkqrdkiS7LZYbcl/S8UVW+X1157gOGD69c2+N9S5zTlyxjS/zzjkz2ADjbV//35NmzpVffr0saTA9nq9Ki0t1R133KGCggILIgSA8EiNt+uK/qn6+i952pNfpIpWh5/7M202mbbgep1tfp9izIASYm26qHeyUuLotQbQfMXbY1XhdWvm2r/rzTF3KnPrySp96Fv5srfL6TAUa3OozFulsv3Jcrbtqf7Df60pRbv0beneBtt0GHalxCboocHXKCuh/jkrUD9/wKNP8x5T/443qE1S75r9xRVb9cn2R+p9T1n1Hq3d/gdlJPWQ3dZVDnvjV8kIBwpstBj//ve/deXkySFbBgwAwiHlUL5OXfcfSVJ++1NU0O6UoN7fZcsatUvoqc7nTah3GTAAaE4eGnyNfnv6ZEnSuPfuV5YzRc+8sUC93rhR13U/V1edfJZG/OOX8o4ISMYO2V66RpXe6qO2eX6HgXrxrFuVGBPXFLcQldblPSmbEdwCVu9tnKqTs8fp9C6/ClFUoUGBjRbD5/WqvLw83GEAgKUMmbIHDg9tTC3aq9jq+idAa0h8ZakcCiguJuQrdwJAyMU5YhWnWEnSzadeqHiHUzGJrTRryP+qZ+sOat0qUw8Pn6ZAEBNudU7OUlIsEz42ht0Wq0Fdfq1tBf/SAVdwq/b4AtXyBzwhiix0KLAR8Xbk5al1auoJt7N79+4TD6aFSEhIUKfOnY/r3BiHQ46YwzMSOxwO9ezZUz6//7jem5eXp6rK4IoBAN9z5mTL/4MfDg9//Qtywp3EbMVmtrEyrKjm81eprHr/cZ1rmn4FAofzYcD0qaQyT4ZxfEPwk+Jy5LDTWwaciJ91OqPmz1eefFbNn6/vcV44wmmR7LYYnZR1gQrLvg66wG6uKLAR8a666qpwh9DinHbaaVqxcmXQ78vMzNTnGzYc9/lnnnmmPv7oo6CvA+CwLnfPDHcILU5h+Td6b+Mvg35ftfeQ/vn58f97dl6feWqb0jfo6wAAwosCG0C9GrM8WrDv4WlP4MQ053VCAQA4ll2FK+Wq2qOzev6u2YzqocAGUEdhYaHeeP314zrXsNk0duxYxcfHq6qqSv/5z39kHufSZ4WFhScSJgA0ubiYVHVIH3mcZ5vaW7xaftMjuy1W7VoP0fH+tBgXE9x6vAAQydJanazs1NO1v+TToN5X5S2Sr6xKpnl8jx9GAgpsAHVs3rxZl1xyyXGdGxMTox07dig+Pl7FxcW6/LLL5DvGWpIA0FylJnTWqJ5zjuvcQMCn1z8dpypPkZyOFI3ofr9sNr56AWh5Tsker8zkvnpn/aQg32nIZjjUnMY9kuUBAAAAABGna+ZYndb5l3LYm88s7hTYAAAAAICI47DHKT42PdxhBIUCGwAAAAAQUjbDroTY4JaFjHW0ClE0oUOBDQAAAAAIqeT4Dppw+pvBvakZrpZBgQ0AAAAACCnDMGQY0V9+2sIdAAAAAAAA0YACGwAAAAAAC1BgAwAAAABgAQpsAAAAAAAsEP1PmQMIKdM0VVZeroTSUpWVlYU7HACIKDH2BPns1XLYE8IdCgCgCVBgAzghPp9Ppw0YIMMwZJqmfD5fuEMCgIhgGHb9T//nJNOUDEOGYQ93SACAEKPABnDCKioqwh0CAEQcwzAUQ881ALQoPIMNAAAAAIAFKLABAAAAALAABTYAAAAAABagwAYAAAAAwAIU2AAAAAAAWCDoAnvVqlW64IILlJOTI8MwtHDhwlrHDcOod3v44YdrzunUqVOd4w899NAJ3wwAAAAAAOESdIFdUVGhvn37au7cufUe379/f63t2WeflWEYmjBhQq3z7rvvvlrn3XTTTY27AwAAAAAAIkDQ62CPHTtWY8eObfB4VlZWrddvv/22Ro0apS5dutTan5SUVOdcAAAAAACaq5A+g11QUKB//etfuu666+oce+ihh5Senq7+/fvr4Ycfls/na7Adt9stl8tVawMAkB8B4GjIkQCaWkgL7L///e9KSkrSRRddVGv/zTffrAULFmj58uWaMmWKHnzwQd12220NtjNnzhylpKTUbLm5uaEMGwCaDfIjADSMHAmgqRmmaZqNfrNh6K233tK4cePqPd69e3edc845evzxx4/azrPPPqspU6aovLxcTqezznG32y23213z2uVykSABnJDS0lIlJyeHO4wT1lB+jJb7A9B0XC6XUlJSoip/kCMBWCGY/Bj0M9jH64MPPtCWLVv06quvHvPcwYMHy+fzaceOHTrllFPqHHc6nfUW3gDQ0pEfAaBh5EgATS1kQ8SfeeYZnXbaaerbt+8xz92wYYNsNpsyMzNDFQ4AAAAAACEVdA92eXm5tm3bVvM6Ly9PGzZsUFpamjp06CDpcBf6a6+9pj/+8Y913r969WqtXbtWo0aNUlJSklavXq3p06dr8uTJat269QncCgAAAAAA4RN0gf3ZZ59p1KhRNa9nzJghSbr66qv13HPPSZIWLFgg0zQ1ceLEOu93Op1asGCBZs2aJbfbrc6dO2v69Ok17QAAAAAA0Byd0CRn4XLkIXMAaKxoneAmGicpAtA0WkL+aAn3CMB6weSOkC7TBQAAAABAS0GBDQAAAACABSiwAQAAAACwAAU2AAAAAAAWoMAGAAAAAMACFNgAAAAAAFiAAhsAAAAAAAtQYAMAAAAAYAEKbAAAAAAALECBDQAAAACABSiwAQAAAACwAAU2AAAAAAAWoMAGAAAAAMACFNgAAAAAAFiAAhsAAAAAAAs4wh1AY5imGe4QADRz0ZpHjtyXy+UKcyQAmpsjeSNa86NEjgTQOMHkx2ZZYJeVlYU7BADNXFlZmVJSUsIdhuWO5Mfc3NwwRwKguYrW/ChJRUVFksiRABrnePKjYTbDnykDgYC2bNminj17avfu3UpOTg53SBHF5XIpNzeXz+ZH+Fwa1pI+G9M0VVZWppycHNls0feUDPnx6FrSf+vB4rOpX0v6XKI9P0pSSUmJWrdurV27dkXtjwiN1ZL+Ww8Wn03DWspnE0x+bJY92DabTe3atZMkJScnR/Vf5ongs6kfn0vDWspnE81fqsiPx4fPpmF8NvVrKZ9LNOdHSTVfjFNSUlrE32djtJT/1huDz6ZhLeGzOd78GJ0/TwIAAAAA0MQosAEAAAAAsECzLbCdTqfuvfdeOZ3OcIcScfhs6sfn0jA+m+jC32fD+GwaxmdTPz6X6MLfZ8P4bBrGZ9MwPpu6muUkZwAAAAAARJpm24MNAAAAAEAkocAGAAAAAMACFNgAAAAAAFiAAhsAAAAAAAtQYAMAAAAAYAEKbAAAAAAALECBDQAAAACABSiwAQAAAACwAAU2AAAAAAAWoMAGAAAAAMACFNgAAAAAAFiAAhsAAAAAAAtQYAMAAAAAYAEKbAAAAAAALECBDQAAAACABSiwAQAAAACwAAU2AAAAAAAWoMAGAAAAAMACFNgAAAAAAFiAAhsAAAAAAAtQYAMAAAAAYAEKbAAAAAAALECBDQAAAACABSiwAQAAAACwAAU2AAAAAAAWoMAGAAAAAMACFNgAAAAAAFiAAhsAAAAAAAtQYAMAAAAAYAEKbAAAAAAALECBDQAAAACABSiwAQAAAACwAAU2AAAAAAAWoMAGAAAAAMACFNgAAAAAAFiAAhsAAAAAAAtQYAMAAAAAYAEKbAAAAAAALECBDQAAAACABSiwAQAAAACwAAU2AAAAAAAWoMAGAAAAAMACFNgAAAAAAFiAAhsAAAAAAAuEtcCeO3euOnXqpLi4OA0ePFiffPJJOMMBAAAAAKDRwlZgv/rqq5oxY4buvfderV+/Xn379tWYMWN04MCBcIUEAAAAAECjGaZpmuG48ODBg3X66afriSeekCQFAgHl5ubqpptu0h133HHU9wYCAe3bt09JSUkyDKMpwgUQJUzTVFlZmXJycmSzRd9TMuRHAI0V7flRIkcCaJxg8qOjiWKqxePxaN26dZo5c2bNPpvNptGjR2v16tV1zne73XK73TWv9+7dq549ezZJrACi0+7du9W+fftwh3HCyI8ArBYt+VEiRwKw1vHkx7AU2IWFhfL7/Wrbtm2t/W3bttU333xT5/w5c+Zo9uzZTRUegBYgKSkp3CFYoqH8uHv3biUnJ4chIgDNlcvlUm5ubtTkR4kcCcAaweTHsAwR37dvn9q1a6ePP/5YQ4YMqdl/2223aeXKlVq7dm2t83/86+ORGwSAxiotLY2KL1cN5cdouT8ATcflciklJSWq8gc5EoAVgsmPYenBzsjIkN1uV0FBQa39BQUFysrKqnO+0+mU0+lsqvAAoNkgPwJAw8iRAJpaWGawiI2N1WmnnaalS5fW7AsEAlq6dGmtHm0AAAAAAJqLsPRgS9KMGTN09dVXa+DAgRo0aJAeffRRVVRU6Oc//3m4QgIAAAAAoNHCVmBfdtllOnjwoO655x7l5+erX79+WrRoUZ2JzwAAAAAAaA7CVmBL0rRp0zRt2rRwhgAAAAAAgCXC8gw2AAAAAADRhgIbAAAAAAALUGADAAAAAGABCmwAAAAAACxAgQ0AAAAAgAUosAEAAAAAsAAFNgAAAAAAFqDABgAAAADAAhTYAAAAAABYgAIbAAAAAAALUGADAAAAOKYV28v1/rdlMk1T73zt0ie7KuX1m1qwoUTfFrrDHR4QERzhDgAAAABAZHJV+1XuDkiS1u+tlt80dc5JrbSrxCu7IQVMU98Ve9QtIzbMkQKRgQIbAAAAQL1WfFehdzeX1bw+KSNWhmFo6pC0mn0zR7UJR2hARKLAbkEyMjL02uuvy263N+r9zz7zjJ577jlrgwKACFDm9mvemmIFzMa9f1inBA3tlGhtUAAQJlsOurVwk0uSVFzpV6tYm248I002Q4qLOfyEqWEYx93eh3kV+mhnZc3r3JQYXdE/1dKYW7pvvy7QH2ctOuo5/3ffWHXrntlEEbVcFNhRzjAMjR07VnHx8UpLS9PQoUPlcDTur33vnj1ylR3+BXPN6tXat2+flaECQJMyTVNf5rvl9Zuq8Aa0rcgjs5EFdut4u+Ich790dk2PVWp8437IRG3uaq9Wr9wus5G/fHQ6qY06d8uwOCogum056NbG/dXaXuTRqVlOpaTFKDHGpm7psbLZjr+o/iGnw1B8jKGv8g8/p13pCWjd3ir1zHQqPoYpoU7Ul+v36NOP8rRx3Z6jnvfB0m9VXeVR7/7tmyiylskwzcZ+nQgfl8ullJSUcIcR8QzDUHx8vLZu26bs7GxL277k4ov19ttvy+fzWdpuSxUTE1PzWTocDnm9XtlsNtntdnm9XtntdhmGIZ/PJ4fDIdM05ff75XA4FAgEFAgEwnwHzU9paamSk5PDHYbljuTHaL0/qwRMU16/qd8sKpDLbe3/P1MGp6lfTpzsjfwiiu8dyHfpohFz5fc17u/of381XNfdfKbFUUWvlpA/WsI9NkYgYOrI/2V/XFmo7cUe2Q1pztgsy34wPFTl18z/5NcaKXTP6EzlJDkaXbi3dKZpyucL6DdT39AHS7ce13tGntddc+ZOCHFk0SeY3MFPRlFs7Nix2rptmzIzrR8K8penn9ar//iH5e22RFlZWdq6bZvOHTNGgwYNUt6OHerWrZsmX3mlvvzqKyUmJuqBBx/U+4sXS5Jee/11PTVvnhwOhz759FP96le/CvMdAM3PV/lu/ea9ApVZXFxL0gvrD+kva4stbxcAQuWtTS7N/E++Zv4nXzsOeZrsuo+sKtTCr11Ndr1oU3ywQhePelJrP/gu3KHgBxgiHqWuve46jRkzxvKe6yPS0tI0YMAAzZo1S0888YQKCwtDcp2WoKKiQn99+mntyMtTVVWV/jJvnkpKSrRp0yY9++yz8nq9WrVqlXbk5UmS3l64UOXl5QoEAnrh+ef12bp1Yb4DoHn5MK9Cmwqq5aoOzciPSq+pXYe8eudrl0Z1TVSSk+HiACJbpTeg0h/kxOwkhwblxivOYV3PcpzD0AU9krR2d5Xyyw6P2iv3BFTlbXaDaSOGPxBQ0cHyRo/yQWhQYEcZh8OhzMxMTbnhBp0+aFCD57nd7uMqiuPi4pSenl7vsY4dO+rue+7R8uXLtWHDBpWWljY67pasoqJC8+fPV3FxsQKBgJ555hmVlJSoavNm7du7V16vVx9/9JE+iz28/MW7775bMyz8lVdeUUVFRZjvAGge/AFTLndAq/IqtOOQt8Hz7DYpKfbYA7w8flOVDXwxLK7y693NZTqljVO5KYYSjqM9AGhqpmmqtDogj+/7XNYq9vDz1j/tYe0Q+vgYm37aI1mFlYeX/Sr3HC4KPT5TJVV+pcTZgpo4raWrKHer+GCFgp08xF3t1cGCMqVlJMpu59+mUKDAjjInn3yy1n/++TEnMvvkk0909llnHbO98RddpFdfffWo5yxeskTznnpKN998c1Cx4rCsrCx9u3Wrrpg4UcXFxVqydKn69umjkSNH6oEHH1Snjh31wAMP6CdDh6p/v3567fXXVZCfr8mTJ2vN2rX669NP64EHHgj3bQARr6Dcp98uOSD/Mb6LdG4dq1uGH3tirPV7q/TXTw4d9Zw/fVCoEV0SNbFfahCRAkDTqPaZmrW4oFYv8o1npKlbeujWtL6yf6oG53r0yAeHO3rW7KrUF/ur9NDYLMXFUGAfrwXPrtX8Jz6U/1j/qP3ImpXbdclZT+r15VOVkdkqRNG1bBTYUeTGG2/UhIsvVkxMTL2/ABYVFenGKVPk8/tVVFh4XBOUffThhxo/frwk6fLLL9dll11W67hhGHI4HDr/pz9VWnq6/ve661RdXW3NDbUQxcXFumLiRH366afyeDy67NJLtXfvXr3//vvas2ePqqur9fTTT+uNN96QJN1z992qdrvl9/s1ZcqUmqHjABq2Ynu51u+tbrC4Tow1NLl/a9kMqZXTdlwTlJ2U4dQvzji8Duyne6r02Z6qOucETOnL/dWq8BTrqtNaK9bOl0cAkeGr/Got316hap+pH6ZGm6GQTjpmsxn6YfOmDhf6f/3kkM7qlqhebeNCdu1ocvb5PZWZlayHfvNvBYJYaaFXv3a6cspPlJTC5xwqFNhRwG63a/DgwTpv7FidffbZ9Z6zY8cOrVu3TgsXLpTf7z/utvft26e3Fy6UJKW1bq2OHTtq0KBBstlqDynp0qWL2rZtqxeef15ffPGF8vPzG30/LU0gEFBxcbE8Ho+8Pp+Ki4vl9/tVXV2tQ4cOyTRNVVRW1oxKcLlc8ngOT0BScuiQqqrqfqkHcJg/YCqv2KOvCtz65qC73nPSEuzqmBoT9MzfqfF29W8XL0mq8AZUVOlTXnHdoeeFlX653NU6o4Nb7VNiWMILQEQorPDpy/zwdIrEx9h0Ukasdhzyyus3D/8YmV+trCSHEmNt6phaf2cRvtepW4YSWsXKsBlSEAV2RtskDT/n5BBGBgbeR4GEhAT9Z9EiXXjhhXWOmaYp0zQ1/9lndcnFFwdVXP/Y/PnzdfGECfL5fKpvdbfExMQG40DDMjIytGTpUg0bNkx9+/TR8hUr1LFjR/1s3Dj969//Vnx8vG6ZMUPzn3tOkvT4E09o9n33yeFw6I0339TVV18d3hsAIpjHb+rPHxVp4/6Gv0QO65SgXwxJP6FltYZ1StSNZ6SroQ5qj9/UY8eIAwBaitzUGN06PEPpCbV/cFy8tVzzPzv6ozdApKMHO8r5fD6NPvtsbd682ZL2Dhw4oL59+ujxJ57Q6NGjLWmzpTvyme7du1d+v1+9e/XSd999p/z8fC1bulSVlZWaPXu24uMP95RdOXmyfD6fvF6vRo4YoZKSkvDeANBM2QxpxpkZyk625p/CZKdN94zO1CsbShvsLQeASNUm0a5pP0mvU/QCCA492M1cr1699L//+7+KiYmpc2z79u2aN2+evvjiC8uW0fL5fNqyZYtee+01LXzrrXrPGTFiRJ1ntdEwp9OpkSNHqk2bNkpNTdWoUaOUkJCgdu3a6czhw2W323XyKado0H9nhe8/YIB69e4tm82mnwwdqo6dOoX3BoAItbfUqw93VMhfz9C5jES7RnRJVG5qjGXLaNlthrKTYzSwfbz65dT/bNuWQrc+2V1Z7yggAGgKpmnqk92V+raw9nrXDruhrCSHYh1NVx4Myo3XSRm1J1Sr9AS04rsKlVY3ftRlSxGfEKvxEwcou31quEPBD1j+f9CcOXN0+umnKykpSZmZmRo3bpy2bNlS65yRI0fKMIxa24033mh1KC3C8OHD9cdHHlFcXN0vcxs+/1y/uvlmuVwuy6/716ef1p/+9Kd6j0284grdddddll8zWiUnJ+v+Bx5Qz5491blzZz04Z44yMzN1xhln6N5775XT6dT4ceP06+nTJUk3TpmiSZMmyWaz6c477zyu2eCBlmhroVuvbXSpvuVBO6TGaGK/VMXHWP9FcniXRI3uVv/MrJ/urtK/vimz/JoAEIx3N5fVmpgxxibFNfEkjIZh6H96JOuMDgm11tsurQ7olQ2lOlh+7Ml4W7qk5DjdMmuMTuqRGe5Q8AOWDxFfuXKlpk6dqtNPP10+n0933nmnzj33XH399ddKTEysOe/666/XfffdV/M6ISHB6lCAZiE/P1+dO3VSdXW1TNNUbvv2qqysVF5enl599VWVl5frjjvukN1+uJdt/PjxMk1TPp9Ppw0YIK+34fV8AQAAjuWiU1M0rFN4vosP6ZigU7PidPd7BXIHueQUEIks/+l+0aJFuuaaa9SrVy/17dtXzz33nHbt2qV169bVOi8hIUFZWVk1W3Jyw4vZu91uuVyuWhuO7uGHH9Zfnn46pNfYunWrbpwyRfv37w/pdaJdSkqKfve736lXr17q3KWLHv7DH5SZmakhP/mJfnv//XI6nZpw8cW69dZbJUlTp07VVVddJbvdrrvvuUejzzknzHeAcCI/Bu/ck1tpeOfEY594AjJbOTSpf6qSnTyJBYQTOfL4xNgMOR22sMzc7bAZcjoMiUnDG+2iSafpiusGN3g8I7OVbvvtWN12/1iNm9i/CSNrmUL+L39paakkKS0trdb+l156SRkZGerdu7dmzpypysrKBtuYM2eOUlJSarbc3NyQxhwN3nn7bS1ZvDik18jPz9fTTz/NJFsnKDY2Vj8ZOlTp6elKSU7W0KFDFR8fr+ysLJ1xxhmy2+3q1rWr+g8YIEnq06ePunfvLsMwNGjQIHXg/4cWjfwYvH7ZceoZ4nVWU+PtGt45QQmxFNhAOJEjm6/CCr+KKxkmfjwGn9lFoy/opc4nZdS79eiTo3ET+2v8xAEaPKxLuMONeiGdRTwQCOjXv/61hg4dqt69e9fsv+KKK9SxY0fl5ORo48aNuv3227Vlyxa9+eab9bYzc+ZMzZgxo+a1y+UiQSJqHDhwQP379at53efUUyWpZoi4pFqPU1x11VU1fz5r1KimCRIRi/wIAA0jRzZfz352SAPaxenGM9LDHUqz0L13ll76zw0NHmdd8aYT0gJ76tSp+uqrr/Thhx/W2n/DDd//5Z966qnKzs7W2Wefre3bt6tr16512nE6nXI6naEMFQCaJfIjolnrtAQ9+fJkNXbS9aychh8/Q8tAjvze+ryn5PFXaHDXW3Td6a31ye4qLd5aHu6wYBEK6MgRsgJ72rRpevfdd7Vq1Sq1b9/+qOcOHnz4mYFt27bVW2ADAICWJybWoT6n0dsIWCHWkSS3v0y7ilbokKevqrwUZEAoWF5gm6apm266SW+99ZZWrFihzp07H/M9GzZskCRlZ2dbHQ4AAADQ4vXOnayC0g1678ubtL7o96r0tZNkD3dYQNSxvMCeOnWqXn75Zb399ttKSkpSfn6+pMMzJcfHx2v79u16+eWXdf755ys9PV0bN27U9OnTNXz4cPXp08fqcAAAAABIykjqpYtPf0vODbdr26FTtLNiUrhDAqKO5QX2U089JUkaOXJkrf3z58/XNddco9jYWC1ZskSPPvqoKioqlJubqwkTJuiuu+6yOhQAAAAA/2W3xSg+Nl19c8+X15asnRXhjgiIPiEZIn40ubm5WrlypdWXBQAAAHAcOmaM0q4yjyR/uEMBog4LdAIAAAAtyPtf3qTPdz4d7jCAqESBHaUeePBB/d///V9Ir9GzZ0+9+dZbx5wlHgAiyVubXFq0pSyk19jn8uqpNcU6VEXvEIDIc3qXX+ukrAtrXq/4rkILNpQccyRqKHyVX62/fXJIXn/ta4/vlawxJyc1eTzAiQrpOtgIn+HDh+vggQMhvUZ6errGjRsX0msAgNW2FnqU5Azt78vl7oA27KsO6TUAoLFiHa3ksH2/TNeeUq+qvAENaBev3NQYxcc0XR9cYYVPX+bXzZcnZcSqc1psk8UBWIUe7GbO1OHn3uv9xTHEC843tKC9aZpq+t8/ASAI5lFy54k2fYw2WXkWQLh9uOU+fb13Qa19RZV+/WFVofaWepssDr4zIhpRYDdzC155RYMGDVJlZWWdY+ecc46+2LhRbdq0sfy6jz32mP7+/PP1Hps9a5Z+duGF9R4DgKZyem6C7jyrjWLtdUvarw+4dd+SAyr3BCy/7oIvSvXsZ4fqPXZBjyRNHZJu+TUBIBhn9/qD+na4NqwxmKapJz4u0r82135kp02iXbPPyVSH1JgwRQacGArsZq6kpETfbtmiQKDul8Tk5GT17NlT1153nU4bONCS6yUmJmrKlCk6c/hwdezYsd5z8vPzlZeXZ8n1AKCxEmNtatvKUe9gnmqfqX0unz7Mq9SOQx5Lruf2BbTyuwptLXSruLL+Z69T4uxq08rR4AggAAg1wzCUFN9O8bFp9R7/fF+1vqpnyHYoHKzwy+Wu/R3WYTeUleRQrIMyBc0Tz2BHAdM05XK55HQ6FRtb+1kVu92uOXPmKCkpSVu++Ubl5eWNvo7T6VT79u312OOPKyam7q+KpmmqrKxMHo81X1YB4EQZkuIdhnx+Uz+aP0emDk94Vu1rpaxWDjkdRqMLX6/fVHGVX69sKFGggfGOcQ5Ddr4vAogQdpuhOIehal/tpLV4a7kOlPvUNb3u88+xdkN224n9QOjxBWry8Y+fqImxSXH1jDoCmhPDDMd0gSfI5XIpJSUl3GFElFatWunRP/9Z115b/3Aft9utHTt26NTeveXz+Rp1jTvvvFN3zJypxMTEer+ElpWVqfspp6iwsFBeb9M9vwM0RmlpqZKTk8MdhuWO5Mdovb9gmaYpt9/Ugg2l+nhn3UdpJMlukzISHJp1Tmajvzj+65sy/eebMnl+XMX/l9Nh6LfntlUrp63WxEJAJGkJ+aMl3OPx8gVMlbsDuvu9Arl/lLtshhRTT6F7/aDW6pMdf0LX/evaYm38bw+5x1f7GezL+qZoWKcExdob/4MnEArB5A56sKNEeXm5/v7cc9q9e7fuueeeOknpSO/z3CefVCAQ0NatW/XIH/94zHb79++vG6ZMkSQNHDhQrVq1qve8lStW6O/PP09xDSCiGMbhHpqfdEpQWoJd726uuzyXPyAdqvLrpc9LZBhS21YOnXscS8PsOuTRyrwKSdLOQ94Gi+uTM2I1pGMCxTWAiOKwGXI6jHpnXgyYkttXN6ct21ahL/Z/P3z8lAynBnVIOK7rVXoCemuTS98Ve+ptW5JibIacDA1HM0eBHUU++OAD7du3TxMmTFCXzp2VkJhY63hiYqKuv/56SdJnn32mRYsWHbPN4SNGaMp/C+z6mKapbdu2acnSpXpu/vwTuwEACJGTM5xKjbNr/d4qFVb46xTDHr+pD3cc7uHumBqj3m3jjtnmt4UefZBXf6/4EW0S7eqR6dTQTolHPQ8AwsFmSNlJDrn9pnx+Uwcr6p8/4oivD7hrva72mmqfcnyTkbncAX2QV1HvYzSGpLZJDiXE8iMkmj+GiEepDz74QEOHDWvweDB/7UcbomOapnr16qVvNm8OKj4g3KJ1eCDDHxt2JO/9fmWhthc1zVwRs8/JVFYSk5qheWgJ+aMl3GMwfvh9cJ/Lp9lLDoQljjiHod+dn6W4E5gLAwilYHIHYzCi1C9/+UvdddddDR43DOO4t4Zs2LBBZ555pnbu2BGCOwAAax3JaZP6p+pnPY89BPxEtE+J0W0jMpSeYOfLIoCIVes7X5hi6JcTp18PyzihiSaBSMIQ8Sj15ZdfKiExUQMGDNDYsWMVH39iE1L82OrVq7VkyRJ9/NFHlrYLAKHWPiVGHp+pXSVefZlfLZ/FS2F3SYtVj0ynumU4rW0YAEIoLsbQgHZx2nLQowqPxYmxAT0znTo1K05d6pmxHGiu6MGOYmvXrNHll12mgwcOyOPxyOv1BjU0/Mf8fr88Ho88Ho9+99BDuveeeyyMFgCaTpf0WN0wOE3JTrvsxuHnEE+EIcluHN7OO6WVftaLoacAmpe0BIduPCNd7ZIdNfnsx1tjU+UPc+QPt8v6pejMzsxRgehCD3aU8/l8GjRokGx2u7KzsrRm7dp617A+Hn965BE98qc/SZIOFRdbGSYANDmbId15VhsFTMlV7deDyw82uIb1sZxzUiuNPunwKguJsfx2DaD5uvGMtAZH9vx93SFtKnDXf/AofpgjfyjJSb5E9KHAbgEOHDg8YUVFebnumz1bNlvjktmKlSuVv3+/laEBQNgYhqHkOLukw+tUX9AjSY0dFHlKhlOp8XbrggOAMGnlbDiX/aRjQqOGc5Mj0ZJQYLcgZWVleuCBB8IdBgBEnPgYm37ag2HdAHA0p+ce35rXQEvGuAwAAAAAACxAgQ0AAAAAgAUosAEAAAAAsAAFNgAAAAAAFqDABgAAAADAAhTYAAAAAABYgAIbAAAAAAALWF5gz5o1S4Zh1Nq6d+9ec7y6ulpTp05Venq6WrVqpQkTJqigoMDqMAAAAAAAaFIh6cHu1auX9u/fX7N9+OGHNcemT5+uf/7zn3rttde0cuVK7du3TxdddFEowgAAAAAAoMk4QtKow6GsrKw6+0tLS/XMM8/o5Zdf1llnnSVJmj9/vnr06KE1a9bojDPOCEU4AAAAAACEXEh6sLdu3aqcnBx16dJFkyZN0q5duyRJ69atk9fr1ejRo2vO7d69uzp06KDVq1c32J7b7ZbL5aq1AQDIjwBwNORIAE3N8gJ78ODBeu6557Ro0SI99dRTysvL05lnnqmysjLl5+crNjZWqamptd7Ttm1b5efnN9jmnDlzlJKSUrPl5uZaHTYANEvkRwBoGDkSQFMzTNM0Q3mBkpISdezYUY888oji4+P185//XG63u9Y5gwYN0qhRo/S73/2u3jbcbnet97hcLhIkgBNSWlqq5OTkcIdxwhrKj9FyfwCajsvlUkpKSlTlD3IkACsEkx9D8gz2D6Wmpurkk0/Wtm3bdM4558jj8aikpKRWL3ZBQUG9z2wf4XQ65XQ6Qx0qADQ75EcAaBg5EpHENE0p4Dv6SYZNhs3eNAEhJEJeYJeXl2v79u268sorddpppykmJkZLly7VhAkTJElbtmzRrl27NGTIkFCHAgAAAADhEfBr7/yfyV9R2OApSX0uVtqo25owKFjN8gL71ltv1QUXXKCOHTtq3759uvfee2W32zVx4kSlpKTouuuu04wZM5SWlqbk5GTddNNNGjJkCDOII6LExsbq1ltvVWyIfvUO+P165JFHVF5eHpL2ASBUAh6P8l/5hwJeb0jaN2w2ZV1+iewJCSFpHwDCwXNwq8o3vS1v8U6Z3soGz6vM+1BGbKJSB18vwxHbhBHCKpYX2Hv27NHEiRNVVFSkNm3aaNiwYVqzZo3atGkjSfrTn/4km82mCRMmyO12a8yYMXryySetDgM4IU6nU7ffcYeSkpJC0r7H49HTf/0rBTaAZsf0erXvhZcVqKoKSfuG3a42F/4PBTaAZs/0++SvLJRMqXrPZyr5aO4x3+PZ/6W8hduV2H2s7AlxMhx2GY5MGYbRBBHDCpYX2AsWLDjq8bi4OM2dO1dz5x77PzAAAAAAaI68xXna87fzD78wA8f9PtNbqT1Pn6tWvYuVeFIHJZzyeYgiRCiEZB1sAAAAAGjZ/jupWcAXVIEtSQr4VbUjXiWry1Xw+hR5CreFJkRYLuSTnAEAAABAS+ItzpN7/5cn1IbfFSu/yy9pseI7D5NMU7FtTrImQIQMPdgAAAAAYKGSNX/VwX/eall7Re/dq0MrH5FpmoeX+0LEosAGAAAAgAhXmfeB9vx1rAKVxeEOBUdBgQ0AAAAAFjD9Xrk+XyDvwa2Wt+0rcavs8wMKeIN8nhtNimewAQAAAMACps+toiX3y/RUWN62ryxGlVtay/TTRxrJKLABAAAAIMLFtqlS7KjdssX6wx0KjoKfPwAAAAAgwhk2ybD5VLxsjiq2Lgt3OGgABTYAAAAANAOmaapkzbuq3vV1uENBAyiwAQAAAKA5MCXX2ixV70wKdyRoAAU2AAAAAFjA9BtyfdJW7oL40FzAkJJOOyBn+/LQtI8TRoENAAAAABYwbA616n2WYtPbh+wapsfOTOIRjL8ZAAAAALCALc6prvf+RimDB0s2u/UXMKWyL9Pl2Z9gfduwBAU2AAAAAFgobeT/KfuKF8MdBsKAAhsAAAAALGSLS5Y9Ic3ydg2bTdlXXK6k/v0sbxvWcIQ7AAAAAACIOjaH7Elt5Ssplen3yBYbOKHmTL8h0x+jrMsukSMp06IgYTV6sAEAAADAYjFpndVh6gdy7z9b5ZvST7g9z8F4HVrZXgF3CJ7thmXowQYAAAAACz216d/aXLJbf/7JDWo/5XqVreupg/9+Tq1OLZIt5vh6sqt2Jsmwd1THX0+TJAWqHfK54uRIYg3sSEaBDQAAAAAWqvBVq9RTIUlK7tdXMk251m+Wz7VKhuGuOc/eyisZpvxlsZIkR4pHsW08sieeIdPXWoa9ixJOPleGYYTlPhA8CmwAAAAAsNCtfS+q9Tq5fz+1+vOf9MVFl8tbVFSzP+m0A7LFBFS6JkuS1Hr4PqUOLVarXvNl2Ompbo4osAEAAAAgxAy7XT3mPirT76/ZZ4vzSYYUqDpcltkTfHIk+yQb61w3VxTYQD08Ho/+9te/yhkXF5L2/X6/KisqQtI2AISS4YhRmwt/KtPjDU37Nptscc6QtA0A4WQYhuJy24c7DIQYBTZQD7fbrVtuuSXcYQBAxLE5Y9Xx5qnhDgMAgIjEMl0AAAAAAFjA8gK7U6dOMgyjzjZ16uFfu0eOHFnn2I033mh1GAAAAAAANCnLh4h/+umn8v/gwf2vvvpK55xzji655JKafddff73uu+++mtcJCTzEDwAAAABo3iwvsNu0aVPr9UMPPaSuXbtqxIgRNfsSEhKUlZVl9aUBAAAAAAibkD6D7fF49OKLL+raa6+ttTj6Sy+9pIyMDPXu3VszZ85UZWXlUdtxu91yuVy1NgAA+REAjoYcCaCphbTAXrhwoUpKSnTNNdfU7Lviiiv04osvavny5Zo5c6ZeeOEFTZ48+ajtzJkzRykpKTVbbm5uKMMGgGaD/AgADSNHAmhqhmmaZqgaHzNmjGJjY/XPf/6zwXOWLVums88+W9u2bVPXrl3rPcftdsvtdte8drlcJEgAJ6S0tFTJycnhDuOENZQfo+X+ADQdl8ullJSUqMof5EgAVggmP4ZsHeydO3dqyZIlevPNN4963uDBgyXpqAW20+mU0+m0PEYAaO7IjwDQMHIkgKYWsgJ7/vz5yszM1E9/+tOjnrdhwwZJUnZ2dqhCiSqXX365uvfoIZmmnnjiCRUWFoY7JACICHkH3ldJ1U4ZkrrnXKy4mNbhDgkAALQwISmwA4GA5s+fr6uvvloOx/eX2L59u15++WWdf/75Sk9P18aNGzV9+nQNHz5cffr0CUUoUefSSy/VuPHjZZqmFixYQIENAP+VV7hUu4tWSZI6tTmHAhsAJAVMv6o9xYp1JMthpzcfCLWQTHK2ZMkS7dq1S9dee22t/bGxsVqyZInOPfdcde/eXbfccosmTJhw1Ge0AQAAADROtadYb352ifaVrA13KECLEJIe7HPPPVf1zZ2Wm5urlStXhuKSAAAAAH4g78D72nbg3/IHPPpy999VWPa1BnS6MdxhAVEtZM9gAwAAAAifkqqd2nfocM91YdnXshuxMk1TReXfKNbRSsnxrMoDWC2k62ADAAAACD/TlI6ML/1wy2x9vXdBWOMBohU92AAAAECU272xjb4prtK6RTOV1e+gslLDHREQnSiwAQAAgChRVrVXew+tkSQVlW2u2R+f7JGnukwF+wOy7YzX7pj9Smv1trplni+bLSZc4QJRhwIbAAAAiBKHKrdr/Y4n5fVX6ftB4VKbzqWKS/KoZH8r7d+SLp9nu7xxX6hzxmgKbMBCPIMNAAAARIn2rX+iCae/pbiY1uEOBWiR6MGOEFdMmqQzzzzzmOf17dev5s+zZs3SoZKSo57v9/t158yZcrlcJxghAITHdwcWqaD0i2Oed6h8a82fv9j5N8U6ko56vmHYNKDTLxTraHXCMQJApLDZHIpRggzDqHPM2cqjDn0LtO+bdJUVxivvC7vWtP6zHA674mMz1LfDtfW+D8Dxo8COEGeeeaamTJly3OcbhqFLL7vsmOd5PB7df//9FNgAmq2C0i/0bf7CoN6zo3DpMc8xDLv65P6cAhtAVEqO76CA6ZfbW1KzLzbOrzadSnVge6q81Q6VFQW0Nf9dtUpIV0arHuELFogiDBEHAAAAoohh2DXm1Lk6JfuiBs9Ja1+m7sN3y2Y31bfDtRrV83f0XgMWoAc7Qvzh4Yf14gsvHPO8Bx58UMOHD5dpmrpy8mTt3LnzqOcHTFNFhYVWhQkATa5X+0nqknneMc9bv2OeDrg2SJLOPGWWEp1ZRz3fMAw5Y1KsCBEAIsqxCuXOA/NljwnIYY/TWT0fUuvEbhTXgEUosCPE9u3btX379mOeV1xUVPPn9evX65tvvgllWAAQdsnx7ZUc3/6Y5zljkmv+nNbqFKUmdAphVADQPBmGlJjqliTZjBi1SeqtGEdimKMCogdDxAEAAIAWx5DNcEii5xqwEj3YAAAAQAvTNXOsTuv8Szns8eEOBYgq9GADAAAAUSg75TT1bHeF6uuldtjjFB+bzrPXgMXowQYAAACiUFbqAKUkdNTOwmUyTX+tYyxRCIQGBTYAAAAQpeJi0nTRwNfqHqDnGggJCmwAAAAgShmGIcPgKz/QVHgGGwAAAAAAC1BgNzOmJNM0ZZpmuEMBgIhi1PMnAACApmSYzbBSc7lcSklJCXcYYdGuXTslJSVJkr777jt5PJ4wRwQ0T6WlpUpOTg53GJY7kh+j9f6OpsJ9QF5/pSQpKS5HdltsmCMCmpeWkD9awj0CsF4wuYMHMpqZvXv3hjsEAIhIic7McIcAAABaOIaIAwAAAABgAQpsAAAAAAAsQIENAAAAAIAFKLABAAAAALBA0AX2qlWrdMEFFygnJ0eGYWjhwoW1jpumqXvuuUfZ2dmKj4/X6NGjtXXr1lrnFBcXa9KkSUpOTlZqaqquu+46lZeXn9CNAAAAAAAQTkEX2BUVFerbt6/mzp1b7/Hf//73euyxxzRv3jytXbtWiYmJGjNmjKqrq2vOmTRpkjZt2qTFixfr3Xff1apVq3TDDTc0/i4AAAAAAAizoJfpGjt2rMaOHVvvMdM09eijj+quu+7Sz372M0nS888/r7Zt22rhwoW6/PLLtXnzZi1atEiffvqpBg4cKEl6/PHHdf755+sPf/iDcnJyTuB2AAAAAAAID0ufwc7Ly1N+fr5Gjx5dsy8lJUWDBw/W6tWrJUmrV69WampqTXEtSaNHj5bNZtPatWvrbdftdsvlctXaAADkRwA4GnIkgKZmaYGdn58vSWrbtm2t/W3btq05lp+fr8zMzFrHHQ6H0tLSas75sTlz5iglJaVmy83NtTJsAGi2yI8A0DByJICm1ixmEZ85c6ZKS0trtt27d4c7JACICORHAGgYORJAUwv6GeyjycrKkiQVFBQoOzu7Zn9BQYH69etXc86BAwdqvc/n86m4uLjm/T/mdDrldDqtDBUAogL5EQAaRo4E0NQs7cHu3LmzsrKytHTp0pp9LpdLa9eu1ZAhQyRJQ4YMUUlJidatW1dzzrJlyxQIBDR48GArwwEAAAAAoMkE3YNdXl6ubdu21bzOy8vThg0blJaWpg4dOujXv/617r//fp100knq3Lmz7r77buXk5GjcuHGSpB49eui8887T9ddfr3nz5snr9WratGm6/PLLmUEcAAAAANBsBV1gf/bZZxo1alTN6xkzZkiSrr76aj333HO67bbbVFFRoRtuuEElJSUaNmyYFi1apLi4uJr3vPTSS5o2bZrOPvts2Ww2TZgwQY899pgFtwMAAAAAQHgYpmma4Q4iWC6XSykpKeEOA0AzVlpaquTk5HCHYbkj+TFa7w9A6LSE/NES7hGA9YLJHc1iFnEAAAAAACIdBTYAAAAAABagwAYAAAAAwAIU2AAAAAAAWIACGwAAAAAAC1BgAwAAAABgAQpsAAAAAAAsQIENAAAAAIAFKLABAAAAALAABTYAAAAAABagwAYAAAAAwAIU2AAAAAAAWIACGwAAAAAAC1BgAwAAAABgAQpsAAAAAAAsQIENAAAAAIAFKLABAAAAALAABTYAAAAAABagwAYAAAAAwAIU2AAAAAAAWIACGwAAAAAAC1BgAwAAAABgAQpsAAAAAAAsQIENAAAAAIAFgi6wV61apQsuuEA5OTkyDEMLFy6sOeb1enX77bfr1FNPVWJionJycnTVVVdp3759tdro1KmTDMOotT300EMnfDMAAAAAAIRL0AV2RUWF+vbtq7lz59Y5VllZqfXr1+vuu+/W+vXr9eabb2rLli268MIL65x73333af/+/TXbTTfd1Lg7AAAAAAAgAjiCfcPYsWM1duzYeo+lpKRo8eLFtfY98cQTGjRokHbt2qUOHTrU7E9KSlJWVlawlwcAAAAAICKF/Bns0tJSGYah1NTUWvsfeughpaenq3///nr44Yfl8/kabMPtdsvlctXaAADkRwA4GnIkgKYW0gK7urpat99+uyZOnKjk5OSa/TfffLMWLFig5cuXa8qUKXrwwQd12223NdjOnDlzlJKSUrPl5uaGMmwAaDbIjwDQMHIkgKZmmKZpNvrNhqG33npL48aNq3PM6/VqwoQJ2rNnj1asWFGrwP6xZ599VlOmTFF5ebmcTmed4263W263u+a1y+UiQQI4IaWlpUfNS81FQ/kxWu4PQNNxuVxKSUmJqvxBjgRghWDyY9DPYB8Pr9erSy+9VDt37tSyZcuOGcTgwYPl8/m0Y8cOnXLKKXWOO53OegtvAGjpyI8A0DByJICmZnmBfaS43rp1q5YvX6709PRjvmfDhg2y2WzKzMy0OhwAAAAAAJpE0AV2eXm5tm3bVvM6Ly9PGzZsUFpamrKzs3XxxRdr/fr1evfdd+X3+5Wfny9JSktLU2xsrFavXq21a9dq1KhRSkpK0urVqzV9+nRNnjxZrVu3tu7OAAAAAABoQkEX2J999plGjRpV83rGjBmSpKuvvlqzZs3SO++8I0nq169frfctX75cI0eOlNPp1IIFCzRr1iy53W517txZ06dPr2kHAAAAAIDmKOgCe+TIkTravGjHmjNtwIABWrNmTbCXBQAAAAAgooV8HWwAAAAAAFoCCuzjFB8fr4SEhHCHAQARp9LnVoW3OtxhAAAAhB0F9nH629/+ptffeCPcYQBAxPnfFX/WhPcfDHcYAAAAYUeBfZzi4+PVt29fPTVvnrKyssIdDgBEjCq/R18U52nKqie0v7I43OEAAACEDQV2ELKzs3XDDTdo4MCBys7ODnc4ABAx8isP6enNi/TZga3aV0GRDQAAWiYK7CAZhqG333lHv7nrrnCHAgAR58L3fqv71y8IdxgAAABhQYHdCIZhaPz48Vr03nuKj48PdzgAEFHeylutc/91tyq97nCHAgAA0KSCXgcbh2VnZyslJUUXXXSRPv74Y+Xl5YU7JACICPlVh1S6v0Jv5n2sn2T1UJdk5q0A0LIdrCrVyv1f1byOsdl1fofTFWOzhzEqAKFAD/YJSEhI0AsvvqjzzjtPDge/VQDAEVV+j65c/kct2r1O3oBPpmmGOyQACAtfwK+NxTt0yeI5NduVy/4ol6dSHr+vZguQJ4GoQFVogQfnzNEll16qs0aNCncoABBRZn7yd/1j+4dafgHLeAFomW5Y9bje3rG21r5yb5V6/eMXshlGzb6FY+7WoMyTmzo8ABajB9sCKSkp6tmzp2bNnq3cDh3CHQ4ARAyXp1JfH9qpez97STvLDoQ7HABocofc5Sp2l9XaZ0oqqCrR/spDNdtTX/9bC7atCk+QACxDgW2RzMxM3XPPPRrQv7/S0tLCHQ4ARIyD1S79dv0CfV64XUXVZcd+AwC0QM9tWaIXti7T3ooi+QOBcIcDoJEosC322uuv68knnwx3GAAQcS5ePEe//GBuuMMAgIj1n13rdNIr1+tAdUm4QwHQSBTYFnM4HBo6bJj+8dprat26dbjDAYCI4TcD+jD/a138/pw6wyUBAJIpU9V+r65d8We9uv2DcIcDoBEosEOgXbt2GjdunEaMGKEOPJMNADX2VRZr4Y7VWrnvS57JBoB6mDK1aPc6vbd7ndYUfMPs4kAzQ4EdIg6HQ2++9ZauvuaacIcCABHFbwZ00fsP6rktS2SaJkt4AUA95m9Zognvz5Hf9Ic7FABBoMAOsWnTpmnZ8uWy2+3hDgUAIsoTm97VqH/OlN9kMh8AABAdKLBDrE2bNurXr59u/MUv1Llz53CHAwARo7DapQ1FeXrq63/rO1d+uMMBgIhT4avWvK//Q44EmhFHuANoCVJTU/X444+ruKhIBQUFqqysDHdIABARSj0VuvmjvyjdmaS28alKjIkLd0gAEDHIkUDzQw92E/rbM8/o9TfeCHcYABBxrlv5mCa8/2C4wwCAiESOBJoPCuwmFB8fr759++qpefOUlZUV7nAAIGJU+z36ojhPU1Y9of2VxeEOBwAiCjkSaD4osJtYdna2brjhBg0cOFDZ2dnhDgcAIkZ+5SE9vXmRPjuwVfsq+AIJAD9EjgSaBwrsMDAMQ2+/845+c9dd4Q4FACLOhe/9VvevXxDuMAAgIpEjgchGgR0mhmFo/PjxWvTee4qPjw93OAAQUd7KW61z/3W3Kr3ucIcCAABw3IIusFetWqULLrhAOTk5MgxDCxcurHX8mmuukWEYtbbzzjuv1jnFxcWaNGmSkpOTlZqaquuuu07l5eUndCPNUXZ2toYNG6aLLrqIJbwA4Afyqw7pw/2b9GbexyxPA6BZG9K2u4Zn9w53GACaSNAFdkVFhfr27au5c+c2eM55552n/fv312yvvPJKreOTJk3Spk2btHjxYr377rtatWqVbrjhhuCjjwIJCQl64cUXdd5558nhYNU0ADiiyu/Rlcv/qEW718kb8Mk0zXCHBABBu63fxbr/9CsVY7PXuzkMe1DtOQy77AaDUIFIFXRFN3bsWI0dO/ao5zidzgZnyd68ebMWLVqkTz/9VAMHDpQkPf744zr//PP1hz/8QTk5OcGGFBUenDNHl1x6qc4aNSrcoQBARJn5yd/1j+0favkFLFEDoHkalHmydlwxv95j35bu0ah/3nncbS294AH1TWfkIxCpQtJlumLFCmVmZqp169Y666yzdP/99ys9PV2StHr1aqWmptYU15I0evRo2Ww2rV27VuPHj6/Tntvtltv9/XN4LpcrFGEf1YIFC/T555+H9BoHDx4MafsAok8k5MfLu41Q/4yuIb1Gm7j/b+/Ow6Oqz/6Pf87MJJM9IYSQBMK+IyCiBAoqFBDweVQQrSAqtlbRiv4ELZa2Lrihta1WRH1s3TdcQautyg5aQBYRF0SCQdYkkJBM1sks5/cHOpqSAEPOZDLJ+3Vd57oyZ7nnPifhZu75niU5pPEBNE9NoUZKktMepaz41DqXOWx2zT196gnH6p2SreToeKtSA2AxyxvscePGBa4p3rlzp37/+99r/PjxWrt2rex2u/Lz85Wenl47CYdDqampys+v+zq7efPmae7cuVanGpTXXnstrO8PAHVpCvXxkq5nhvX9AaA+TaFGHk96bLJuHzQl3GkAsIjlF3BMnjxZ559/vvr166cJEybo3Xff1YYNG7Ry5cqTjjlnzhyVlpYGpj179liXMABEMOojANSPGgmgsYX8rlpdunRRWlqacnNzNWrUKGVkZKiwsLDWOl6vV8XFxfVet+10OuV0OkOdKgBEHOojANSPGgmgsYW8wd67d6+KioqUmZkpSRo6dKhKSkq0adMmDRo0SJK0fPly+f1+5eTkhDodAAAAAGgyTF+pfFWfyVsaLdNnyDAkR4pbtpjuskVlhjs9BCnoBru8vFy5ubmB13l5edqyZYtSU1OVmpqquXPnatKkScrIyNDOnTs1e/ZsdevWTWPHjpUk9e7dW+PGjdPVV1+tJ554Qh6PRzNmzNDkyZNb7B3EAQBoroJ5vJphGCHMBACajp/WRl/lFlXmjtThVVnyVUTJsPuVOmqvYjs/pqjWtR9lTJ1s+oJusDdu3KiRP3mU1KxZsyRJ06ZN0+OPP66tW7fqueeeU0lJibKysnTOOefo7rvvrnV6zksvvaQZM2Zo1KhRstlsmjRpkh555BELdgcIj0ceeUSjRo8+7nqXTZ0a8rvRA0BT4t53o3xly467XkzHF2WPO60RMgKA8DO9haraOUZlXySpptAvsyZLvsojrZnpM1TycaZKP/m7DPvrgW1iu5yptHPuCFfKOEFBN9gjRow45rfRH3zwwXFjpKam6uWXXw72rYEm5Ywzzgg8bu7Ms85S7969j7vNpVOnasiQIfL7/XrhhRdUWVkZ6jQBoNH5Kj6Rr2rjkZ/L18jv/vq423gOvyxf5TpJNkWlXi7DxmOIADQvpt+vQ+9/KJvTJUdigdwH9si9P06+smhJUT9Z05CvIkpS0ffT93MdTrk2v6SEUybKFh3XyNnjRIX8GmygOUpISNCFkybp1ltvDWq7m2++WZLk8Xi0fPly7d69u9bzOQEgkpmmKfnL5S19SzWFfwpqW8/Bv37/k0OOhJ9L0R1k2GKsTxIAwsT0+7XvH0/KkbJTcV1dklKC2r6m4Csd+uBOxXUfQ4PdhFn+mC6guXM4HNq0ebNuuummsMYAgKbHq4rtg1Rz8OGGxfimoTEAoOkxDFMpw/YrtrMr3KkghBjBBoJkGIYS4uMb9NgPwzCUkJCg6OhoCzMDgHA7MoIts6ZhYfwVDY8BAE1IxfZvVPj22zI9ZTIaMsTp96l4+Tx5DreWv7KN2k//tQy73bI80XCMYANBSExKUp8+feSIijr+yicgPT1dPXr0sCQWAIST6XPJX/2VZHqtiectlL96e1B3IQeApqqmcL+Kly+W313dwEimyr9YLNfGV+Ta8o5EjWxyaLCBIIwZPVqbP/1UaWlplsT7zfXXa+myZXI4OJkEQGTzli1R5fbTZPoOWRLPc+gxVe4cLcmahh0AwikqrUopw/fLiPZbEq/VWfvV8cbPJDsNdlPDp3ogGIZh6fMHDYvjAUDzwgdHAM2HlR/5qvckyFfVRvF9JPFRsklhBBsAAAAAIoi/2iHPIb8qvv5A3rKCcKeDn6DBBgAAAIAI468uVeGiG+TevyXcqeAnaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAABCyFvilGtzG/k9tF/NHb9hIAiHDh7UmjVr5Ha7LYm3a9curVu3TqbJs14BRDbDkSZ7/HDJiLYmXlRH2eOGiAe8AmgO/NV21RTEST4La5o9SjHZZ8gW28q6mGgwGmwgCKtXr9boUaNUXFRkSbxnnn5aF190kXw+nyXxACBcHAlnK7bbUhn2VEviRbX+pWI7vyHDcFgSDwDCypDl3xfa41KVOfUlxXYYbG1gNAgNNgAAAACEUPIZZ+iU556So1VyuFNBiNFgA0Hy+Xx67rnntHHDhobFePZZbdy40cLMACDc7IpKnSZb7KAGxLDJ0eoK2eNOtywrAAg3e3ycYjt1UtLAS2SLPV3Ve+PVkCsEnZn9ldhvkmTQzjU1nHcFBMnv9+v3v/+9ysrK1L1HD0lSXFycoqKijrttRUWFvF6vPB6Pfve736mgoCDU6QJAozEMu5xZ86SCRNW4dxyZ6a+U5D3+xrY4SQ7JiJIz637ZojJCmSoANDrDZlfqyNmq3v2iipe75Mz65qROGzei4xXfa7xSfnat9UmiwfjKAzhJf/nLX5Tdvr2y27fXB++/f0LbTJw4Udnt26tL584qLCwMcYYAEB7RbW5WQt89Sui7R/aksSe0TWynRUe26fOtDEd6iDMEgPDJmHKJ+j79hAyHPfiNbQ61v+qfSh78K+sTgyUYwQZOUk1NjWpqaiRJjz76qN59993jbvP51q0qLy8PdWoAEFaGzSnJKUmKTpshf9L/HncbW2w/GfbEEGcGAOFni45SVEpbpY27WzJNVe/Zq/xXXlNc9xLZYn688W1C3wvkzGwvd/5dik6/RTZnN8mwyZ6YIcNhzRMbYD0abMACH374YbhTAIAmyXGCI9gA0JLYouOUNHCKJMket12GY5uiUgtki/vxkpq4HmMU17WnHHnPKabD/8oed1q40kUQaLABAAAAIEzievbQKS88fYzlWxsxGzQUDTYAAAAAhIlhWPyAbIQVNzkDAAAAAMACNNgAAAAAAFgg6AZ79erVOu+885SVlSXDMLR48eJayw3DqHN68MEHA+t06tTpqOX3339/g3cGAAAAAIBwCbrBrqio0IABA7RgwYI6lx84cKDW9PTTT8swDE2aNKnWenfddVet9W644YaT2wMAAAAAAJqAoG9yNn78eI0fP77e5RkZGbVev/322xo5cqS6dOlSa35iYuJR6wIAAAAAEKlCeg12QUGB3nvvPV111VVHLbv//vvVunVrDRw4UA8++KC8Xm8dEY5wu91yuVy1JgAA9REAjoUaCaCxhbTBfu6555SYmKgLL7yw1vwbb7xRCxcu1IoVKzR9+nTdd999mj17dr1x5s2bp+Tk5MCUnZ0dyrQBIGJQHwGgftRIAI3NME3TPOmNDUOLFi3ShAkT6lzeq1cvjRkzRvPnzz9mnKefflrTp09XeXm5nE7nUcvdbrfcbnfgtcvlokACaJDS0lIlJSWFO40Gq68+Npf9A9B4XC6XkpOTm1X9oEYCsEIw9THoa7BP1Jo1a7R9+3a9+uqrx103JydHXq9Xu3btUs+ePY9a7nQ662y8AaCloz4CQP2okQAaW8hOEX/qqac0aNAgDRgw4LjrbtmyRTabTenp6aFKBwAAAACAkAp6BLu8vFy5ubmB13l5edqyZYtSU1PVoUMHSUeG0F9//XX95S9/OWr7tWvXav369Ro5cqQSExO1du1azZw5U5dddplatWrVgF0BAAAAACB8gm6wN27cqJEjRwZez5o1S5I0bdo0Pfvss5KkhQsXyjRNTZky5ajtnU6nFi5cqDvvvFNut1udO3fWzJkzA3EAAAAAAIhEDbrJWbj8cJE5AJys5nqDm+Z4kyIAjaMl1I+WsI8ArBdM7QjpY7oAAAAAAGgpaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAsQIMNAAAAAIAFaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgAUe4EzgZpmmGOwUAEa651pEf9svlcoU5EwCR5oe60Vzro0SNBHBygqmPEdlgl5WVhTsFABGurKxMycnJ4U7Dcj/Ux+zs7DBnAiBSNdf6KElFRUWSqJEATs6J1EfDjMCvKf1+v7Zv364+ffpoz549SkpKCndKTYrL5VJ2djbH5r9wXOrXko6NaZoqKytTVlaWbLbmd5UM9fHYWtLferA4NnVrSceluddHSSopKVGrVq20e/fuZvslwslqSX/rweLY1K+lHJtg6mNEjmDbbDa1a9dOkpSUlNSsf5kNwbGpG8elfi3l2DTnD1XUxxPDsakfx6ZuLeW4NOf6KCnwwTg5OblF/D5PRkv5Wz8ZHJv6tYRjc6L1sXl+PQkAAAAAQCOjwQYAAAAAwAIR22A7nU7dcccdcjqd4U6lyeHY1I3jUj+OTfPC77N+HJv6cWzqxnFpXvh91o9jUz+OTf04NkeLyJucAQAAAADQ1ETsCDYAAAAAAE0JDTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAsQIMNAAAAAIAFaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAsQIMNAAAAAIAFaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAsQIMNAAAAAIAFaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAsQIMNAAAAAIAFaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAsQIMNAAAAAIAFaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWCCsDfaCBQvUqVMnxcTEKCcnR5988kk40wEAAAAA4KSFrcF+9dVXNWvWLN1xxx3avHmzBgwYoLFjx6qwsDBcKQEAAAAAcNIM0zTNcLxxTk6OzjjjDD366KOSJL/fr+zsbN1www363e9+F46UAAAAAAA4aY5wvGlNTY02bdqkOXPmBObZbDaNHj1aa9euPWp9t9stt9sdeO33+1VcXKzWrVvLMIxGyRlA82CapsrKypSVlSWbLfJvQ0F9BGCV5lYfJWokAGsEUx/D0mAfOnRIPp9Pbdu2rTW/bdu2+vrrr49af968eZo7d25jpQegBdizZ4/at28f7jQajPoIwGrNpT5K1EgA1jqR+hiWU8T379+vdu3a6T//+Y+GDh0amD979mytWrVK69evr7X+f3/7WFpaqg4dOjRavgCan5KSEiUnJ4c7jQarrz7u2bNHSUlJYcwMQKRxuVzKzs5uNvVRokYCsEYw9TEsI9hpaWmy2+0qKCioNb+goEAZGRlHre90OuV0OhsrPQAtQHM5NbC++piUlMSHRwAnpbnUR4kaCcBaJ1Ifw3KBTXR0tAYNGqRly5YF5vn9fi1btqzWiDYAAAAAAJEiLCPYkjRr1ixNmzZNp59+ugYPHqyHH35YFRUV+uUvfxmulAAAAAAAOGlha7AvueQSHTx4ULfffrvy8/N16qmn6v333z/qxmcAAAAAAESCsDXYkjRjxgzNmDEjnCkAAAAAAGCJ5vGQQwAAAAAAwowGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAsQIMNAAAAAIAFaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAsQIMNAAAAAIAFaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAs4Ah3AgAi15AhQzR79mxJ0sKFC/Xaa6+FOSMAaBoKXZ/ri70vSpI6txmjzm1GhzkjAEBjoMEGcNKy2rXThIkTJUmffvppmLMBgKajquaQ9hStliS1ju8R5mwAAI2FU8QBAAAAALAADTYAAAAAABagwQYAAAAAwAKWN9jz5s3TGWecocTERKWnp2vChAnavn17rXVGjBghwzBqTddee63VqQAAgEbi9fhUXuYO2VRVWRPuXQQAy/j9pirqqXcV5W6ZphnuFHGSLL/J2apVq3T99dfrjDPOkNfr1e9//3udc845+uqrrxQfHx9Y7+qrr9Zdd90VeB0XF2d1KgAAoJH8Z2Wu5t78Tsji9xvUXg8/MyVk8QGgMRUfLNel456Uz+c/allcfLReXXqd4uKjw5AZGsryBvv999+v9frZZ59Venq6Nm3apLPOOiswPy4uThkZGVa/PRBydrtd9953n1avXq3/fPyx7r33Xj355JOqqKzUzTffrLl33qkePXtq4sSJ+t2tt2rSRRepW9euuuuuu3TLLbfI5XLpqaeeqjPGZ599Fu7dA4CT4vP6VVkRulFmd5UnZLEBoLH5TVOVlTXyeY9usGvcXj101wdyOOxKa5ugX91wpgzDCEOWOBkhf0xXaWmpJCk1NbXW/JdeekkvvviiMjIydN555+m2226rdxTb7XbL7XYHXrtcrtAlDByHYRgaPHiwduXlaWN0tH42bJjefPNNORwODRs2TLGxscrMyNCQIUNkt9vVrWtXDTztNElS//79dejQoXpjAMGiPgJA/aiRaEpM09TuvGL5vH4VF1VI9ZwG7vX69e4bWyVJ7Tq00shxvSVJ8QlOtc1KarR8Q2X/nhJV/+RL01at49SqdfwxtogsIW2w/X6/brrpJg0bNkynnHJKYP6ll16qjh07KisrS1u3btWtt96q7du366233qozzrx58zR37txQpgqcMK/Xq5+PHBl4PfDUUwM/9+/XT5KUl5enV199VZJqXQpxxRVXBH6uLwYQDOojANSPGommxOf16/qpL6qosPyEt9m3+7Cmjn9SkjRyXC/dt2BSqNJrNHNvfltbN+0NvP71TWfpqhvODGNG1grpXcSvv/56ffHFF1q4cGGt+ddcc43Gjh2rfv36aerUqXr++ee1aNEi7dy5s844c+bMUWlpaWDas2dPKNMGjslut2vR4sW68sorlZaWphUrV+rMM8/UgAEDtHrNGnXs2FETJk7U+x98oNjYWN1yyy16/vnnJUkLHntMd999d70xgGBRHwGgftRINDkNuHfZ5nXfafovntP0Xzynd9+IrMsKX3xybSD33K8Lay1bu36V5j85V//aMl0FpZ+GKUPrhGwEe8aMGXr33Xe1evVqtW/f/pjr5uTkSJJyc3PVtWvXo5Y7nU45nc6Q5AmcjIMHD6q8vFx+v18F+fmqdrtVU1Oj/Px8eb1eVVZWqrCgQKZpyuVy6dChQ5Kk4qIilZSU1BsDCBb1EQDqR41Ec1JaUhUY+W2TkajU1vEacnZX2WxN8/rssqp9Kq74RpK0u+Ar7T5w5AsutydOUTF+xbeqVmlBvA4fLtbObw/Km1igVs71Mk1TGSmnhTP1BrG8wTZNUzfccIMWLVqklStXqnPnzsfdZsuWLZKkzMxMq9MBLOfz+fSb666T3++X3+/X5ZdfLq/XK0maeuml8ng8OnDggFYsXy6Px6OnnnoqcGOKuXPnyjTNY8YAAAAAjmXZe9v02YY9en35b2R32GQYksNhD3datew7vE7rd/5ZkuRJbquupydLhrR9TbYcMV51Of2AvljSWTIkn8eubzdmybC9oZKaLTonab5shiMib+5m+Sni119/vV588UW9/PLLSkxMVH5+vvLz81VVVSVJ2rlzp+6++25t2rRJu3bt0jvvvKMrrrhCZ511lvr37291OoDlHA6HPtmwQf/v//0/ZWRkaEdurs4ZO1aDBw9W3q5d6tatmy67/HJ9/sUXio+P17333acPlyyRJL3+xht6/Ikn6o0BAAAAnIjiQxW6aOQCTTzzUf3xhkXhTueY9n6Vpm/+U/usZsOQep+9W227Hq41/1DZl3pzw4Uqq96rSGT5CPbjjz8uSRoxYkSt+c8884yuvPJKRUdHa+nSpXr44YdVUVGh7OxsTZo0SX/84x+tTgUICb/frxeef14bN21SRUWF/v7kk9qVl6eqqir93xNPqKSkRF9++aWefvppeTwerV69Wrvy8iRJby9eHDgtvK4YAAAAwInw+00VHayQJH39xQH9/eFVuuiKM9Qqte4nM4VTYutKtWpXJklK61gqm8Mvw5CiYnySJJvDp7ZdSxSTWCO/6VVVzSF9tW+h2rX6mbJbDwtn6kELySnix5Kdna1Vq1ZZ/bZAo/H7/XrllVdUUVGhiooKPfPMMyouLpbf79dTTz2lkpISVW3bpv379snj8eg/H3+sjdHRkqR33303cFp4XTEAAACAYBXsd+np+R9pzP/2bTINtsMeq9jo1qqqKVbr7DKZpiGv266M7sVyOKLldKSoylMs0/QpKsanrJ5FMn5yfvX2A0eeMBVpDXZI7yIONEcOh0Pr1q/XjTfeqIyMDH2zY4fOOeccDR48WN/m5alr16667LLLfjxF/N579cGHH0o6cor4Y489Vm8MAAAAoDnomj5OF5z2khz2GLXrc0g9fnbklO+ugw9o/OQM3fGHP2nw/xxW/3Py1Pus3VLkXW5dp5A+Bxtojnw+n6ZPn65deXkqLi7WpVOmaMOGDaqpqdElv/iF9u3bpw8//FB79+5VdXW1nnzySb355puSpNtvu03Vbne9MRrTkCFDNHv27AbFyMrKCvw8ecoUDRw4sEHxFi5cqNdee61BMQCgoQpdn+uLvS82KEaV+1Dg57yDS1T0/Z10T1bnNmPUuc3oBsUAgMa0//B6fX3gTfl8NTIMyRnnVdfB+xWb5JZhSHabXYZNtUatmwMabOAklBw+rKqqKvn9fhUXF6umpkYer1fFxcXy+Xyqrq7W4cOHZZqmKior5XAc+afmcrlUU1NTb4zGlNWunSZMnGhZvN69e6t3794NivHpp5H/7EMAka+q5pD2FK22LF5p1XcqrfquQTFax/ewKBsAaBxl1fu1r3it0pL6qrxqv6pUpFZZFeFOK+Sa2fcFQOg5HA69+dZbmjZtmtLS0rR02TINHz5cA/r314qVK9WxY0ddMGGC3vvXvxQbG6ubZ83SM88+K0ma/+ijmnvXXfXGaFSmKdOC6cdw1sUCAPy3ZnLuJIAWxWGP0ei+Dym7dSN/zg0jRrCBIHk8Ho04+2yVlJSopKREA/r31759++Tz+XRK37769ttvlZ+fr+XLlqmyslJz585VbGysJOnyyy6T1+utN0ZjWrJkifr26dOgGOecc44e/tvfJEmPPvqoHn/ssQbFO3To0PFXAoAQy0oZrAsGvdKgGPsPr9eGbx+WJPXKvEg9syY1KF5MVEqDtgeAcPD4qvTell+puqbl3MyXBhsIks1m08+GDdOXX36pqm3bNGLECH344Yeqrq7WyJEjlZ+fr3bt2mnI0KHKy8tTj549lZmRoby8PA087TRVVlbqwIEDdcYoLy9vtP0oKyvT119/3aAYffr2Dfx86ODBBscDgKYgyhGvFEd8g2KUVv746MWYqBSlxHVqYFYAEIlMuap2HzW3vHqftucvktdXHYacQotTxIEg2Ww2/f73v9eon/9cSUlJuufee9WnTx917txZ982bp/T0dA0ZMkR33HGHnE6nJk6YoJtmzpQkXTt9uqZOnVpvDAAAACBYNruh+ASnDFtkXE5SXLFDn+z8qzy+5ndNNiPYQJC8Xq8GnXaaPB6PPB6POnfqpOrqapmmqez27VVZWam8vDy9+uqrKi8v1+9+9zvZ7XZJ0sSJE2WaZr0xAAAAgGD1HdBODz0zWbFx0eFOpcVjBBsIkt1u1223367RY8YoOTlZDzzwgPr27avOXbrowT//Wenp6Rr6s5/p7nvukdPp1KSLLtItt9wiSbr++ut1xRVX1BsDAAAACJb9+xFsWxMawc5IHqgzutwkmxFc058Y015Dus1WbFRqiDILLRpsIEiGYWjw4MHqkJ2t6Oho/WzYMLVu3VrJSUkaNmyYYmNjlZmRoSFDhshut6tb164aeNppkqT+/furV69e9cYAAABAM2VIHbqkKiU1zpJwdrtNnbqlqXP3NGW2T7EkppVS4ruoe8b5So3vqmhHYq1lUfY4Jcd1kmHYj9ou2pGg9KQBahXfTbHRaY2VrmU4RRwIktfr1c9Hjgy8HnjqqYGf+/frJ0mBU8Ql6a677gosv+KKKwI/1xcDAAAAzY/dbtOCly7TPx5eracf/ajB8VLT4vTCu7+W3dF0x0wdthide+pTWpf7gL7JfzswPzNlsM7qdZfe/GSiqjxFtbYpKv9a72yeqgsGvazk2E6NnHHDNd3fRoTp1LmzVq9Zo37fN1hovux2uxYtXqwrr7xSaWlpWrFypc4880wNGDBAq9esUceOHTVh4kS9/8EHio2N1S233KLnn39ekrTgscd099131xsDaI6+deVr+NuztbUo7/grAwDQTBmGIcMwLHms/XkXD9D9T1wsu8P2Y9wm6Ifc+ra/TMN73C5Jyun6W53WabpshkM/7/snjev/hEb2vl+GbBrY6VqN6/+ExvV/QgnOzCa7X8fCCLZF4uLiNGzYMJ13/vmKi4/X+nXrwp0SQujgwYMqLy+X3+9XQX6+qt1u1dTUKD8/X16vV5WVlSosKJBpmnK5XIHnOxcXFamkpKTeGEBzVOl16+P8r/TOd5+owuvW0La9wp0SAABh07lbGw0d0VXrVu2UaQa3rWFIQ8/uppyzuqpP/6zQJBgCSbHtZbdFqWPrkcpMOU3J3z+6MC3xyFN03J5SdUwbqcyU09UmMbLvS0SDbbF77rlHg047TZMnT5bH4wl3OggBn8+na66+OvB68uTJgZ9/cfHFkqR9+/bpww8+kCQ9+eSTgeW33XZb4Of6YgDN1W0bXtDmg7laOPpWRdnsEfmtNOpn2Aw5QniaYlM+BRIAgjH6f/towBnZuvjnj8nn9cs0JZ/PL7vDJtNvyu8/0nXbbMZRNy1zRNk1Z97/KC09IRypN0i8s61G9LmvzmXOqGSd3fueRs4oNGiwQ2Dc+PHakZurwWecocLCwnCnAwBNxr/3bFS3V36tDRc+rLZxKeFOBxYaenY3LVozI2Txo6KOvhEOAESq1LR4vbHiekmmvvh0n+b85k3Nf2GqPt+0R4//eaUk6brfjtS4Caf815aGUltbc5M0hAYNdgjExsaqXbt2mn3rrfrnO+9o1apV4U4JAJqEap9H+yqL9MCWN3R+pxyNyOK+Fc2FM8YhZ0zi8VcEUKd/bPtQCVExurjLcP116yKdkd6DGtmM2e22wCh0r1My9eubzlLnbmmKiXHI4/FLkk7/WSelpVNXIw0NdojY7XbNmjVLpt+v7du3Kz8/P9wpAUCT4DdNPfT5YtkMQz1T2ikjthWniwNo8Z7dvkTJzngNadtTf976li7v/nN1T86UJKU6kxTrCO5ZwogcGe2SddUNR252m5Iap94RdG01jsYFTSF208yZ+s/atXI4+C4DAH7qoa2LNXTRzfKavnCnAgBNwr93b1L3V65RYVWpHtq6WJ1eukqdXrpKH+7dHO7UAJwgGuwQs9vtatu2rV5/4w0NHjw43OkAQJPhl6mCqhJd9OE8rS/YHu50ACDsTJmBLx393//sNX2679PX9IdPng9zdgBOBMOqjSA2NlYXXHCBNnzyiaqrq7V169ZwpwQATUK1z6N3vluvwek9FOOI0oDWXcKdEgA0qgpPtTYdylWpp7Ledb7ZvV3x5W6tzj5Np7fprjiHsxEzBBAMRrAb0T333qtH5s8PdxqAdUxT5vcT0BB/3PCCbvjo//h7QjPFPQZQv7yyAp39zu/0RfF3tReYZmA6/YBXcVtzdfY7v9OusgLqJNCEMYLdyE4//XR98eWXOv+88/Ttt9+GOx2gQZYsWaK+ffpIkg4dOhTmbBDpNh7cob6v/Ub/HH+7uiZlhjsdoEGyUgbrgkGvSJJiolLCmwwiUp8ivwbl+3TuTq+KeleoMEmK/TpFzz18m4bnjNF5514W7hQB1IEGu5HFxcWpd+/emjZtmpYuXao1a9aEOyXgpJWVlenrr78OdxpoJqp8NdpWskfPbV+m0e1P1VmZ//3sTyByRDnileKID3caiESmqe6H/RpcXalhtmp1dsUq1edWut2rDIcpVR5W+bfxWrY6WTtSbToz6xT1Te0Y7qyDZpqmNuytUkqMXT3acMo7mg8a7DAwDEO33X67MrOytGXLFpWVlYU7peNKSEiQzXbiVxT4fD5VVFSEMCMAzdXdmxdqf2WxTm3dRYlRsU36EV6macrtNRXMyZqGITntRpPeLwDhY0g644BPI2PL9bOkUh12ZKmDWaNoo0pKOfLZasv+j/Xq9m/0Uk+nHh45I6Ia7BqvX77vi+Y/vypT97RoZadESZKi7IYcNmojIpvlDfadd96puXPn1prXs2fPwChXdXW1br75Zi1cuFBut1tjx47VY489prZt21qdSpN3xRVX6Nxzz1Wvnj1VXl4e7nTq5XA4tGnzZmVkZJzwNtu2bdOQnJwQZgWgOXv+m2X61+6N2j75/5QYFRvudOrlN6V7lhWq1O0/4W0yEx2aM7JNCLMC0FwYTp9SR+6VbLW/xusX51LXmAppeyd1O/XE609T8NymEm3Nr5Yk1XhNHarwasPeKknS5AHJGtaJMz8Q2UIygt23b18tXbr0xzf5yTOgZ86cqffee0+vv/66kpOTNWPGDF144YX6+OOPQ5FKkxYdHa20tDQ9/Le/6blnn22Sp4sPHDhQ1153nbKyshQff+IFr1u3bnr8iSf05wcf1M6dO0OYIYDmyOP36VB1qf7fx/+nK3uObpKni+8+XKOV31aopNqvGt+Jj2EXlnv14qclGtsjUekJnEgGoG41hbGq3N9KcT0P679PeLEbUqz8OmuPTys+Xa7VMaW6fdCUJndmjN80tegLlyo9P34J8G1xjdzeH2umz5R837/+z65KfVtcE1jWISVaZ3eh4UZkCcn/7A6Ho87RztLSUj311FN6+eWX9fOf/1yS9Mwzz6h3795at26dhgwZEop0mrTo6Gj96le/0p49e7R///4m1Yx26txZZ519tq6++uqjlu3evVtlLlfgdavUVGVlZQVep6amavr06froo4/krqnR3j17GiVnAM2Hx+/TM9uXKjuhjbLiUtU1KbPJfHg8VOHVN4dq9NGuox+r0yrWrpioH/OsrPGrtPrHD5eVHlNr8irVvbVTDpuUGkeTDbRU+yuKtNN14Kj5JU5DFcVR8pZGS6bqvBG93ZT61Li1aN9WfbbDpUldhqlzYlvFR8WEPvHjOFThVY3XlM+UPt5VqfKaExtl31FUox1FPzbYvdN96p4WrbYJDtk5dRwRIiSP6dqxY4eysrLUpUsXTZ06Vbt375Ykbdq0SR6PR6NHjw6s26tXL3Xo0EFr166tN57b7ZbL5ao1NTe33367Fi1eHO40ann++ef117/+tc5lN/2//6d+/foFpnvvuSfoGAAariXUx7s2vaIJH9RdY8Ll6Q2H9drW0jqXTR6QrDtHpwemc3sl1h1jY/0xAFijqdfIuXXUN1PSv7o6lHdahZJyCup/ypvdVPLPDig6o0I7XQfU7/Xr9WlR03hCzdMbDuvOpYW6e1nhCTfXddlW6NbdSwtVFsRlOEC4Wd5g5+Tk6Nlnn9X777+vxx9/XHl5eTrzzDNVVlam/Px8RUdHKyUlpdY2bdu2VX5+fr0x582bp+Tk5MCUnZ1tddphZxiGunTurDVr1qhfv35hzaVT585avWaN+vfvX2u06Kl//ENnDh+uM4cP1+rVq2tts2jRosCyt99+OzDfMAyNGDFCK1asUOvWrRttH4CWoiXUR+nIc2LPfHu2thblhTWPQxVe/WnlQe0t9dSaP6xTnGafnabZZ6epe5pThmEEptPaxQaWDcisPbK0/WCN/rzqoMrdvsbcDaDFaOo18rcDLtTzI2fVmmdIGrPLqx6HfTIMHXV6eGC975dd/qVHd35UfeSZ2f/lnde26NZrX5fP2zgNan01EmhJLD8vbfz48YGf+/fvr5ycHHXs2FGvvfaaYmNP7kY1c+bM0axZPxYfl8vV5AqkFeLi4zVs+HCdd/75iouP1/p16xo9h4EDB+qss8/WsGHDAs213+/Xv//9b33wwQf1Xiufn58f+JKkb9++iouN1egxY2QYhtLS0jRs+HBNmDBBH//nP/p627ZG2x+guWsp9bHS69bHBdv0znefqMLr1tC2vRo9h92Ha/TNoRrl/uT0RUnql+FU37Yx6pZW92NmkmPsSo6xS5L2ubyq8ZnaVuiWJFXU+JVbVKNP91erW+toZSZFhXYngBamqdfIbslZqvYd3Yy2qTSVECuZUYZqDsbKkeKWPabuL+JivFL892Vp5f7PZTdsgRoZFxetVqlx9Y+CW6i+Ggm0NCE5RfynUlJS1KNHD+Xm5iojI0M1NTUqKSmptU5BQcEx71DtdDqVlJRUa2rO7rnnHs3+7W8VFdX4H7SumT5dDz30UK3muqqqSlf/+td64403TijGk08+qZtmzpTH45Hff+QbU4fDob//4x+6+KKLQpY70BK1tPp424YX9OCWN1Xj88qsY7QmlFblVdQ6pduQFG03dMWgVjq9/Yl9gXx2l3hdMiBZ9p982PWb0gubS7RxX5XFGQOI1BrpNyTTZpPpi1bZlrQj12LXY1trm9ZlHfkS779r5MjxvXTz3HHy+035/aGpmaZpyus3tfLbCi57AdQIDXZ5ebl27typzMxMDRo0SFFRUVq2bFlg+fbt27V7924NHTo01KlElHHjx2tHbq7S09PDmse///1vde/WTYWFhUFtt+Obb9SpUydt3LgxRJkBaKn+vWejur3yaxVWhfeD3CkZTt07tq0SncH9V9o2waF54zPUqRWj1QCOZkpa3D1Ke//nGmVf+6ZSf56v6LQT/wLupzVy4dPrNfHMRzXxzEe1fk1ors92+0zd+WGBPtnDl4SAFIIG+5ZbbtGqVau0a9cu/ec//9HEiRNlt9s1ZcoUJScn66qrrtKsWbO0YsUKbdq0Sb/85S81dOjQFnkH8WOJjY1Vu3btNPvWW3X22WeHLY/qqirl5+cHRqJPlNfrVf6BA6qp4TQhANaq9nm0r7JID2x5Qyv3fx62PKLshpJj7bIFeWdzu81QSqydO+ICqJthqDrK0Je7c7Vi8waljrpe0eld6119d7Khr9J+/Ehf7fMov+qwfKZfvftlaewFfVVcVKEat9fyVPOKa/TetjIVVfmCelwh0JxZfg323r17NWXKFBUVFalNmzYaPny41q1bpzZt2kiSHnroIdlsNk2aNElut1tjx47VY489ZnUazYLdbtesWbNk+v3avn37MW8EFwoHDx5UcXFxg2IcOnRIRUVF3OAMgKX8pqmHPl8sm2GoZ0o7ZcS2atRHeCVE2xQf1bDvqBOdNsVFGar08KEUwNG+2LlVb9d8pz6//Zu8uz6Tb98uSZIt2if5DZk+u6JSU7U3OkHb7EdOEU+LSVK0LUpRNrvshk09T8lQXHy0Xn3mE0tzM01TZW6/thW69cE35ZbGBiKd5Q32woULj7k8JiZGCxYs0IIFC6x+62brppkzNemii9Sje3d5vdZ/+1ifiy+6qN6bmgUT48JJk/Tqq69alBUA/OihrYv1xrcfaceUvyvKaLznSV87JFXdWtd/TeSJmJ6Tqs37qvT3Tw5blBWA5uTzNjZ9qXL9Y+E1mreyUmftbi9JSjlzvzxFMare1V4D3lyohM3PSl9/KEl6Y8wc/SyjtyTJYdj19Pw1eubRj+QLwejyX9cc0gFX430uBSJF430aaeb27N6tCy+8MGTxq6uq5PM17mNcfD5fg9/TihgAIluHhDZ665w/hCx+rCNadiPktxSpxWZItgae4m23GUGfXg6g+flpjVxb8LUe/OzNIwsMQ35JftOnjKlTVONx6o8bXtATF8zVl/l79PjGVUpe85A+K/lO2fFp+tuw6ToltaOibD9+vB91bh+lZyTp/j/8y7J897s8WvylS0WVPnH+DXA0GmyLlJWV6e3Fi8OdBgA0OUnRcZrYmRtZAkBdflojU50JWlf49VHrlHfO1Kd+r1YVOGR2H66oNnvlU5GKvZXKTmijzoltNaHTkKMulenULU0pqXH64O0vlNzq5B6X+1P7XR59XejWlv3VDY4FNFc02Ag5xmcANDemjlyD2JDrvhv7MWMAmr6zs/pp9fkPHDV/4gf3aPGudYHPVBM7Dz3hLy5TUuP06EuXWZLf4i9dNNfAcdBgo14vvPii3n77bc286aYGxQjnXdABIBSe2nBYp2bGaPKpKQ2K8c1Bt3VJAWi25g+7VvcNniZJ6pKUEeZsABwLDTbq1alTJ7Vv165BMTp27Kj27dtblBEANA3FlT6VVDfs/hJFlT6VVAf3CEQALVP7hLRwpyBJOqVtjCQxig0cQ+PeFQYRxxEVpcSkpKBPg7TZbEpKSpLDwXc4AJonn1+q8viDPtXbb5qq8vjl5xRxABHmrC7xOqd7gmIcBpcAAvWgwcYxnXvuucrLy1N6enpQ2/Xo0UO79+zRGaefHqLMACC8Ps+v1px/58vlDm4UuqDMq1v/la9dxZ4QZQYAodMlNVoPnJuh9AQGUYC68C8Dtbz80kvKz8/X7bffLsMwFBUVpeTkZP3pwQf16quv6l/vvXfcGJdOnaoLzj9fiYmJgZFvr9erP/z+91q2bFmodwEAQiKnQ5ySY+x6d1uZJMlvSlUeU298Xqoz2sepf2bMcWOs312pLfurVe39cfTakDTxlCT1TneGKnUAsIzNZijGkM7vk6gqz4+1bP3uSu0oqrH0vbJTojSiS7xioxgvR+SgwUYta9as0f79+zVp0iR16dxZcfHxstvtuvzyy+UqLdWuXbskHXnud1lZWWC7Vq1aKTMrS5J0wfnn6+Jf/CKwzFVaqm/z8vTMM8/o0KFDjbo/AGCVHmlOpcTYtXlflQ5V+FTjM2VKWr+7SrEOm9Li7JKkVnF2xUb9eIJYRY1fpVVHrtfesr9am/ZVBZbFOAylxds1rFOcEp32Rt0fADhZhmHojOy4WvNKq30q9/x4Rk+1x9ThquDvVeGwSW3iHZIh9UiL1pmd4xucL9CYDDMCnxPicrmUnJwc7jSavTVr1mjY8OGB1z/9U7nwwgtrPff7uuuu06MLFgRe//Sa7TfffFMXX3RRaJMFglRaWqqkpKRwp2G5H+pjc92/cPuhDv5p1SHtrGek5rohqRrY7sfnza7cWa6Xt5TWue5pWTGaPiRVkhr0yC/ACi2hfrSEfQyX/24pNu+r1v+tLw46TmaiQ3eO+fHSRGojmoJgagcNNurVr18/nXf++brnnnuOWrZt2zYVFRUFXmdmZqpr165Hrfeb667TihUrtH379pDmCgSruX644sNj49hb6tFn+6v09ldlRy3LSHQoPvrHEezSap8OVRw9inPpqcnq2capzKSokOYKnKiWUD9awj42FeVun/a5vHpiXbEqauq/V8WpWTE6p3tC4HW03VB2ShSNNZqUYGoHp4ijXp9//rni4uN12mmnafz48YqN/XFEpnfv3sfc1lVaqiVLl9JcA2iW2idHqcZraneJR5/nV8v7k8+O+WXeY24b4zDUJ91Jcw2gWUtw2tWttU0Ds2JU6am/we7bNkbd0rgHBZoPGmwc0/p16zT5kkuUm5urjMzME97u27w8TgsH0Kx1aR2ta3JS9Yf3C1QaxDOx0+Idmj4kldEZAM2e3WboikGtwp0G0KhosHFcXq9XgwcPls1+4jfg8Xp4/AyA5s9mSL//eRv5g7jYys4DMgEAaLZosHFCCgsLw50CADQ5hmEoKYa7fwMAgCP4Hh0AAAAAAAvQYAMAAAAAYAEabAAAAAAALECDDQAAAACABWiwAQAAAACwAA02AAAAAAAWoMEGAAAAAMACNNgAAAAAAFiABhsAAAAAAAtY3mB36tRJhmEcNV1//fWSpBEjRhy17Nprr7U6DQAAAAAAGpXD6oAbNmyQz+cLvP7iiy80ZswYXXzxxYF5V199te66667A67i4OKvTAAAAAACgUVneYLdp06bW6/vvv19du3bV2WefHZgXFxenjIwMq98aAAAAAICwCek12DU1NXrxxRf1q1/9SoZhBOa/9NJLSktL0ymnnKI5c+aosrLymHHcbrdcLletCQBAfQSAY6FGAmhsIW2wFy9erJKSEl155ZWBeZdeeqlefPFFrVixQnPmzNELL7ygyy677Jhx5s2bp+Tk5MCUnZ0dyrQBIGJQHwGgftRIAI3NME3TDFXwsWPHKjo6Wv/85z/rXWf58uUaNWqUcnNz1bVr1zrXcbvdcrvdgdcul4sCCaBBSktLlZSUFO40Gqy++thc9g9A43G5XEpOTm5W9YMaCcAKwdRHy6/B/sF3332npUuX6q233jrmejk5OZJ0zAbb6XTK6XRaniMARDrqIwDUjxoJoLGF7BTxZ555Runp6fqf//mfY663ZcsWSVJmZmaoUgEAAAAAIORCMoLt9/v1zDPPaNq0aXI4fnyLnTt36uWXX9a5556r1q1ba+vWrZo5c6bOOuss9e/fPxSpAAAAAADQKELSYC9dulS7d+/Wr371q1rzo6OjtXTpUj388MOqqKhQdna2Jk2apD/+8Y+hSAMAAAAAgEYTkgb7nHPOUV33TsvOztaqVatC8ZYAAAAAAIRVSB/TBQAAAABAS0GDDQAAAACABWiwAQAAAACwAA02AAAAAAAWoMEGAAAAAMACNNgAAAAAAFggJI/pAgAAABA5/J79qsm/S0VLsmWL9arV8EI5s+6TYU8Od2pARKHBBgAAAFowv2e/fOUbVbXrGZX8p5fs8R7FZH8nw3GpDEebwHqOpCzZnAlhzBRo+miwAQAAgBasJv8uVe16RodXZyqmk0uSdHhVGx1edVmt9dpe9H+K73lOOFIEIgYNdjNyyy236PwLLghqm8svu0zfffddiDICgKahpuBBeV3vBLVNTMcXZIvuFJqEACDMKr7+QCXr/y5JMt075XenSZIM44c1jKO2KV7xJ5V/tUCJpxRTI4F60GA3Aw6HQ+eee67GjBmj4cOHn/B2pmnqggsu0OrVq7Vly5bQJQgAYWKaHnld/5K3fIl8FR8Hta239G3Z48+SPW5giLIDgMZnmqZ8ZUtUc/BDufdu+smS6ONu6ynaKb+7RvaYUvmqlig6/WdyZvQNXbJABOIu4hHOZrMpKSlJL7z4osacE9wpO4Zh6OG//U3XTJ+uqKioEGUIAOFhmj7J51L1d5fLV7Y06O3d+2bKU/SkTLNGpmmGIEMAaFym6Zf8HlXvuUmeQ/84qRi+8mi5NrfRwX/eJ9fml2X6PNRI4CdosCPcZZdfri++/FLx8fEnHePyyy/X51980aAYANDUeA+/qIqvT5H8FScdw1P8giq+7tegGADQVLj3f6bdjw7XoferVPFNSoPjlX+xSHuePEemp7LhyQHNBA12BJsxY4Z+8YtfKCMjQzbbyf8q4+Pj1bFjR/3hj3/UoEGDLMwQABqfaZqqOThfnsOvyfQWSGrAyIpZKbPmO9UU3Ctf5UbLcgSAcLAnpCvptKmSkSrT2/A2wPRUyVe2TzX51EjgB1yDHYGio6PVpk0b3XDjjerevbslMZ1Op373u9+p5PBh7dmzR4WFhZbEBYDGZPrdMr0HVXNwvsyaXIuC1qim8AHJniIjKluGI12GcfTNfwCgqXPEt1HigF+o7PNF8lcVWxLT9LtVseMhJTi6yB53uiUxgUjGCHYEGjx4sL7Ny1O3bt0sj33vfffpgw8/tDwuADQGX+Unqviqi3XN9U/UHPiDqnaOtTwuADSW6v1btHvBWfIetu4JMn63Xd/ec4ZKN6RbFhOIZIxgRyDDMEJ2UzK73S6H3R6S2AAQeqYkb4hi+0MYGwAagyn5ra1jhs1UYr9DikqrtjQuEKkYwQYAAACauaq8XarcsdPyuIZNim5bJc+hXar4ejt3FEeLxwg2AAAA0MztffIplX2xVClDfmiArb2XRP4rr8lwbNMpLzxtaVwg0tBgAwAAAM1cx5k3ynN4gGoKp6p0Yxv5K0NzuSHQ0nGKOAAAANDMRae3UVTrTHmKnDJ91rcAUa2rZY8/oMJFb6umqMjy+ECkoMEGAAAAWgBfRZRcW9Lkr7b+hrbOrApFtd6lvU/8XTUHCiyPD0QKGmwAAACgBfCWOHV4ebZ8FaG5SjSmQ7ZOXfSa4nv3DEl8IBLQYAMAAAAtgLNde3WcdbOiUluH5g0MQ7a4OBk88hUtGA02AAAA0AJEp7VWmwvOkyMxIdypAM1W0A326tWrdd555ykrK0uGYWjx4sW1lpumqdtvv12ZmZmKjY3V6NGjtWPHjlrrFBcXa+rUqUpKSlJKSoquuuoqlZeXN2hHAAAAAAAIp6Ab7IqKCg0YMEALFiyoc/mf/vQnPfLII3riiSe0fv16xcfHa+zYsaqurg6sM3XqVH355ZdasmSJ3n33Xa1evVrXXHPNye8FAAAAgBNStqWNqr5NCncaQLMU9B0Oxo8fr/Hjx9e5zDRNPfzww/rjH/+oCy64QJL0/PPPq23btlq8eLEmT56sbdu26f3339eGDRt0+umnS5Lmz5+vc889V3/+85+VlZXVgN1pGQ4dOqQ333xTY8aMUVKStcVx8+bNWr1qlaUxAaCxGI40OZIvlLdsieQvszS2LXag7AlnWRoTAMIhvtfP5DlsSlpvWczotn0V0zHHsnhApLL0Guy8vDzl5+dr9OjRgXnJycnKycnR2rVrJUlr165VSkpKoLmWpNGjR8tms2n9+rr/kbvdbrlcrlpTS7Zt2zZdfNFF2rVrl7xeryUxTdNUTU2Nnnj8cc2aNcuSmABCj/pYmz2mj2I6vS5bdCdJVt5kJ0pRadcqpt1DMgzDwrgAQokaeTTDMNTxphvU5rz/kemXTNOCoDaHkgZdprQxt1Ej0eJZ2mDn5+dLktq2bVtrftu2bQPL8vPzlZ6eXmu5w+FQampqYJ3/Nm/ePCUnJwem7OxsK9OOWGNGj9Ztt91mSazy8nKd0revXn75ZUviAWgc1Me6xXZboujMu60JZktQfO8vFNXqUmviAWg01Mj6ufcl6PDqdpKvYQ2xERWn7OkfKuGUCyzKDIhsEXEX8Tlz5qi0tDQw7dmzJ9wpNQkHDx7U0qVLdd+998rtdp90nE8++UT33nuvdu/ercrKSgszBBBq1MejGYYhmyNdjsTRik6fIxnRJx3LFneGotv+QUZUBxm2eAuzBNAYqJH1i+vRW5lTf61WI25SdGa/oLdPOv0KtTrrJrUaPkOOpHayRcWGIEsg8lj6lPmMjAxJUkFBgTIzMwPzCwoKdOqppwbWKSwsrLWd1+tVcXFxYPv/5nQ65XQ6rUy12di0caO2f/21Jl10kdq3b6/4+BP/AGiapgoLC7V06VL96YEHQpglgFChPtbPHne6bM6e8pS+KdOzV/IH9wWi4UiXI2G0nG1vDVGGAEKNGlm/hN69lNC7lyTJ9FXKV/6Tz+f+Kpm+wzIcbWX6KyR/pQxHhhQY7DaUfMYvFZXaqbHTBpo8Sxvszp07KyMjQ8uWLQs01C6XS+vXr9d1110nSRo6dKhKSkq0adMmDRo0SJK0fPly+f1+5eRwY4STUV5ern6nnKIFjz2mq6++Oqhtx4wera+++ipEmQFAmNkSFN/rc7n3XC9P8T+C2jS26xLZYvqEKDEAaDpSR/xWqWffHHjtLV2kqu8uV0Kfj1Vz6El5ip9RQu8VkvGT1sFmaRsBNBtB/8soLy9Xbm5u4HVeXp62bNmi1NRUdejQQTfddJPuuecede/eXZ07d9Ztt92mrKwsTZgwQZLUu3dvjRs3TldffbWeeOIJeTwezZgxQ5MnT+YO4g3g9Xr12IIF+te//nXiG5mmdu3aJb/fH7rEACCMjtxsJ0pRab+RPencoLa1RXeSYVh5ozQAaJoMm10/vTGkPXGYYru8JiOqjaJaXypH4jDJ7pRhRMTVpUBYBd1gb9y4USNHjgy8/uGO09OmTdOzzz6r2bNnq6KiQtdcc41KSko0fPhwvf/++4qJiQls89JLL2nGjBkaNWqUbDabJk2apEceecSC3WnZPvvsM3322WfhTgMAmhx73Kmyx50a7jQAICLYotrJltxOkmSP6S3F9A5zRkDkMEzTkpvzNyqXy6Xk5ORwpwEggpWWllr+HPmm4If62Fz3D0DotIT60RL2EYD1gqkdnOcBAAAAAIAFaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAsQIMNAAAAAIAFaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAABHANE395c4PtPiVzeFOBUA9aLABAACACFFeVq2qKk+40wBQD0e4EwAAAABwfIZh6I6/XBDuNAAcAyPYAAAAAABYgBHs44iPj9dll10mmy0030XUeDx6/rnn5PFwqg+AyOKrqlLR+0tkmv6QxDccUUobf45sUVEhiQ8AAGA1GuzjSE5O1iPz5ysqRB/wXC6XXl24kAYbQMTxlZdr10OPSD5fSOLb4uLUevRIiQYbQDNgmqbkr5Dp90t+44S3Mxx+yRYnw6AWApGABhsAAABoBJW5I1W5Y7/Kt7U6ofVtTp9Sz96v2E4vyZH8vyHODoAVaLABAACAEClY9Lbc+79WbOdSeUsOy1Pil1lTcULb+nx+lX0Wpaq8lxWdcUApQ64OcbYAGooGGwAAAAiR8q3rVP7lUnlLDn0/J+bEN/bZVL0nUdJaRReVK67r2Ypq1VGGwxmKVAFYgLuIAwAAACES36dYCf0PHX/F46jJ/1x7nxwrT8keC7ICECpBN9irV6/Weeedp6ysLBmGocWLFweWeTwe3XrrrerXr5/i4+OVlZWlK664Qvv3768Vo1OnTjIMo9Z0//33N3hnAAAAgKbAV1mk/S9courdn8g48XuaHVfh2zepZN3frQsIwFJBN9gVFRUaMGCAFixYcNSyyspKbd68Wbfddps2b96st956S9u3b9f5559/1Lp33XWXDhw4EJhuuOGGk9sDAAAAoInx13jl2vi1PCVllsatyf9SNQUr5Sl9W6bJU2iApiboa7DHjx+v8ePH17ksOTlZS5YsqTXv0Ucf1eDBg7V792516NAhMD8xMVEZGRnBvj0AAADQ5Jkem8o+S1NCf4/sMVWWxva6PlD1rveUcMoeyc7ju4CmJOTXYJeWlsowDKWkpNSaf//996t169YaOHCgHnzwQXm93npjuN1uuVyuWhMAgPoIAMcSzhppc/rUasQ+RadZ21xLUvW+eBWvypLps/DccwCWCGmDXV1drVtvvVVTpkxRUlJSYP6NN96ohQsXasWKFZo+fbruu+8+zZ49u9448+bNU3JycmDKzs4OZdoAEDGojwBQv3DXSJvTJ8MegsA+m/xum2SGIDaABglZg+3xePSLX/xCpmnq8ccfr7Vs1qxZGjFihPr3769rr71Wf/nLXzR//ny53e46Y82ZM0elpaWBac8e7p4IABL1EQCOpVnXSNNUzaEi+aqsHyEHcPJC8hzsH5rr7777TsuXL681el2XnJwceb1e7dq1Sz179jxqudPplNPJ8/4A4L9RHwGgfs25RvqqqvT55b9Ux5tmKf38/w13OgC+Z/kI9g/N9Y4dO7R06VK1bt36uNts2bJFNptN6enpVqcDAAAANDq/x6ayzW3kKYkOSXzDZirxlHxFta4MSXwAJyfoEezy8nLl5uYGXufl5WnLli1KTU1VZmamLrroIm3evFnvvvuufD6f8vPzJUmpqamKjo7W2rVrtX79eo0cOVKJiYlau3atZs6cqcsuu0ytWrWybs8AAACAMDHs0YrOGCKbc62k/BC8gWRE+WXYuBAbaEqCbrA3btyokSNHBl7PmjVLkjRt2jTdeeedeueddyRJp556aq3tVqxYoREjRsjpdGrhwoW688475Xa71blzZ82cOTMQBwAAAIh0USkp6j3/r8p/fboqv7G+wTZ9hlwb2qrVmXGWxwZw8oJusEeMGCHTrP+bsmMtk6TTTjtN69atC/ZtAQAAAHzPsJtKHrZfzsyKcKcC4CdC/hxsAAAAoKXyHIyVOz8Eo8ym5C2Okc8diueAAThZNNgAAABAiNQUpqkmPwQ38vXbVLW7vXxlCdbHBnDSQvKYLgAAAABS5zm/VdV3Z6ng9WmWxrXFxar/K8/LHs9NgoGmhBFsAAAAIERsTqecmb2UNv4eufd3l3t/fINjxnTIUdo5d8iRmCJbFONlQFPCv0gAAAAghByJ6UoceKns8V9KZq6iUotVvWevTP+Rx2zZ4rwyjGPHML2GfFUOxbRvp9jOw5Q44OLGSR5AUGiwAQAAgBAzDEPdH7hXkuSvrNSnEy6Wv7JS9gSPUs7cf9ztva5ola7LVL+XnlVMxw6hThfASaLBBgAAABqB8f0wtS0mRr0eevDICLbdlCOp5rjb+r2GsqZFKzqjbSAOgKaHBvs4qqurteitt2S3h+YRCFVVVfJ6vSGJDQChZIt2KvXsM2X6/aGJ73TKCFHtBYBwMux2JZzSN9xpAAgBGuzjKC4u1uTJk8OdBgA0OY7kJHW7+45wpwEAANBkcBdxAAAAAAAsQIMNAAAAAIAFaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAsQIMNAAAAAIAFaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABggaAb7NWrV+u8885TVlaWDMPQ4sWLay2/8sorZRhGrWncuHG11ikuLtbUqVOVlJSklJQUXXXVVSovL2/QjgAAAAAAEE5BN9gVFRUaMGCAFixYUO8648aN04EDBwLTK6+8Umv51KlT9eWXX2rJkiV69913tXr1al1zzTXBZw8AAAAAQBPhCHaD8ePHa/z48cdcx+l0KiMjo85l27Zt0/vvv68NGzbo9NNPlyTNnz9f5557rv785z8rKysr2JQAAAAAAAi7kFyDvXLlSqWnp6tnz5667rrrVFRUFFi2du1apaSkBJprSRo9erRsNpvWr19fZzy32y2Xy1VrAgBQHwHgWKiRABqb5Q32uHHj9Pzzz2vZsmV64IEHtGrVKo0fP14+n0+SlJ+fr/T09FrbOBwOpaamKj8/v86Y8+bNU3JycmDKzs62Om0AiEjURwCoHzUSQGOzvMGePHmyzj//fPXr108TJkzQu+++qw0bNmjlypUnHXPOnDkqLS0NTHv27LEuYQCIYNRHAKgfNRJAYwv6GuxgdenSRWlpacrNzdWoUaOUkZGhwsLCWut4vV4VFxfXe9220+mU0+kMdaoAEHGojwBQP2okgMYW8udg7927V0VFRcrMzJQkDR06VCUlJdq0aVNgneXLl8vv9ysnJyfU6QAAAAAAEBJBj2CXl5crNzc38DovL09btmxRamqqUlNTNXfuXE2aNEkZGRnauXOnZs+erW7dumns2LGSpN69e2vcuHG6+uqr9cQTT8jj8WjGjBmaPHkydxAHAAAAAESsoEewN27cqIEDB2rgwIGSpFmzZmngwIG6/fbbZbfbtXXrVp1//vnq0aOHrrrqKg0aNEhr1qypdXrOSy+9pF69emnUqFE699xzNXz4cD355JPW7RUAAAAAAI0s6BHsESNGyDTNepd/8MEHx42Rmpqql19+Odi3BgAAAACgyQr5NdgAAAAAALQENNgAAAAAAFiABhsAAAAAAAvQYAMAAAAAYAEabAAAAAAALECDDQAAAACABWiwAQAAAACwAA02AAAAAAAWoMEGAAAAAMACNNgAAAAAAFiABhsAAAAAAAvQYAMAAAAAYAEabAAAAAAALECDDQAAAACABWiwAQAAAACwAA02AAAAAAAWoMEGAAAAAMACNNgAAAAAAFiABhsAAAAAAAvQYAMAAAAAYAEabAAAAAAALECDDQAAAACABWiwAQAAAACwAA02AAAAAAAWCLrBXr16tc477zxlZWXJMAwtXry41nLDMOqcHnzwwcA6nTp1Omr5/fff3+CdAQAAAAAgXIJusCsqKjRgwAAtWLCgzuUHDhyoNT399NMyDEOTJk2qtd5dd91Va70bbrjh5PYAAAAAAIAmwBHsBuPHj9f48ePrXZ6RkVHr9dtvv62RI0eqS5cuteYnJiYetS4AAAAAAJEqpNdgFxQU6L333tNVV1111LL7779frVu31sCBA/Xggw/K6/XWG8ftdsvlctWaAADURwA4FmokgMYW0gb7ueeeU2Jioi688MJa82+88UYtXLhQK1as0PTp03Xfffdp9uzZ9caZN2+ekpOTA1N2dnYo0waAiEF9BID6USMBNDbDNE3zpDc2DC1atEgTJkyoc3mvXr00ZswYzZ8//5hxnn76aU2fPl3l5eVyOp1HLXe73XK73YHXLpeLAgmgQUpLS5WUlBTuNBqsvvrYXPYPQONxuVxKTk5uVvWDGgnACsHUx6CvwT5Ra9as0fbt2/Xqq68ed92cnBx5vV7t2rVLPXv2PGq50+mss/EGgJaO+ggA9aNGAmhsITtF/KmnntKgQYM0YMCA4667ZcsW2Ww2paenhyodAAAAAABCKugR7PLycuXm5gZe5+XlacuWLUpNTVWHDh0kHRlCf/311/WXv/zlqO3Xrl2r9evXa+TIkUpMTNTatWs1c+ZMXXbZZWrVqlUDdgUAAAAAgPAJusHeuHGjRo4cGXg9a9YsSdK0adP07LPPSpIWLlwo0zQ1ZcqUo7Z3Op1auHCh7rzzTrndbnXu3FkzZ84MxAEAAAAAIBI16CZn4fLDReYAcLKa6w1umuNNigA0jpZQP1rCPgKwXjC1I6SP6QIAAAAAoKWgwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAsQIMNAAAAAIAFaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAsQIMNAAAAAIAFHOFO4GSYphnuFABEuOZaR37YL5fLFeZMAESaH+pGc62PEjUSwMkJpj5GZINdVlYW7hQARLiysjIlJyeHOw3L/VAfs7Ozw5wJgEjVXOujJBUVFUmiRgI4OSdSHw0zAr+m9Pv92r59u/r06aM9e/YoKSkp3Ck1KS6XS9nZ2Ryb/8JxqV9LOjamaaqsrExZWVmy2ZrfVTLUx2NrSX/rweLY1K0lHZfmXh8lqaSkRK1atdLu3bub7ZcIJ6sl/a0Hi2NTv5ZybIKpjxE5gm2z2dSuXTtJUlJSUrP+ZTYEx6ZuHJf6tZRj05w/VFEfTwzHpn4cm7q1lOPSnOujpMAH4+Tk5Bbx+zwZLeVv/WRwbOrXEo7NidbH5vn1JAAAAAAAjYwGGwAAAAAAC0Rsg+10OnXHHXfI6XSGO5Umh2NTN45L/Tg2zQu/z/pxbOrHsakbx6V54fdZP45N/Tg29ePYHC0ib3IGAAAAAEBTE7Ej2AAAAAAANCU02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAsEJEN9oIFC9SpUyfFxMQoJydHn3zySbhTanR33nmnDMOoNfXq1SuwvLq6Wtdff71at26thIQETZo0SQUFBWHMOHRWr16t8847T1lZWTIMQ4sXL6613DRN3X777crMzFRsbKxGjx6tHTt21FqnuLhYU6dOVVJSklJSUnTVVVepvLy8EffCesc7LldeeeVRf0Pjxo2rtU5zPC4tQUuvkdTHH1Ef60eNbJmoj9THH1Af60d9bJiIa7BfffVVzZo1S3fccYc2b96sAQMGaOzYsSosLAx3ao2ub9++OnDgQGD66KOPAstmzpypf/7zn3r99de1atUq7d+/XxdeeGEYsw2diooKDRgwQAsWLKhz+Z/+9Cc98sgjeuKJJ7R+/XrFx8dr7Nixqq6uDqwzdepUffnll1qyZIneffddrV69Wtdcc01j7UJIHO+4SNK4ceNq/Q298sortZY3x+PS3FEjj6A+HkF9rB81suWhPh5BfTyC+lg/6mMDmRFm8ODB5vXXXx947fP5zKysLHPevHlhzKrx3XHHHeaAAQPqXFZSUmJGRUWZr7/+emDetm3bTEnm2rVrGynD8JBkLlq0KPDa7/ebGRkZ5oMPPhiYV1JSYjqdTvOVV14xTdM0v/rqK1OSuWHDhsA6//73v03DMMx9+/Y1Wu6h9N/HxTRNc9q0aeYFF1xQ7zYt4bg0R9RI6mN9qI/1o0a2DNRH6mN9qI/1oz4GL6JGsGtqarRp0yaNHj06MM9ms2n06NFau3ZtGDMLjx07digrK0tdunTR1KlTtXv3bknSpk2b5PF4ah2nXr16qUOHDi3uOOXl5Sk/P7/WsUhOTlZOTk7gWKxdu1YpKSk6/fTTA+uMHj1aNptN69evb/ScG9PKlSuVnp6unj176rrrrlNRUVFgWUs+LpGKGvkj6uPxUR+PjxrZfFAff0R9PD7q4/FRH+sXUQ32oUOH5PP51LZt21rz27Ztq/z8/DBlFR45OTl69tln9f777+vxxx9XXl6ezjzzTJWVlSk/P1/R0dFKSUmptU1LPE4/7O+x/mby8/OVnp5ea7nD4VBqamqzPl7jxo3T888/r2XLlumBBx7QqlWrNH78ePl8Pkkt97hEMmrkEdTHE0N9PDZqZPNCfTyC+nhiqI/HRn08Nke4E8DJGT9+fODn/v37KycnRx07dtRrr72m2NjYMGaGSDF58uTAz/369VP//v3VtWtXrVy5UqNGjQpjZkDDUB9hBWokmiPqI6xAfTy2iBrBTktLk91uP+puhgUFBcrIyAhTVk1DSkqKevToodzcXGVkZKimpkYlJSW11mmJx+mH/T3W30xGRsZRNzjxer0qLi5uUcerS5cuSktLU25uriSOSySiRtaN+lg36mNwqJGRjfpYN+pj3aiPwaE+1hZRDXZ0dLQGDRqkZcuWBeb5/X4tW7ZMQ4cODWNm4VdeXq6dO3cqMzNTgwYNUlRUVK3jtH37du3evbvFHafOnTsrIyOj1rFwuVxav3594FgMHTpUJSUl2rRpU2Cd5cuXy+/3Kycnp9FzDpe9e/eqqKhImZmZkjgukYgaWTfqY92oj8GhRkY26mPdqI91oz4Gh/r4X8J9l7VgLVy40HQ6neazzz5rfvXVV+Y111xjpqSkmPn5+eFOrVHdfPPN5sqVK828vDzz448/NkePHm2mpaWZhYWFpmma5rXXXmt26NDBXL58ublx40Zz6NCh5tChQ8OcdWiUlZWZn376qfnpp5+aksy//vWv5qeffmp+9913pmma5v3332+mpKSYb7/9trl161bzggsuMDt37mxWVVUFYowbN84cOHCguX79evOjjz4yu3fvbk6ZMiVcu2SJYx2XsrIy85ZbbjHXrl1r5uXlmUuXLjVPO+00s3v37mZ1dXUgRnM8Ls0dNZL6+FPUx/pRI1se6iP18aeoj/WjPjZMxDXYpmma8+fPNzt06GBGR0ebgwcPNtetWxfulBrdJZdcYmZmZprR0dFmu3btzEsuucTMzc0NLK+qqjJ/85vfmK1atTLj4uLMiRMnmgcOHAhjxqGzYsUKU9JR07Rp00zTPPKohdtuu81s27at6XQ6zVGjRpnbt2+vFaOoqMicMmWKmZCQYCYlJZm//OUvzbKysjDsjXWOdVwqKyvNc845x2zTpo0ZFRVlduzY0bz66quP+pDRHI9LS9DSayT18UfUx/pRI1sm6iP18QfUx/pRHxvGME3TDO0YOQAAAAAAzV9EXYMNAAAAAEBTRYMNAAAAAIAFaLABAAAAALAADTYAAAAAABagwQYAAAAAwAI02AAAAAAAWIAGGwAAAAAAC9BgAwAAAABgARpsAAAAAAAsQIMNAAAAAIAFaLABAAAAALDA/weuoMxqExO/3wAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import utilities\n",
        "\n",
        "utilities.run(UNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuaHX4fsDo_1"
      },
      "source": [
        "### Grading Rubric (Total: 10 Marks)\n",
        "\n",
        "The lab is graded based on the following criteria:\n",
        "\n",
        "1. **Data Loading and Preparation (1 Mark)**\n",
        "   - Ensure reproducible results by setting seed to `27` before all randomized operations. (1 Mark)\n",
        "   \n",
        "2. **U-Net Architecture (4 Marks)**\n",
        "   - Uses appropriate U-Net Architecture to the problem with a full pipeline. (4 Marks)\n",
        "\n",
        "3. **Hyperparameters Tuning (2 Marks)**\n",
        "   - Report evaluation metrics on validation set. (1 Mark)\n",
        "   - Analyzes results and tunes hyperparameters. (1 Mark)\n",
        "  \n",
        "4. **Model Evaluation and Understanding (3 Marks)**\n",
        "   - Reports Jaccard Index and Dice Scores for test set. (1 Mark)\n",
        "   - **Comparison amongst your peers.** Compares the model's performance against those of peers to identify strengths and areas for improvement. (2 Marks)\n",
        "\n",
        "Each section of the lab will be evaluated on completeness, and correctness in approach and analysis. Part of the rubric also includes the student's ability to explain and justify their choices and results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoXZX-XUDo_1"
      },
      "source": [
        "## Submission Instructions\n",
        "\n",
        "To ensure a smooth evaluation process, please follow these steps for submitting your work:\n",
        "\n",
        "1. **Prepare Your Submission:** Alongside your main notebook, include any additional files that are necessary for running the notebook successfully. This might include data files, images, or supplementary scripts.\n",
        "\n",
        "2. **Rename Your Files:** Before submission, please rename your notebook to reflect the IDs of the two students working on this project. The format should be `ID1_ID2`, where `ID1` and `ID2` are the student IDs. For example, if the student IDs are `9123456` and `9876543`, then your notebook should be named `9123456_9876543.ipynb`.\n",
        "\n",
        "3. **Check for Completeness:** Ensure that all required tasks are completed and that the notebook runs from start to finish without errors. This step is crucial for a smooth evaluation.\n",
        "\n",
        "4. **Submit Your Work:** Once everything is in order, submit your notebook and any additional files via the designated submission link on Google Classroom **(code: 2yj6e24)**. Make sure you meet the submission deadline to avoid any late penalties.\n",
        "5. Please, note that the same student should submit the assignments for the pair throughout the semester.\n",
        "\n",
        "By following these instructions carefully, you help us in evaluating your work efficiently and fairly **and any failure to adhere to these guidelines can affect your grades**. If you encounter any difficulties or have questions about the submission process, please reach out as soon as possible.\n",
        "\n",
        "We look forward to seeing your completed projects and wish you the best of luck!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}